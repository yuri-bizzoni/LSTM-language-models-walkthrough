{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a code to walk through the details of designing and using our preplexity-based Long-Short Term Memory systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#importing keras layers\n",
    "\n",
    "from numpy import array\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Embedding\n",
    "import tensorflow as tf\n",
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('1.10.0', '2.2.4')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#keras works on tensorflow\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "tf.VERSION,keras.__version__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing nltk\n",
    "\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making a toy language model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 22\n",
      "Total Sequences: 24\n",
      "(24,) (24,)\n"
     ]
    }
   ],
   "source": [
    "# source text: this is gonna be our corpus\n",
    "data = \"\"\" Jack and Jill went up the hill\\n\n",
    "\t\tTo fetch a pail of water\\n\n",
    "\t\tJack fell down and broke his crown\\n\n",
    "\t\tAnd Jill came tumbling after\\n \"\"\"\n",
    "\n",
    "# integer encode text: each word gets a unique integer id\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts([data])\n",
    "encoded = tokenizer.texts_to_sequences([data])[0]\n",
    "\n",
    "# determine the vocabulary size\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print('Vocabulary Size: %d' % vocab_size)\n",
    "\n",
    "# create word -> word sequences: each word is linked to the its follower\n",
    "sequences = list()\n",
    "for i in range(1, len(encoded)):\n",
    "\tsequence = encoded[i-1:i+1]\n",
    "\tsequences.append(sequence)\n",
    "print('Total Sequences: %d' % len(sequences))\n",
    "\n",
    "# split into X and y elements\n",
    "sequences = array(sequences)\n",
    "X, y = sequences[:,0],sequences[:,1]\n",
    "print(X.shape, y.shape)\n",
    "\n",
    "# one hot encode outputs: each word's id becomes a position in an otherwise empty vector\n",
    "y = to_categorical(y, num_classes=vocab_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we write the model: here a 50 cells LSTM connected to a dense output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 1, 10)             220       \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 50)                12200     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 22)                1122      \n",
      "=================================================================\n",
      "Total params: 13,542\n",
      "Trainable params: 13,542\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# define model\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 10, input_length=1))\n",
    "model.add(LSTM(50))\n",
    "model.add(Dense(vocab_size, activation='softmax'))\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we try to define or compute perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a way of defining perplexity\n",
    "def perplexity(y_true, y_pred):\n",
    "    crossentropy =  K.categorical_crossentropy(y_true, y_pred)\n",
    "    return K.exp(crossentropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a better way to define perplexity\n",
    "def perplexity2(y_true, y_pred):\n",
    "    cross_entropy = K.categorical_crossentropy(y_true, y_pred) \n",
    "    perplexity = K.pow(2.0, cross_entropy) \n",
    "    return perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we train the model on the training part of our corpus, for 500 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      " - 1s - loss: 3.0909 - acc: 0.0417 - perplexity: 21.9976\n",
      "Epoch 2/500\n",
      " - 0s - loss: 3.0902 - acc: 0.0833 - perplexity: 21.9807\n",
      "Epoch 3/500\n",
      " - 0s - loss: 3.0895 - acc: 0.0833 - perplexity: 21.9662\n",
      "Epoch 4/500\n",
      " - 0s - loss: 3.0887 - acc: 0.1667 - perplexity: 21.9493\n",
      "Epoch 5/500\n",
      " - 0s - loss: 3.0880 - acc: 0.1667 - perplexity: 21.9335\n",
      "Epoch 6/500\n",
      " - 0s - loss: 3.0873 - acc: 0.2083 - perplexity: 21.9174\n",
      "Epoch 7/500\n",
      " - 0s - loss: 3.0865 - acc: 0.2083 - perplexity: 21.9005\n",
      "Epoch 8/500\n",
      " - 0s - loss: 3.0857 - acc: 0.2083 - perplexity: 21.8835\n",
      "Epoch 9/500\n",
      " - 0s - loss: 3.0849 - acc: 0.2083 - perplexity: 21.8665\n",
      "Epoch 10/500\n",
      " - 0s - loss: 3.0841 - acc: 0.2083 - perplexity: 21.8494\n",
      "Epoch 11/500\n",
      " - 0s - loss: 3.0833 - acc: 0.2083 - perplexity: 21.8321\n",
      "Epoch 12/500\n",
      " - 0s - loss: 3.0825 - acc: 0.2083 - perplexity: 21.8147\n",
      "Epoch 13/500\n",
      " - 0s - loss: 3.0817 - acc: 0.2083 - perplexity: 21.7970\n",
      "Epoch 14/500\n",
      " - 0s - loss: 3.0808 - acc: 0.2083 - perplexity: 21.7791\n",
      "Epoch 15/500\n",
      " - 0s - loss: 3.0800 - acc: 0.2083 - perplexity: 21.7609\n",
      "Epoch 16/500\n",
      " - 0s - loss: 3.0791 - acc: 0.2083 - perplexity: 21.7425\n",
      "Epoch 17/500\n",
      " - 0s - loss: 3.0782 - acc: 0.1250 - perplexity: 21.7238\n",
      "Epoch 18/500\n",
      " - 0s - loss: 3.0773 - acc: 0.1250 - perplexity: 21.7046\n",
      "Epoch 19/500\n",
      " - 0s - loss: 3.0763 - acc: 0.1250 - perplexity: 21.6852\n",
      "Epoch 20/500\n",
      " - 0s - loss: 3.0754 - acc: 0.1250 - perplexity: 21.6654\n",
      "Epoch 21/500\n",
      " - 0s - loss: 3.0744 - acc: 0.1250 - perplexity: 21.6450\n",
      "Epoch 22/500\n",
      " - 0s - loss: 3.0734 - acc: 0.1250 - perplexity: 21.6244\n",
      "Epoch 23/500\n",
      " - 0s - loss: 3.0724 - acc: 0.1250 - perplexity: 21.6034\n",
      "Epoch 24/500\n",
      " - 0s - loss: 3.0714 - acc: 0.1250 - perplexity: 21.5819\n",
      "Epoch 25/500\n",
      " - 0s - loss: 3.0703 - acc: 0.1250 - perplexity: 21.5599\n",
      "Epoch 26/500\n",
      " - 0s - loss: 3.0692 - acc: 0.1250 - perplexity: 21.5374\n",
      "Epoch 27/500\n",
      " - 0s - loss: 3.0681 - acc: 0.1250 - perplexity: 21.5144\n",
      "Epoch 28/500\n",
      " - 0s - loss: 3.0669 - acc: 0.1250 - perplexity: 21.4908\n",
      "Epoch 29/500\n",
      " - 0s - loss: 3.0657 - acc: 0.2083 - perplexity: 21.4666\n",
      "Epoch 30/500\n",
      " - 0s - loss: 3.0645 - acc: 0.2083 - perplexity: 21.4418\n",
      "Epoch 31/500\n",
      " - 0s - loss: 3.0633 - acc: 0.2083 - perplexity: 21.4164\n",
      "Epoch 32/500\n",
      " - 0s - loss: 3.0619 - acc: 0.2083 - perplexity: 21.3900\n",
      "Epoch 33/500\n",
      " - 0s - loss: 3.0606 - acc: 0.2083 - perplexity: 21.3630\n",
      "Epoch 34/500\n",
      " - 0s - loss: 3.0592 - acc: 0.2083 - perplexity: 21.3355\n",
      "Epoch 35/500\n",
      " - 0s - loss: 3.0578 - acc: 0.2083 - perplexity: 21.3074\n",
      "Epoch 36/500\n",
      " - 0s - loss: 3.0564 - acc: 0.2083 - perplexity: 21.2784\n",
      "Epoch 37/500\n",
      " - 0s - loss: 3.0549 - acc: 0.2083 - perplexity: 21.2484\n",
      "Epoch 38/500\n",
      " - 0s - loss: 3.0533 - acc: 0.2083 - perplexity: 21.2176\n",
      "Epoch 39/500\n",
      " - 0s - loss: 3.0517 - acc: 0.2083 - perplexity: 21.1860\n",
      "Epoch 40/500\n",
      " - 0s - loss: 3.0501 - acc: 0.2083 - perplexity: 21.1535\n",
      "Epoch 41/500\n",
      " - 0s - loss: 3.0484 - acc: 0.2083 - perplexity: 21.1205\n",
      "Epoch 42/500\n",
      " - 0s - loss: 3.0467 - acc: 0.2083 - perplexity: 21.0866\n",
      "Epoch 43/500\n",
      " - 0s - loss: 3.0449 - acc: 0.2083 - perplexity: 21.0514\n",
      "Epoch 44/500\n",
      " - 0s - loss: 3.0431 - acc: 0.2083 - perplexity: 21.0157\n",
      "Epoch 45/500\n",
      " - 0s - loss: 3.0412 - acc: 0.2083 - perplexity: 20.9790\n",
      "Epoch 46/500\n",
      " - 0s - loss: 3.0392 - acc: 0.2083 - perplexity: 20.9413\n",
      "Epoch 47/500\n",
      " - 0s - loss: 3.0372 - acc: 0.2083 - perplexity: 20.9026\n",
      "Epoch 48/500\n",
      " - 0s - loss: 3.0352 - acc: 0.2083 - perplexity: 20.8628\n",
      "Epoch 49/500\n",
      " - 0s - loss: 3.0331 - acc: 0.2083 - perplexity: 20.8221\n",
      "Epoch 50/500\n",
      " - 0s - loss: 3.0309 - acc: 0.2083 - perplexity: 20.7798\n",
      "Epoch 51/500\n",
      " - 0s - loss: 3.0286 - acc: 0.2083 - perplexity: 20.7364\n",
      "Epoch 52/500\n",
      " - 0s - loss: 3.0263 - acc: 0.2083 - perplexity: 20.6923\n",
      "Epoch 53/500\n",
      " - 0s - loss: 3.0239 - acc: 0.2083 - perplexity: 20.6466\n",
      "Epoch 54/500\n",
      " - 0s - loss: 3.0214 - acc: 0.2083 - perplexity: 20.5997\n",
      "Epoch 55/500\n",
      " - 0s - loss: 3.0189 - acc: 0.2083 - perplexity: 20.5520\n",
      "Epoch 56/500\n",
      " - 0s - loss: 3.0163 - acc: 0.2083 - perplexity: 20.5030\n",
      "Epoch 57/500\n",
      " - 0s - loss: 3.0136 - acc: 0.2083 - perplexity: 20.4524\n",
      "Epoch 58/500\n",
      " - 0s - loss: 3.0108 - acc: 0.2083 - perplexity: 20.4005\n",
      "Epoch 59/500\n",
      " - 0s - loss: 3.0080 - acc: 0.2083 - perplexity: 20.3473\n",
      "Epoch 60/500\n",
      " - 0s - loss: 3.0051 - acc: 0.2083 - perplexity: 20.2931\n",
      "Epoch 61/500\n",
      " - 0s - loss: 3.0021 - acc: 0.2083 - perplexity: 20.2376\n",
      "Epoch 62/500\n",
      " - 0s - loss: 2.9990 - acc: 0.2083 - perplexity: 20.1806\n",
      "Epoch 63/500\n",
      " - 0s - loss: 2.9958 - acc: 0.2083 - perplexity: 20.1223\n",
      "Epoch 64/500\n",
      " - 0s - loss: 2.9926 - acc: 0.2083 - perplexity: 20.0625\n",
      "Epoch 65/500\n",
      " - 0s - loss: 2.9892 - acc: 0.2083 - perplexity: 20.0013\n",
      "Epoch 66/500\n",
      " - 0s - loss: 2.9858 - acc: 0.2083 - perplexity: 19.9386\n",
      "Epoch 67/500\n",
      " - 0s - loss: 2.9822 - acc: 0.2083 - perplexity: 19.8739\n",
      "Epoch 68/500\n",
      " - 0s - loss: 2.9786 - acc: 0.2083 - perplexity: 19.8077\n",
      "Epoch 69/500\n",
      " - 0s - loss: 2.9748 - acc: 0.2083 - perplexity: 19.7403\n",
      "Epoch 70/500\n",
      " - 0s - loss: 2.9710 - acc: 0.2083 - perplexity: 19.6714\n",
      "Epoch 71/500\n",
      " - 0s - loss: 2.9671 - acc: 0.2083 - perplexity: 19.6009\n",
      "Epoch 72/500\n",
      " - 0s - loss: 2.9630 - acc: 0.2083 - perplexity: 19.5288\n",
      "Epoch 73/500\n",
      " - 0s - loss: 2.9588 - acc: 0.2083 - perplexity: 19.4550\n",
      "Epoch 74/500\n",
      " - 0s - loss: 2.9545 - acc: 0.2083 - perplexity: 19.3791\n",
      "Epoch 75/500\n",
      " - 0s - loss: 2.9502 - acc: 0.2083 - perplexity: 19.3020\n",
      "Epoch 76/500\n",
      " - 0s - loss: 2.9456 - acc: 0.2083 - perplexity: 19.2232\n",
      "Epoch 77/500\n",
      " - 0s - loss: 2.9410 - acc: 0.2083 - perplexity: 19.1427\n",
      "Epoch 78/500\n",
      " - 0s - loss: 2.9362 - acc: 0.2083 - perplexity: 19.0600\n",
      "Epoch 79/500\n",
      " - 0s - loss: 2.9314 - acc: 0.2083 - perplexity: 18.9760\n",
      "Epoch 80/500\n",
      " - 0s - loss: 2.9264 - acc: 0.2083 - perplexity: 18.8903\n",
      "Epoch 81/500\n",
      " - 0s - loss: 2.9212 - acc: 0.2083 - perplexity: 18.8023\n",
      "Epoch 82/500\n",
      " - 0s - loss: 2.9160 - acc: 0.2083 - perplexity: 18.7130\n",
      "Epoch 83/500\n",
      " - 0s - loss: 2.9106 - acc: 0.2083 - perplexity: 18.6220\n",
      "Epoch 84/500\n",
      " - 0s - loss: 2.9051 - acc: 0.2083 - perplexity: 18.5292\n",
      "Epoch 85/500\n",
      " - 0s - loss: 2.8994 - acc: 0.2083 - perplexity: 18.4346\n",
      "Epoch 86/500\n",
      " - 0s - loss: 2.8936 - acc: 0.2083 - perplexity: 18.3382\n",
      "Epoch 87/500\n",
      " - 0s - loss: 2.8877 - acc: 0.2083 - perplexity: 18.2401\n",
      "Epoch 88/500\n",
      " - 0s - loss: 2.8816 - acc: 0.2083 - perplexity: 18.1401\n",
      "Epoch 89/500\n",
      " - 0s - loss: 2.8753 - acc: 0.2083 - perplexity: 18.0379\n",
      "Epoch 90/500\n",
      " - 0s - loss: 2.8689 - acc: 0.2083 - perplexity: 17.9338\n",
      "Epoch 91/500\n",
      " - 0s - loss: 2.8624 - acc: 0.2083 - perplexity: 17.8285\n",
      "Epoch 92/500\n",
      " - 0s - loss: 2.8557 - acc: 0.2083 - perplexity: 17.7215\n",
      "Epoch 93/500\n",
      " - 0s - loss: 2.8489 - acc: 0.2083 - perplexity: 17.6127\n",
      "Epoch 94/500\n",
      " - 0s - loss: 2.8419 - acc: 0.2083 - perplexity: 17.5022\n",
      "Epoch 95/500\n",
      " - 0s - loss: 2.8347 - acc: 0.2083 - perplexity: 17.3899\n",
      "Epoch 96/500\n",
      " - 0s - loss: 2.8274 - acc: 0.2083 - perplexity: 17.2760\n",
      "Epoch 97/500\n",
      " - 0s - loss: 2.8199 - acc: 0.2083 - perplexity: 17.1598\n",
      "Epoch 98/500\n",
      " - 0s - loss: 2.8123 - acc: 0.2083 - perplexity: 17.0419\n",
      "Epoch 99/500\n",
      " - 0s - loss: 2.8045 - acc: 0.2083 - perplexity: 16.9230\n",
      "Epoch 100/500\n",
      " - 0s - loss: 2.7965 - acc: 0.2083 - perplexity: 16.8024\n",
      "Epoch 101/500\n",
      " - 0s - loss: 2.7884 - acc: 0.2083 - perplexity: 16.6803\n",
      "Epoch 102/500\n",
      " - 0s - loss: 2.7801 - acc: 0.2083 - perplexity: 16.5566\n",
      "Epoch 103/500\n",
      " - 0s - loss: 2.7716 - acc: 0.2083 - perplexity: 16.4309\n",
      "Epoch 104/500\n",
      " - 0s - loss: 2.7630 - acc: 0.2083 - perplexity: 16.3042\n",
      "Epoch 105/500\n",
      " - 0s - loss: 2.7542 - acc: 0.2083 - perplexity: 16.1761\n",
      "Epoch 106/500\n",
      " - 0s - loss: 2.7453 - acc: 0.2083 - perplexity: 16.0466\n",
      "Epoch 107/500\n",
      " - 0s - loss: 2.7361 - acc: 0.2083 - perplexity: 15.9157\n",
      "Epoch 108/500\n",
      " - 0s - loss: 2.7268 - acc: 0.2083 - perplexity: 15.7835\n",
      "Epoch 109/500\n",
      " - 0s - loss: 2.7174 - acc: 0.2083 - perplexity: 15.6500\n",
      "Epoch 110/500\n",
      " - 0s - loss: 2.7077 - acc: 0.2083 - perplexity: 15.5147\n",
      "Epoch 111/500\n",
      " - 0s - loss: 2.6978 - acc: 0.2083 - perplexity: 15.3783\n",
      "Epoch 112/500\n",
      " - 0s - loss: 2.6878 - acc: 0.2083 - perplexity: 15.2413\n",
      "Epoch 113/500\n",
      " - 0s - loss: 2.6777 - acc: 0.2083 - perplexity: 15.1032\n",
      "Epoch 114/500\n",
      " - 0s - loss: 2.6674 - acc: 0.2083 - perplexity: 14.9641\n",
      "Epoch 115/500\n",
      " - 0s - loss: 2.6568 - acc: 0.2083 - perplexity: 14.8235\n",
      "Epoch 116/500\n",
      " - 0s - loss: 2.6462 - acc: 0.2500 - perplexity: 14.6826\n",
      "Epoch 117/500\n",
      " - 0s - loss: 2.6354 - acc: 0.2500 - perplexity: 14.5408\n",
      "Epoch 118/500\n",
      " - 0s - loss: 2.6244 - acc: 0.2500 - perplexity: 14.3982\n",
      "Epoch 119/500\n",
      " - 0s - loss: 2.6132 - acc: 0.2500 - perplexity: 14.2549\n",
      "Epoch 120/500\n",
      " - 0s - loss: 2.6019 - acc: 0.2500 - perplexity: 14.1104\n",
      "Epoch 121/500\n",
      " - 0s - loss: 2.5904 - acc: 0.2500 - perplexity: 13.9652\n",
      "Epoch 122/500\n",
      " - 0s - loss: 2.5787 - acc: 0.2500 - perplexity: 13.8201\n",
      "Epoch 123/500\n",
      " - 0s - loss: 2.5669 - acc: 0.2500 - perplexity: 13.6745\n",
      "Epoch 124/500\n",
      " - 0s - loss: 2.5550 - acc: 0.2500 - perplexity: 13.5285\n",
      "Epoch 125/500\n",
      " - 0s - loss: 2.5429 - acc: 0.2500 - perplexity: 13.3822\n",
      "Epoch 126/500\n",
      " - 0s - loss: 2.5306 - acc: 0.2500 - perplexity: 13.2356\n",
      "Epoch 127/500\n",
      " - 0s - loss: 2.5182 - acc: 0.2500 - perplexity: 13.0887\n",
      "Epoch 128/500\n",
      " - 0s - loss: 2.5057 - acc: 0.2500 - perplexity: 12.9412\n",
      "Epoch 129/500\n",
      " - 0s - loss: 2.4930 - acc: 0.2917 - perplexity: 12.7941\n",
      "Epoch 130/500\n",
      " - 0s - loss: 2.4802 - acc: 0.2917 - perplexity: 12.6469\n",
      "Epoch 131/500\n",
      " - 0s - loss: 2.4672 - acc: 0.2917 - perplexity: 12.4998\n",
      "Epoch 132/500\n",
      " - 0s - loss: 2.4541 - acc: 0.2917 - perplexity: 12.3528\n",
      "Epoch 133/500\n",
      " - 0s - loss: 2.4409 - acc: 0.2917 - perplexity: 12.2060\n",
      "Epoch 134/500\n",
      " - 0s - loss: 2.4276 - acc: 0.2917 - perplexity: 12.0588\n",
      "Epoch 135/500\n",
      " - 0s - loss: 2.4141 - acc: 0.2917 - perplexity: 11.9124\n",
      "Epoch 136/500\n",
      " - 0s - loss: 2.4005 - acc: 0.2917 - perplexity: 11.7664\n",
      "Epoch 137/500\n",
      " - 0s - loss: 2.3868 - acc: 0.2917 - perplexity: 11.6203\n",
      "Epoch 138/500\n",
      " - 0s - loss: 2.3730 - acc: 0.2917 - perplexity: 11.4751\n",
      "Epoch 139/500\n",
      " - 0s - loss: 2.3591 - acc: 0.2917 - perplexity: 11.3299\n",
      "Epoch 140/500\n",
      " - 0s - loss: 2.3451 - acc: 0.2917 - perplexity: 11.1857\n",
      "Epoch 141/500\n",
      " - 0s - loss: 2.3310 - acc: 0.2917 - perplexity: 11.0421\n",
      "Epoch 142/500\n",
      " - 0s - loss: 2.3168 - acc: 0.2917 - perplexity: 10.8992\n",
      "Epoch 143/500\n",
      " - 0s - loss: 2.3026 - acc: 0.2917 - perplexity: 10.7570\n",
      "Epoch 144/500\n",
      " - 0s - loss: 2.2882 - acc: 0.2917 - perplexity: 10.6155\n",
      "Epoch 145/500\n",
      " - 0s - loss: 2.2738 - acc: 0.2917 - perplexity: 10.4748\n",
      "Epoch 146/500\n",
      " - 0s - loss: 2.2593 - acc: 0.3333 - perplexity: 10.3349\n",
      "Epoch 147/500\n",
      " - 0s - loss: 2.2448 - acc: 0.3333 - perplexity: 10.1958\n",
      "Epoch 148/500\n",
      " - 0s - loss: 2.2301 - acc: 0.3333 - perplexity: 10.0577\n",
      "Epoch 149/500\n",
      " - 0s - loss: 2.2155 - acc: 0.3333 - perplexity: 9.9206\n",
      "Epoch 150/500\n",
      " - 0s - loss: 2.2007 - acc: 0.3333 - perplexity: 9.7844\n",
      "Epoch 151/500\n",
      " - 0s - loss: 2.1859 - acc: 0.3750 - perplexity: 9.6492\n",
      "Epoch 152/500\n",
      " - 0s - loss: 2.1711 - acc: 0.3750 - perplexity: 9.5146\n",
      "Epoch 153/500\n",
      " - 0s - loss: 2.1562 - acc: 0.3750 - perplexity: 9.3816\n",
      "Epoch 154/500\n",
      " - 0s - loss: 2.1413 - acc: 0.3750 - perplexity: 9.2497\n",
      "Epoch 155/500\n",
      " - 0s - loss: 2.1263 - acc: 0.3750 - perplexity: 9.1189\n",
      "Epoch 156/500\n",
      " - 0s - loss: 2.1113 - acc: 0.5000 - perplexity: 8.9893\n",
      "Epoch 157/500\n",
      " - 0s - loss: 2.0963 - acc: 0.5000 - perplexity: 8.8608\n",
      "Epoch 158/500\n",
      " - 0s - loss: 2.0813 - acc: 0.5000 - perplexity: 8.7337\n",
      "Epoch 159/500\n",
      " - 0s - loss: 2.0662 - acc: 0.5000 - perplexity: 8.6077\n",
      "Epoch 160/500\n",
      " - 0s - loss: 2.0512 - acc: 0.5000 - perplexity: 8.4830\n",
      "Epoch 161/500\n",
      " - 0s - loss: 2.0361 - acc: 0.5000 - perplexity: 8.3596\n",
      "Epoch 162/500\n",
      " - 0s - loss: 2.0210 - acc: 0.5000 - perplexity: 8.2376\n",
      "Epoch 163/500\n",
      " - 0s - loss: 2.0060 - acc: 0.5000 - perplexity: 8.1168\n",
      "Epoch 164/500\n",
      " - 0s - loss: 1.9908 - acc: 0.5000 - perplexity: 7.9971\n",
      "Epoch 165/500\n",
      " - 0s - loss: 1.9758 - acc: 0.5417 - perplexity: 7.8791\n",
      "Epoch 166/500\n",
      " - 0s - loss: 1.9607 - acc: 0.5417 - perplexity: 7.7625\n",
      "Epoch 167/500\n",
      " - 0s - loss: 1.9456 - acc: 0.5417 - perplexity: 7.6473\n",
      "Epoch 168/500\n",
      " - 0s - loss: 1.9306 - acc: 0.5417 - perplexity: 7.5335\n",
      "Epoch 169/500\n",
      " - 0s - loss: 1.9155 - acc: 0.5417 - perplexity: 7.4211\n",
      "Epoch 170/500\n",
      " - 0s - loss: 1.9005 - acc: 0.5417 - perplexity: 7.3102\n",
      "Epoch 171/500\n",
      " - 0s - loss: 1.8855 - acc: 0.5417 - perplexity: 7.2007\n",
      "Epoch 172/500\n",
      " - 0s - loss: 1.8706 - acc: 0.5417 - perplexity: 7.0927\n",
      "Epoch 173/500\n",
      " - 0s - loss: 1.8556 - acc: 0.5417 - perplexity: 6.9859\n",
      "Epoch 174/500\n",
      " - 0s - loss: 1.8407 - acc: 0.5417 - perplexity: 6.8808\n",
      "Epoch 175/500\n",
      " - 0s - loss: 1.8258 - acc: 0.5833 - perplexity: 6.7770\n",
      "Epoch 176/500\n",
      " - 0s - loss: 1.8109 - acc: 0.5833 - perplexity: 6.6749\n",
      "Epoch 177/500\n",
      " - 0s - loss: 1.7961 - acc: 0.5833 - perplexity: 6.5740\n",
      "Epoch 178/500\n",
      " - 0s - loss: 1.7814 - acc: 0.5833 - perplexity: 6.4749\n",
      "Epoch 179/500\n",
      " - 0s - loss: 1.7666 - acc: 0.5833 - perplexity: 6.3773\n",
      "Epoch 180/500\n",
      " - 0s - loss: 1.7520 - acc: 0.5833 - perplexity: 6.2812\n",
      "Epoch 181/500\n",
      " - 0s - loss: 1.7373 - acc: 0.5833 - perplexity: 6.1864\n",
      "Epoch 182/500\n",
      " - 0s - loss: 1.7227 - acc: 0.5833 - perplexity: 6.0932\n",
      "Epoch 183/500\n",
      " - 0s - loss: 1.7082 - acc: 0.5833 - perplexity: 6.0016\n",
      "Epoch 184/500\n",
      " - 0s - loss: 1.6937 - acc: 0.5833 - perplexity: 5.9112\n",
      "Epoch 185/500\n",
      " - 0s - loss: 1.6793 - acc: 0.5833 - perplexity: 5.8226\n",
      "Epoch 186/500\n",
      " - 0s - loss: 1.6649 - acc: 0.5833 - perplexity: 5.7352\n",
      "Epoch 187/500\n",
      " - 0s - loss: 1.6506 - acc: 0.6250 - perplexity: 5.6494\n",
      "Epoch 188/500\n",
      " - 0s - loss: 1.6363 - acc: 0.6250 - perplexity: 5.5650\n",
      "Epoch 189/500\n",
      " - 0s - loss: 1.6222 - acc: 0.6250 - perplexity: 5.4822\n",
      "Epoch 190/500\n",
      " - 0s - loss: 1.6081 - acc: 0.6250 - perplexity: 5.4008\n",
      "Epoch 191/500\n",
      " - 0s - loss: 1.5940 - acc: 0.6667 - perplexity: 5.3207\n",
      "Epoch 192/500\n",
      " - 0s - loss: 1.5800 - acc: 0.7083 - perplexity: 5.2422\n",
      "Epoch 193/500\n",
      " - 0s - loss: 1.5661 - acc: 0.7083 - perplexity: 5.1651\n",
      "Epoch 194/500\n",
      " - 0s - loss: 1.5522 - acc: 0.7083 - perplexity: 5.0892\n",
      "Epoch 195/500\n",
      " - 0s - loss: 1.5385 - acc: 0.7083 - perplexity: 5.0149\n",
      "Epoch 196/500\n",
      " - 0s - loss: 1.5248 - acc: 0.7083 - perplexity: 4.9419\n",
      "Epoch 197/500\n",
      " - 0s - loss: 1.5111 - acc: 0.7500 - perplexity: 4.8703\n",
      "Epoch 198/500\n",
      " - 0s - loss: 1.4976 - acc: 0.7500 - perplexity: 4.8000\n",
      "Epoch 199/500\n",
      " - 0s - loss: 1.4841 - acc: 0.7500 - perplexity: 4.7310\n",
      "Epoch 200/500\n",
      " - 0s - loss: 1.4707 - acc: 0.7500 - perplexity: 4.6633\n",
      "Epoch 201/500\n",
      " - 0s - loss: 1.4574 - acc: 0.7500 - perplexity: 4.5969\n",
      "Epoch 202/500\n",
      " - 0s - loss: 1.4441 - acc: 0.7917 - perplexity: 4.5317\n",
      "Epoch 203/500\n",
      " - 0s - loss: 1.4309 - acc: 0.7917 - perplexity: 4.4678\n",
      "Epoch 204/500\n",
      " - 0s - loss: 1.4178 - acc: 0.7917 - perplexity: 4.4050\n",
      "Epoch 205/500\n",
      " - 0s - loss: 1.4048 - acc: 0.7917 - perplexity: 4.3435\n",
      "Epoch 206/500\n",
      " - 0s - loss: 1.3918 - acc: 0.7917 - perplexity: 4.2833\n",
      "Epoch 207/500\n",
      " - 0s - loss: 1.3790 - acc: 0.7917 - perplexity: 4.2242\n",
      "Epoch 208/500\n",
      " - 0s - loss: 1.3661 - acc: 0.7917 - perplexity: 4.1661\n",
      "Epoch 209/500\n",
      " - 0s - loss: 1.3534 - acc: 0.7917 - perplexity: 4.1093\n",
      "Epoch 210/500\n",
      " - 0s - loss: 1.3408 - acc: 0.7917 - perplexity: 4.0537\n",
      "Epoch 211/500\n",
      " - 0s - loss: 1.3283 - acc: 0.7917 - perplexity: 3.9991\n",
      "Epoch 212/500\n",
      " - 0s - loss: 1.3158 - acc: 0.7917 - perplexity: 3.9455\n",
      "Epoch 213/500\n",
      " - 0s - loss: 1.3034 - acc: 0.7917 - perplexity: 3.8930\n",
      "Epoch 214/500\n",
      " - 0s - loss: 1.2911 - acc: 0.7917 - perplexity: 3.8415\n",
      "Epoch 215/500\n",
      " - 0s - loss: 1.2788 - acc: 0.7917 - perplexity: 3.7911\n",
      "Epoch 216/500\n",
      " - 0s - loss: 1.2667 - acc: 0.7917 - perplexity: 3.7417\n",
      "Epoch 217/500\n",
      " - 0s - loss: 1.2546 - acc: 0.8333 - perplexity: 3.6932\n",
      "Epoch 218/500\n",
      " - 0s - loss: 1.2426 - acc: 0.8333 - perplexity: 3.6458\n",
      "Epoch 219/500\n",
      " - 0s - loss: 1.2307 - acc: 0.8333 - perplexity: 3.5992\n",
      "Epoch 220/500\n",
      " - 0s - loss: 1.2188 - acc: 0.8333 - perplexity: 3.5536\n",
      "Epoch 221/500\n",
      " - 0s - loss: 1.2070 - acc: 0.8333 - perplexity: 3.5088\n",
      "Epoch 222/500\n",
      " - 0s - loss: 1.1953 - acc: 0.8333 - perplexity: 3.4649\n",
      "Epoch 223/500\n",
      " - 0s - loss: 1.1837 - acc: 0.8333 - perplexity: 3.4221\n",
      "Epoch 224/500\n",
      " - 0s - loss: 1.1722 - acc: 0.8333 - perplexity: 3.3799\n",
      "Epoch 225/500\n",
      " - 0s - loss: 1.1607 - acc: 0.8750 - perplexity: 3.3387\n",
      "Epoch 226/500\n",
      " - 0s - loss: 1.1494 - acc: 0.8750 - perplexity: 3.2984\n",
      "Epoch 227/500\n",
      " - 0s - loss: 1.1381 - acc: 0.8750 - perplexity: 3.2588\n",
      "Epoch 228/500\n",
      " - 0s - loss: 1.1269 - acc: 0.8750 - perplexity: 3.2201\n",
      "Epoch 229/500\n",
      " - 0s - loss: 1.1158 - acc: 0.8750 - perplexity: 3.1821\n",
      "Epoch 230/500\n",
      " - 0s - loss: 1.1047 - acc: 0.8750 - perplexity: 3.1449\n",
      "Epoch 231/500\n",
      " - 0s - loss: 1.0937 - acc: 0.8750 - perplexity: 3.1084\n",
      "Epoch 232/500\n",
      " - 0s - loss: 1.0829 - acc: 0.8750 - perplexity: 3.0726\n",
      "Epoch 233/500\n",
      " - 0s - loss: 1.0721 - acc: 0.8750 - perplexity: 3.0377\n",
      "Epoch 234/500\n",
      " - 0s - loss: 1.0613 - acc: 0.8750 - perplexity: 3.0034\n",
      "Epoch 235/500\n",
      " - 0s - loss: 1.0507 - acc: 0.8750 - perplexity: 2.9697\n",
      "Epoch 236/500\n",
      " - 0s - loss: 1.0401 - acc: 0.8750 - perplexity: 2.9368\n",
      "Epoch 237/500\n",
      " - 0s - loss: 1.0296 - acc: 0.8750 - perplexity: 2.9045\n",
      "Epoch 238/500\n",
      " - 0s - loss: 1.0192 - acc: 0.8750 - perplexity: 2.8729\n",
      "Epoch 239/500\n",
      " - 0s - loss: 1.0089 - acc: 0.8750 - perplexity: 2.8420\n",
      "Epoch 240/500\n",
      " - 0s - loss: 0.9986 - acc: 0.8750 - perplexity: 2.8116\n",
      "Epoch 241/500\n",
      " - 0s - loss: 0.9884 - acc: 0.8750 - perplexity: 2.7819\n",
      "Epoch 242/500\n",
      " - 0s - loss: 0.9784 - acc: 0.8750 - perplexity: 2.7528\n",
      "Epoch 243/500\n",
      " - 0s - loss: 0.9683 - acc: 0.8750 - perplexity: 2.7243\n",
      "Epoch 244/500\n",
      " - 0s - loss: 0.9584 - acc: 0.8750 - perplexity: 2.6964\n",
      "Epoch 245/500\n",
      " - 0s - loss: 0.9486 - acc: 0.8750 - perplexity: 2.6690\n",
      "Epoch 246/500\n",
      " - 0s - loss: 0.9388 - acc: 0.8750 - perplexity: 2.6422\n",
      "Epoch 247/500\n",
      " - 0s - loss: 0.9291 - acc: 0.8750 - perplexity: 2.6160\n",
      "Epoch 248/500\n",
      " - 0s - loss: 0.9195 - acc: 0.8750 - perplexity: 2.5903\n",
      "Epoch 249/500\n",
      " - 0s - loss: 0.9100 - acc: 0.8750 - perplexity: 2.5651\n",
      "Epoch 250/500\n",
      " - 0s - loss: 0.9006 - acc: 0.8750 - perplexity: 2.5404\n",
      "Epoch 251/500\n",
      " - 0s - loss: 0.8912 - acc: 0.8750 - perplexity: 2.5162\n",
      "Epoch 252/500\n",
      " - 0s - loss: 0.8820 - acc: 0.8750 - perplexity: 2.4926\n",
      "Epoch 253/500\n",
      " - 0s - loss: 0.8728 - acc: 0.8750 - perplexity: 2.4694\n",
      "Epoch 254/500\n",
      " - 0s - loss: 0.8637 - acc: 0.8750 - perplexity: 2.4466\n",
      "Epoch 255/500\n",
      " - 0s - loss: 0.8547 - acc: 0.8750 - perplexity: 2.4244\n",
      "Epoch 256/500\n",
      " - 0s - loss: 0.8457 - acc: 0.8750 - perplexity: 2.4026\n",
      "Epoch 257/500\n",
      " - 0s - loss: 0.8369 - acc: 0.8750 - perplexity: 2.3813\n",
      "Epoch 258/500\n",
      " - 0s - loss: 0.8281 - acc: 0.8750 - perplexity: 2.3603\n",
      "Epoch 259/500\n",
      " - 0s - loss: 0.8194 - acc: 0.8750 - perplexity: 2.3398\n",
      "Epoch 260/500\n",
      " - 0s - loss: 0.8108 - acc: 0.8750 - perplexity: 2.3198\n",
      "Epoch 261/500\n",
      " - 0s - loss: 0.8023 - acc: 0.8750 - perplexity: 2.3001\n",
      "Epoch 262/500\n",
      " - 0s - loss: 0.7939 - acc: 0.8750 - perplexity: 2.2808\n",
      "Epoch 263/500\n",
      " - 0s - loss: 0.7856 - acc: 0.8750 - perplexity: 2.2620\n",
      "Epoch 264/500\n",
      " - 0s - loss: 0.7773 - acc: 0.8750 - perplexity: 2.2435\n",
      "Epoch 265/500\n",
      " - 0s - loss: 0.7692 - acc: 0.8750 - perplexity: 2.2254\n",
      "Epoch 266/500\n",
      " - 0s - loss: 0.7611 - acc: 0.8750 - perplexity: 2.2076\n",
      "Epoch 267/500\n",
      " - 0s - loss: 0.7531 - acc: 0.8750 - perplexity: 2.1903\n",
      "Epoch 268/500\n",
      " - 0s - loss: 0.7452 - acc: 0.8750 - perplexity: 2.1732\n",
      "Epoch 269/500\n",
      " - 0s - loss: 0.7374 - acc: 0.8750 - perplexity: 2.1566\n",
      "Epoch 270/500\n",
      " - 0s - loss: 0.7296 - acc: 0.8750 - perplexity: 2.1403\n",
      "Epoch 271/500\n",
      " - 0s - loss: 0.7220 - acc: 0.8750 - perplexity: 2.1243\n",
      "Epoch 272/500\n",
      " - 0s - loss: 0.7144 - acc: 0.8750 - perplexity: 2.1086\n",
      "Epoch 273/500\n",
      " - 0s - loss: 0.7070 - acc: 0.8750 - perplexity: 2.0932\n",
      "Epoch 274/500\n",
      " - 0s - loss: 0.6996 - acc: 0.8750 - perplexity: 2.0782\n",
      "Epoch 275/500\n",
      " - 0s - loss: 0.6923 - acc: 0.8750 - perplexity: 2.0635\n",
      "Epoch 276/500\n",
      " - 0s - loss: 0.6851 - acc: 0.8750 - perplexity: 2.0491\n",
      "Epoch 277/500\n",
      " - 0s - loss: 0.6780 - acc: 0.8750 - perplexity: 2.0349\n",
      "Epoch 278/500\n",
      " - 0s - loss: 0.6709 - acc: 0.8750 - perplexity: 2.0211\n",
      "Epoch 279/500\n",
      " - 0s - loss: 0.6640 - acc: 0.8750 - perplexity: 2.0075\n",
      "Epoch 280/500\n",
      " - 0s - loss: 0.6571 - acc: 0.8750 - perplexity: 1.9943\n",
      "Epoch 281/500\n",
      " - 0s - loss: 0.6504 - acc: 0.8750 - perplexity: 1.9813\n",
      "Epoch 282/500\n",
      " - 0s - loss: 0.6437 - acc: 0.8750 - perplexity: 1.9685\n",
      "Epoch 283/500\n",
      " - 0s - loss: 0.6371 - acc: 0.8750 - perplexity: 1.9560\n",
      "Epoch 284/500\n",
      " - 0s - loss: 0.6305 - acc: 0.8750 - perplexity: 1.9438\n",
      "Epoch 285/500\n",
      " - 0s - loss: 0.6241 - acc: 0.8750 - perplexity: 1.9318\n",
      "Epoch 286/500\n",
      " - 0s - loss: 0.6178 - acc: 0.8750 - perplexity: 1.9200\n",
      "Epoch 287/500\n",
      " - 0s - loss: 0.6115 - acc: 0.8750 - perplexity: 1.9085\n",
      "Epoch 288/500\n",
      " - 0s - loss: 0.6053 - acc: 0.8750 - perplexity: 1.8973\n",
      "Epoch 289/500\n",
      " - 0s - loss: 0.5992 - acc: 0.8750 - perplexity: 1.8862\n",
      "Epoch 290/500\n",
      " - 0s - loss: 0.5932 - acc: 0.8750 - perplexity: 1.8754\n",
      "Epoch 291/500\n",
      " - 0s - loss: 0.5873 - acc: 0.8750 - perplexity: 1.8648\n",
      "Epoch 292/500\n",
      " - 0s - loss: 0.5814 - acc: 0.8750 - perplexity: 1.8544\n",
      "Epoch 293/500\n",
      " - 0s - loss: 0.5757 - acc: 0.8750 - perplexity: 1.8443\n",
      "Epoch 294/500\n",
      " - 0s - loss: 0.5700 - acc: 0.8750 - perplexity: 1.8343\n",
      "Epoch 295/500\n",
      " - 0s - loss: 0.5644 - acc: 0.8750 - perplexity: 1.8245\n",
      "Epoch 296/500\n",
      " - 0s - loss: 0.5589 - acc: 0.8750 - perplexity: 1.8149\n",
      "Epoch 297/500\n",
      " - 0s - loss: 0.5534 - acc: 0.8750 - perplexity: 1.8056\n",
      "Epoch 298/500\n",
      " - 0s - loss: 0.5481 - acc: 0.8750 - perplexity: 1.7964\n",
      "Epoch 299/500\n",
      " - 0s - loss: 0.5428 - acc: 0.8750 - perplexity: 1.7874\n",
      "Epoch 300/500\n",
      " - 0s - loss: 0.5376 - acc: 0.8750 - perplexity: 1.7786\n",
      "Epoch 301/500\n",
      " - 0s - loss: 0.5324 - acc: 0.8750 - perplexity: 1.7699\n",
      "Epoch 302/500\n",
      " - 0s - loss: 0.5274 - acc: 0.8750 - perplexity: 1.7615\n",
      "Epoch 303/500\n",
      " - 0s - loss: 0.5224 - acc: 0.8750 - perplexity: 1.7532\n",
      "Epoch 304/500\n",
      " - 0s - loss: 0.5175 - acc: 0.8750 - perplexity: 1.7451\n",
      "Epoch 305/500\n",
      " - 0s - loss: 0.5127 - acc: 0.8750 - perplexity: 1.7371\n",
      "Epoch 306/500\n",
      " - 0s - loss: 0.5079 - acc: 0.8750 - perplexity: 1.7293\n",
      "Epoch 307/500\n",
      " - 0s - loss: 0.5032 - acc: 0.8750 - perplexity: 1.7216\n",
      "Epoch 308/500\n",
      " - 0s - loss: 0.4986 - acc: 0.8750 - perplexity: 1.7142\n",
      "Epoch 309/500\n",
      " - 0s - loss: 0.4941 - acc: 0.8750 - perplexity: 1.7068\n",
      "Epoch 310/500\n",
      " - 0s - loss: 0.4896 - acc: 0.8750 - perplexity: 1.6996\n",
      "Epoch 311/500\n",
      " - 0s - loss: 0.4852 - acc: 0.8750 - perplexity: 1.6926\n",
      "Epoch 312/500\n",
      " - 0s - loss: 0.4809 - acc: 0.8750 - perplexity: 1.6857\n",
      "Epoch 313/500\n",
      " - 0s - loss: 0.4766 - acc: 0.8750 - perplexity: 1.6789\n",
      "Epoch 314/500\n",
      " - 0s - loss: 0.4724 - acc: 0.8750 - perplexity: 1.6723\n",
      "Epoch 315/500\n",
      " - 0s - loss: 0.4683 - acc: 0.8750 - perplexity: 1.6658\n",
      "Epoch 316/500\n",
      " - 0s - loss: 0.4642 - acc: 0.8750 - perplexity: 1.6594\n",
      "Epoch 317/500\n",
      " - 0s - loss: 0.4602 - acc: 0.8750 - perplexity: 1.6531\n",
      "Epoch 318/500\n",
      " - 0s - loss: 0.4562 - acc: 0.8750 - perplexity: 1.6470\n",
      "Epoch 319/500\n",
      " - 0s - loss: 0.4524 - acc: 0.8750 - perplexity: 1.6410\n",
      "Epoch 320/500\n",
      " - 0s - loss: 0.4486 - acc: 0.8750 - perplexity: 1.6351\n",
      "Epoch 321/500\n",
      " - 0s - loss: 0.4448 - acc: 0.8750 - perplexity: 1.6294\n",
      "Epoch 322/500\n",
      " - 0s - loss: 0.4411 - acc: 0.8750 - perplexity: 1.6237\n",
      "Epoch 323/500\n",
      " - 0s - loss: 0.4375 - acc: 0.8750 - perplexity: 1.6182\n",
      "Epoch 324/500\n",
      " - 0s - loss: 0.4339 - acc: 0.8750 - perplexity: 1.6128\n",
      "Epoch 325/500\n",
      " - 0s - loss: 0.4304 - acc: 0.8750 - perplexity: 1.6074\n",
      "Epoch 326/500\n",
      " - 0s - loss: 0.4269 - acc: 0.8750 - perplexity: 1.6022\n",
      "Epoch 327/500\n",
      " - 0s - loss: 0.4235 - acc: 0.8750 - perplexity: 1.5971\n",
      "Epoch 328/500\n",
      " - 0s - loss: 0.4202 - acc: 0.8750 - perplexity: 1.5921\n",
      "Epoch 329/500\n",
      " - 0s - loss: 0.4169 - acc: 0.8750 - perplexity: 1.5872\n",
      "Epoch 330/500\n",
      " - 0s - loss: 0.4137 - acc: 0.8750 - perplexity: 1.5824\n",
      "Epoch 331/500\n",
      " - 0s - loss: 0.4105 - acc: 0.8750 - perplexity: 1.5777\n",
      "Epoch 332/500\n",
      " - 0s - loss: 0.4074 - acc: 0.8750 - perplexity: 1.5731\n",
      "Epoch 333/500\n",
      " - 0s - loss: 0.4043 - acc: 0.8750 - perplexity: 1.5686\n",
      "Epoch 334/500\n",
      " - 0s - loss: 0.4013 - acc: 0.8750 - perplexity: 1.5642\n",
      "Epoch 335/500\n",
      " - 0s - loss: 0.3983 - acc: 0.8750 - perplexity: 1.5598\n",
      "Epoch 336/500\n",
      " - 0s - loss: 0.3954 - acc: 0.8750 - perplexity: 1.5556\n",
      "Epoch 337/500\n",
      " - 0s - loss: 0.3925 - acc: 0.8750 - perplexity: 1.5514\n",
      "Epoch 338/500\n",
      " - 0s - loss: 0.3897 - acc: 0.8750 - perplexity: 1.5473\n",
      "Epoch 339/500\n",
      " - 0s - loss: 0.3870 - acc: 0.8750 - perplexity: 1.5433\n",
      "Epoch 340/500\n",
      " - 0s - loss: 0.3842 - acc: 0.8750 - perplexity: 1.5394\n",
      "Epoch 341/500\n",
      " - 0s - loss: 0.3816 - acc: 0.8750 - perplexity: 1.5356\n",
      "Epoch 342/500\n",
      " - 0s - loss: 0.3789 - acc: 0.8750 - perplexity: 1.5318\n",
      "Epoch 343/500\n",
      " - 0s - loss: 0.3763 - acc: 0.8750 - perplexity: 1.5281\n",
      "Epoch 344/500\n",
      " - 0s - loss: 0.3738 - acc: 0.8750 - perplexity: 1.5245\n",
      "Epoch 345/500\n",
      " - 0s - loss: 0.3713 - acc: 0.8750 - perplexity: 1.5209\n",
      "Epoch 346/500\n",
      " - 0s - loss: 0.3688 - acc: 0.8750 - perplexity: 1.5174\n",
      "Epoch 347/500\n",
      " - 0s - loss: 0.3664 - acc: 0.8750 - perplexity: 1.5140\n",
      "Epoch 348/500\n",
      " - 0s - loss: 0.3641 - acc: 0.8750 - perplexity: 1.5107\n",
      "Epoch 349/500\n",
      " - 0s - loss: 0.3617 - acc: 0.8750 - perplexity: 1.5074\n",
      "Epoch 350/500\n",
      " - 0s - loss: 0.3594 - acc: 0.8750 - perplexity: 1.5042\n",
      "Epoch 351/500\n",
      " - 0s - loss: 0.3572 - acc: 0.8750 - perplexity: 1.5010\n",
      "Epoch 352/500\n",
      " - 0s - loss: 0.3550 - acc: 0.8750 - perplexity: 1.4979\n",
      "Epoch 353/500\n",
      " - 0s - loss: 0.3528 - acc: 0.8750 - perplexity: 1.4949\n",
      "Epoch 354/500\n",
      " - 0s - loss: 0.3507 - acc: 0.8750 - perplexity: 1.4919\n",
      "Epoch 355/500\n",
      " - 0s - loss: 0.3486 - acc: 0.8750 - perplexity: 1.4890\n",
      "Epoch 356/500\n",
      " - 0s - loss: 0.3465 - acc: 0.8750 - perplexity: 1.4861\n",
      "Epoch 357/500\n",
      " - 0s - loss: 0.3445 - acc: 0.8750 - perplexity: 1.4833\n",
      "Epoch 358/500\n",
      " - 0s - loss: 0.3425 - acc: 0.8750 - perplexity: 1.4806\n",
      "Epoch 359/500\n",
      " - 0s - loss: 0.3405 - acc: 0.8750 - perplexity: 1.4779\n",
      "Epoch 360/500\n",
      " - 0s - loss: 0.3386 - acc: 0.8750 - perplexity: 1.4752\n",
      "Epoch 361/500\n",
      " - 0s - loss: 0.3367 - acc: 0.8750 - perplexity: 1.4726\n",
      "Epoch 362/500\n",
      " - 0s - loss: 0.3349 - acc: 0.8750 - perplexity: 1.4701\n",
      "Epoch 363/500\n",
      " - 0s - loss: 0.3330 - acc: 0.8750 - perplexity: 1.4676\n",
      "Epoch 364/500\n",
      " - 0s - loss: 0.3312 - acc: 0.8750 - perplexity: 1.4652\n",
      "Epoch 365/500\n",
      " - 0s - loss: 0.3295 - acc: 0.8750 - perplexity: 1.4628\n",
      "Epoch 366/500\n",
      " - 0s - loss: 0.3277 - acc: 0.8750 - perplexity: 1.4604\n",
      "Epoch 367/500\n",
      " - 0s - loss: 0.3260 - acc: 0.8750 - perplexity: 1.4581\n",
      "Epoch 368/500\n",
      " - 0s - loss: 0.3244 - acc: 0.8750 - perplexity: 1.4558\n",
      "Epoch 369/500\n",
      " - 0s - loss: 0.3227 - acc: 0.8750 - perplexity: 1.4536\n",
      "Epoch 370/500\n",
      " - 0s - loss: 0.3211 - acc: 0.8750 - perplexity: 1.4514\n",
      "Epoch 371/500\n",
      " - 0s - loss: 0.3195 - acc: 0.8750 - perplexity: 1.4493\n",
      "Epoch 372/500\n",
      " - 0s - loss: 0.3180 - acc: 0.8750 - perplexity: 1.4472\n",
      "Epoch 373/500\n",
      " - 0s - loss: 0.3164 - acc: 0.8750 - perplexity: 1.4452\n",
      "Epoch 374/500\n",
      " - 0s - loss: 0.3149 - acc: 0.8750 - perplexity: 1.4431\n",
      "Epoch 375/500\n",
      " - 0s - loss: 0.3135 - acc: 0.8750 - perplexity: 1.4412\n",
      "Epoch 376/500\n",
      " - 0s - loss: 0.3120 - acc: 0.8750 - perplexity: 1.4392\n",
      "Epoch 377/500\n",
      " - 0s - loss: 0.3106 - acc: 0.8750 - perplexity: 1.4373\n",
      "Epoch 378/500\n",
      " - 0s - loss: 0.3092 - acc: 0.8750 - perplexity: 1.4354\n",
      "Epoch 379/500\n",
      " - 0s - loss: 0.3078 - acc: 0.8750 - perplexity: 1.4336\n",
      "Epoch 380/500\n",
      " - 0s - loss: 0.3064 - acc: 0.8750 - perplexity: 1.4318\n",
      "Epoch 381/500\n",
      " - 0s - loss: 0.3051 - acc: 0.8750 - perplexity: 1.4300\n",
      "Epoch 382/500\n",
      " - 0s - loss: 0.3038 - acc: 0.8750 - perplexity: 1.4283\n",
      "Epoch 383/500\n",
      " - 0s - loss: 0.3025 - acc: 0.8750 - perplexity: 1.4266\n",
      "Epoch 384/500\n",
      " - 0s - loss: 0.3012 - acc: 0.8750 - perplexity: 1.4249\n",
      "Epoch 385/500\n",
      " - 0s - loss: 0.3000 - acc: 0.8750 - perplexity: 1.4233\n",
      "Epoch 386/500\n",
      " - 0s - loss: 0.2987 - acc: 0.8750 - perplexity: 1.4216\n",
      "Epoch 387/500\n",
      " - 0s - loss: 0.2975 - acc: 0.8750 - perplexity: 1.4200\n",
      "Epoch 388/500\n",
      " - 0s - loss: 0.2963 - acc: 0.8750 - perplexity: 1.4185\n",
      "Epoch 389/500\n",
      " - 0s - loss: 0.2952 - acc: 0.8750 - perplexity: 1.4169\n",
      "Epoch 390/500\n",
      " - 0s - loss: 0.2940 - acc: 0.8750 - perplexity: 1.4154\n",
      "Epoch 391/500\n",
      " - 0s - loss: 0.2929 - acc: 0.8750 - perplexity: 1.4139\n",
      "Epoch 392/500\n",
      " - 0s - loss: 0.2918 - acc: 0.8750 - perplexity: 1.4125\n",
      "Epoch 393/500\n",
      " - 0s - loss: 0.2907 - acc: 0.8750 - perplexity: 1.4111\n",
      "Epoch 394/500\n",
      " - 0s - loss: 0.2896 - acc: 0.8750 - perplexity: 1.4097\n",
      "Epoch 395/500\n",
      " - 0s - loss: 0.2886 - acc: 0.8750 - perplexity: 1.4083\n",
      "Epoch 396/500\n",
      " - 0s - loss: 0.2875 - acc: 0.8750 - perplexity: 1.4069\n",
      "Epoch 397/500\n",
      " - 0s - loss: 0.2865 - acc: 0.8750 - perplexity: 1.4056\n",
      "Epoch 398/500\n",
      " - 0s - loss: 0.2855 - acc: 0.8750 - perplexity: 1.4043\n",
      "Epoch 399/500\n",
      " - 0s - loss: 0.2845 - acc: 0.8750 - perplexity: 1.4030\n",
      "Epoch 400/500\n",
      " - 0s - loss: 0.2835 - acc: 0.8750 - perplexity: 1.4017\n",
      "Epoch 401/500\n",
      " - 0s - loss: 0.2825 - acc: 0.8750 - perplexity: 1.4005\n",
      "Epoch 402/500\n",
      " - 0s - loss: 0.2816 - acc: 0.8750 - perplexity: 1.3992\n",
      "Epoch 403/500\n",
      " - 0s - loss: 0.2807 - acc: 0.8750 - perplexity: 1.3980\n",
      "Epoch 404/500\n",
      " - 0s - loss: 0.2797 - acc: 0.8750 - perplexity: 1.3968\n",
      "Epoch 405/500\n",
      " - 0s - loss: 0.2788 - acc: 0.8750 - perplexity: 1.3956\n",
      "Epoch 406/500\n",
      " - 0s - loss: 0.2779 - acc: 0.8750 - perplexity: 1.3945\n",
      "Epoch 407/500\n",
      " - 0s - loss: 0.2771 - acc: 0.8750 - perplexity: 1.3934\n",
      "Epoch 408/500\n",
      " - 0s - loss: 0.2762 - acc: 0.8750 - perplexity: 1.3922\n",
      "Epoch 409/500\n",
      " - 0s - loss: 0.2753 - acc: 0.8750 - perplexity: 1.3911\n",
      "Epoch 410/500\n",
      " - 0s - loss: 0.2745 - acc: 0.8750 - perplexity: 1.3901\n",
      "Epoch 411/500\n",
      " - 0s - loss: 0.2737 - acc: 0.8750 - perplexity: 1.3890\n",
      "Epoch 412/500\n",
      " - 0s - loss: 0.2729 - acc: 0.8750 - perplexity: 1.3880\n",
      "Epoch 413/500\n",
      " - 0s - loss: 0.2721 - acc: 0.8750 - perplexity: 1.3869\n",
      "Epoch 414/500\n",
      " - 0s - loss: 0.2713 - acc: 0.8750 - perplexity: 1.3859\n",
      "Epoch 415/500\n",
      " - 0s - loss: 0.2705 - acc: 0.8750 - perplexity: 1.3849\n",
      "Epoch 416/500\n",
      " - 0s - loss: 0.2697 - acc: 0.8750 - perplexity: 1.3839\n",
      "Epoch 417/500\n",
      " - 0s - loss: 0.2690 - acc: 0.8750 - perplexity: 1.3830\n",
      "Epoch 418/500\n",
      " - 0s - loss: 0.2682 - acc: 0.8750 - perplexity: 1.3820\n",
      "Epoch 419/500\n",
      " - 0s - loss: 0.2675 - acc: 0.8750 - perplexity: 1.3811\n",
      "Epoch 420/500\n",
      " - 0s - loss: 0.2668 - acc: 0.8750 - perplexity: 1.3802\n",
      "Epoch 421/500\n",
      " - 0s - loss: 0.2661 - acc: 0.8750 - perplexity: 1.3792\n",
      "Epoch 422/500\n",
      " - 0s - loss: 0.2654 - acc: 0.8750 - perplexity: 1.3783\n",
      "Epoch 423/500\n",
      " - 0s - loss: 0.2647 - acc: 0.8750 - perplexity: 1.3775\n",
      "Epoch 424/500\n",
      " - 0s - loss: 0.2640 - acc: 0.8750 - perplexity: 1.3766\n",
      "Epoch 425/500\n",
      " - 0s - loss: 0.2633 - acc: 0.8750 - perplexity: 1.3757\n",
      "Epoch 426/500\n",
      " - 0s - loss: 0.2626 - acc: 0.8750 - perplexity: 1.3749\n",
      "Epoch 427/500\n",
      " - 0s - loss: 0.2620 - acc: 0.8750 - perplexity: 1.3741\n",
      "Epoch 428/500\n",
      " - 0s - loss: 0.2613 - acc: 0.8750 - perplexity: 1.3733\n",
      "Epoch 429/500\n",
      " - 0s - loss: 0.2607 - acc: 0.8750 - perplexity: 1.3725\n",
      "Epoch 430/500\n",
      " - 0s - loss: 0.2601 - acc: 0.8750 - perplexity: 1.3717\n",
      "Epoch 431/500\n",
      " - 0s - loss: 0.2595 - acc: 0.8750 - perplexity: 1.3709\n",
      "Epoch 432/500\n",
      " - 0s - loss: 0.2589 - acc: 0.8750 - perplexity: 1.3701\n",
      "Epoch 433/500\n",
      " - 0s - loss: 0.2583 - acc: 0.8750 - perplexity: 1.3694\n",
      "Epoch 434/500\n",
      " - 0s - loss: 0.2577 - acc: 0.8750 - perplexity: 1.3686\n",
      "Epoch 435/500\n",
      " - 0s - loss: 0.2571 - acc: 0.8750 - perplexity: 1.3679\n",
      "Epoch 436/500\n",
      " - 0s - loss: 0.2565 - acc: 0.8750 - perplexity: 1.3672\n",
      "Epoch 437/500\n",
      " - 0s - loss: 0.2560 - acc: 0.8750 - perplexity: 1.3664\n",
      "Epoch 438/500\n",
      " - 0s - loss: 0.2554 - acc: 0.8750 - perplexity: 1.3657\n",
      "Epoch 439/500\n",
      " - 0s - loss: 0.2548 - acc: 0.8750 - perplexity: 1.3650\n",
      "Epoch 440/500\n",
      " - 0s - loss: 0.2543 - acc: 0.8750 - perplexity: 1.3644\n",
      "Epoch 441/500\n",
      " - 0s - loss: 0.2538 - acc: 0.8750 - perplexity: 1.3637\n",
      "Epoch 442/500\n",
      " - 0s - loss: 0.2532 - acc: 0.8750 - perplexity: 1.3630\n",
      "Epoch 443/500\n",
      " - 0s - loss: 0.2527 - acc: 0.8750 - perplexity: 1.3624\n",
      "Epoch 444/500\n",
      " - 0s - loss: 0.2522 - acc: 0.8750 - perplexity: 1.3617\n",
      "Epoch 445/500\n",
      " - 0s - loss: 0.2517 - acc: 0.8750 - perplexity: 1.3611\n",
      "Epoch 446/500\n",
      " - 0s - loss: 0.2512 - acc: 0.8750 - perplexity: 1.3605\n",
      "Epoch 447/500\n",
      " - 0s - loss: 0.2507 - acc: 0.8750 - perplexity: 1.3598\n",
      "Epoch 448/500\n",
      " - 0s - loss: 0.2502 - acc: 0.8750 - perplexity: 1.3592\n",
      "Epoch 449/500\n",
      " - 0s - loss: 0.2497 - acc: 0.8750 - perplexity: 1.3586\n",
      "Epoch 450/500\n",
      " - 0s - loss: 0.2493 - acc: 0.8750 - perplexity: 1.3580\n",
      "Epoch 451/500\n",
      " - 0s - loss: 0.2488 - acc: 0.8750 - perplexity: 1.3574\n",
      "Epoch 452/500\n",
      " - 0s - loss: 0.2483 - acc: 0.8750 - perplexity: 1.3569\n",
      "Epoch 453/500\n",
      " - 0s - loss: 0.2479 - acc: 0.8750 - perplexity: 1.3563\n",
      "Epoch 454/500\n",
      " - 0s - loss: 0.2474 - acc: 0.8750 - perplexity: 1.3557\n",
      "Epoch 455/500\n",
      " - 0s - loss: 0.2470 - acc: 0.8750 - perplexity: 1.3552\n",
      "Epoch 456/500\n",
      " - 0s - loss: 0.2465 - acc: 0.8750 - perplexity: 1.3546\n",
      "Epoch 457/500\n",
      " - 0s - loss: 0.2461 - acc: 0.8750 - perplexity: 1.3541\n",
      "Epoch 458/500\n",
      " - 0s - loss: 0.2457 - acc: 0.8750 - perplexity: 1.3536\n",
      "Epoch 459/500\n",
      " - 0s - loss: 0.2453 - acc: 0.8750 - perplexity: 1.3530\n",
      "Epoch 460/500\n",
      " - 0s - loss: 0.2448 - acc: 0.8750 - perplexity: 1.3525\n",
      "Epoch 461/500\n",
      " - 0s - loss: 0.2444 - acc: 0.8750 - perplexity: 1.3520\n",
      "Epoch 462/500\n",
      " - 0s - loss: 0.2440 - acc: 0.8750 - perplexity: 1.3515\n",
      "Epoch 463/500\n",
      " - 0s - loss: 0.2436 - acc: 0.8750 - perplexity: 1.3510\n",
      "Epoch 464/500\n",
      " - 0s - loss: 0.2432 - acc: 0.8750 - perplexity: 1.3505\n",
      "Epoch 465/500\n",
      " - 0s - loss: 0.2428 - acc: 0.8750 - perplexity: 1.3500\n",
      "Epoch 466/500\n",
      " - 0s - loss: 0.2425 - acc: 0.8750 - perplexity: 1.3495\n",
      "Epoch 467/500\n",
      " - 0s - loss: 0.2421 - acc: 0.8750 - perplexity: 1.3491\n",
      "Epoch 468/500\n",
      " - 0s - loss: 0.2417 - acc: 0.8750 - perplexity: 1.3486\n",
      "Epoch 469/500\n",
      " - 0s - loss: 0.2413 - acc: 0.8750 - perplexity: 1.3481\n",
      "Epoch 470/500\n",
      " - 0s - loss: 0.2410 - acc: 0.8750 - perplexity: 1.3477\n",
      "Epoch 471/500\n",
      " - 0s - loss: 0.2406 - acc: 0.8750 - perplexity: 1.3472\n",
      "Epoch 472/500\n",
      " - 0s - loss: 0.2402 - acc: 0.8750 - perplexity: 1.3468\n",
      "Epoch 473/500\n",
      " - 0s - loss: 0.2399 - acc: 0.8750 - perplexity: 1.3463\n",
      "Epoch 474/500\n",
      " - 0s - loss: 0.2395 - acc: 0.8750 - perplexity: 1.3459\n",
      "Epoch 475/500\n",
      " - 0s - loss: 0.2392 - acc: 0.8750 - perplexity: 1.3455\n",
      "Epoch 476/500\n",
      " - 0s - loss: 0.2388 - acc: 0.8750 - perplexity: 1.3450\n",
      "Epoch 477/500\n",
      " - 0s - loss: 0.2385 - acc: 0.8750 - perplexity: 1.3446\n",
      "Epoch 478/500\n",
      " - 0s - loss: 0.2382 - acc: 0.8750 - perplexity: 1.3442\n",
      "Epoch 479/500\n",
      " - 0s - loss: 0.2378 - acc: 0.8750 - perplexity: 1.3438\n",
      "Epoch 480/500\n",
      " - 0s - loss: 0.2375 - acc: 0.8750 - perplexity: 1.3434\n",
      "Epoch 481/500\n",
      " - 0s - loss: 0.2372 - acc: 0.8750 - perplexity: 1.3430\n",
      "Epoch 482/500\n",
      " - 0s - loss: 0.2369 - acc: 0.8750 - perplexity: 1.3426\n",
      "Epoch 483/500\n",
      " - 0s - loss: 0.2365 - acc: 0.8750 - perplexity: 1.3422\n",
      "Epoch 484/500\n",
      " - 0s - loss: 0.2362 - acc: 0.8750 - perplexity: 1.3418\n",
      "Epoch 485/500\n",
      " - 0s - loss: 0.2359 - acc: 0.8750 - perplexity: 1.3414\n",
      "Epoch 486/500\n",
      " - 0s - loss: 0.2356 - acc: 0.8750 - perplexity: 1.3410\n",
      "Epoch 487/500\n",
      " - 0s - loss: 0.2353 - acc: 0.8750 - perplexity: 1.3407\n",
      "Epoch 488/500\n",
      " - 0s - loss: 0.2350 - acc: 0.8750 - perplexity: 1.3403\n",
      "Epoch 489/500\n",
      " - 0s - loss: 0.2347 - acc: 0.8750 - perplexity: 1.3399\n",
      "Epoch 490/500\n",
      " - 0s - loss: 0.2344 - acc: 0.8750 - perplexity: 1.3396\n",
      "Epoch 491/500\n",
      " - 0s - loss: 0.2341 - acc: 0.8750 - perplexity: 1.3392\n",
      "Epoch 492/500\n",
      " - 0s - loss: 0.2339 - acc: 0.8750 - perplexity: 1.3389\n",
      "Epoch 493/500\n",
      " - 0s - loss: 0.2336 - acc: 0.8750 - perplexity: 1.3385\n",
      "Epoch 494/500\n",
      " - 0s - loss: 0.2333 - acc: 0.8750 - perplexity: 1.3382\n",
      "Epoch 495/500\n",
      " - 0s - loss: 0.2330 - acc: 0.8750 - perplexity: 1.3378\n",
      "Epoch 496/500\n",
      " - 0s - loss: 0.2327 - acc: 0.8750 - perplexity: 1.3375\n",
      "Epoch 497/500\n",
      " - 0s - loss: 0.2325 - acc: 0.8750 - perplexity: 1.3372\n",
      "Epoch 498/500\n",
      " - 0s - loss: 0.2322 - acc: 0.8750 - perplexity: 1.3368\n",
      "Epoch 499/500\n",
      " - 0s - loss: 0.2319 - acc: 0.8750 - perplexity: 1.3365\n",
      "Epoch 500/500\n",
      " - 0s - loss: 0.2317 - acc: 0.8750 - perplexity: 1.3362\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a2967acc0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compile network\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy',perplexity])\n",
    "# fit network\n",
    "model.fit(X, y, epochs=500, verbose=2)\n",
    "# check perplexity slowly falling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we evaluate the model:\n",
    "we have it generate a new sequence of words, or we test its perplexity on unseen data from the same corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a sequence from the model\n",
    "def generate_seq(model, tokenizer, seed_text, n_words):\n",
    "\tin_text, result = seed_text, seed_text\n",
    "\t# generate a fixed number of words\n",
    "\tfor _ in range(n_words):\n",
    "\t\t# encode the text as integer\n",
    "\t\tencoded = tokenizer.texts_to_sequences([in_text])[0]\n",
    "\t\tencoded = array(encoded)\n",
    "\t\t# predict a word in the vocabulary\n",
    "\t\tyhat = model.predict_classes(encoded, verbose=0)\n",
    "\t\t# map predicted word index to word\n",
    "\t\tout_word = ''\n",
    "\t\tfor word, index in tokenizer.word_index.items():\n",
    "\t\t\tif index == yhat:\n",
    "\t\t\t\tout_word = word\n",
    "\t\t\t\tbreak\n",
    "\t\t# append to input\n",
    "\t\tin_text, result = out_word, result + ' ' + out_word\n",
    "\treturn result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "to fetch a pail of water jack and jill came tumbling\n"
     ]
    }
   ],
   "source": [
    "# evaluate through generating a sentence\n",
    "print(generate_seq(model, tokenizer, 'to', 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 22\n",
      "Total Sequences: 24\n",
      "(24,) (24,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.23141591250896454, 0.875, 1.3358511924743652]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We can also evaluate the model as its perplexity in front of a sentence\n",
    "\n",
    "data = \"\"\" Jack and Jill went up the hill to fetch a pail of water Jack fell down and broke his crown and Jill came tumbling after\\n\"\"\"\n",
    "# integer encode text\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts([data])\n",
    "encoded = tokenizer.texts_to_sequences([data])[0]\n",
    "# determine the vocabulary size\n",
    "#vocab_size = len(tokenizer.word_index) + 1\n",
    "print('Vocabulary Size: %d' % vocab_size)\n",
    "# create word -> word sequences\n",
    "sequences = list()\n",
    "for i in range(1, len(encoded)):\n",
    "\tsequence = encoded[i-1:i+1]\n",
    "\tsequences.append(sequence)\n",
    "print('Total Sequences: %d' % len(sequences))\n",
    "# split into X and y elements\n",
    "sequences = array(sequences)\n",
    "X1, y1 = sequences[:,0],sequences[:,1]\n",
    "print(X1.shape, y1.shape)\n",
    "# one hot encode outputs\n",
    "y1 = to_categorical(y1, num_classes=vocab_size)\n",
    "\n",
    "model.evaluate(X1,y1, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way of preprocessing the data is to break it per line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# per-line model : we can break the input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this time, the input is divided in segments (lines)\n",
    "data = \"\"\" Jack and Jill went up the hill\\n\n",
    "\t\tTo fetch a pail of water\\n\n",
    "\t\tJack fell down and broke his crown\\n\n",
    "\t\tAnd Jill came tumbling after\\n \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Sequences: 21\n",
      "Max Sequence Length: 7\n"
     ]
    }
   ],
   "source": [
    "# integer encode text\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts([data])\n",
    "encoded = tokenizer.texts_to_sequences([data])[0]\n",
    "\n",
    "# determine the vocabulary size\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "# create line-based sequences\n",
    "sequences = list()\n",
    "for line in data.split('\\n'):\n",
    "#for line in data_sents[:100]:\n",
    "\tencoded = tokenizer.texts_to_sequences([line])[0]\n",
    "\tfor i in range(1, len(encoded)):\n",
    "\t\tsequence = encoded[:i+1]\n",
    "\t\tsequences.append(sequence)\n",
    "print('Total Sequences: %d' % len(sequences))\n",
    "\n",
    "# pad input sequences\n",
    "max_length = max([len(seq) for seq in sequences])\n",
    "sequences = pad_sequences(sequences, maxlen=max_length, padding='pre')\n",
    "print('Max Sequence Length: %d' % max_length)\n",
    "\n",
    "# split into input and output elements\n",
    "sequences = array(sequences)\n",
    "X, y = sequences[:,:-1],sequences[:,-1]\n",
    "y = to_categorical(y, num_classes=vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21, 6) (21, 22) (19, 6) (19, 22)\n"
     ]
    }
   ],
   "source": [
    "#we can define the training split\n",
    "\n",
    "trainX = X[:19]\n",
    "trainY = y[:19]\n",
    "\n",
    "print(X.shape, y.shape, trainX.shape, trainY.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_9 (Embedding)      (None, 251, 10)           160       \n",
      "_________________________________________________________________\n",
      "lstm_15 (LSTM)               (None, 50)                12200     \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 16)                816       \n",
      "=================================================================\n",
      "Total params: 13,176\n",
      "Trainable params: 13,176\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# define the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 10, input_length=max_length-1))  #<<<< BEWARE THE INPUT LENGTH\n",
    "model.add(LSTM(50))\n",
    "model.add(Dense(vocab_size, activation='softmax'))\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      " - 5s - loss: 2.5639 - acc: 0.3450 - perplexity2: 40.2948\n",
      "Epoch 2/500\n",
      " - 5s - loss: 2.1329 - acc: 0.3710 - perplexity2: 15.0291\n",
      "Epoch 3/500\n",
      " - 5s - loss: 1.9412 - acc: 0.3900 - perplexity2: 10.8643\n",
      "Epoch 4/500\n",
      " - 5s - loss: 2.0486 - acc: 0.3520 - perplexity2: 9.4152\n",
      "Epoch 5/500\n",
      " - 5s - loss: 1.8381 - acc: 0.4000 - perplexity2: 6.4953\n",
      "Epoch 6/500\n",
      " - 5s - loss: 1.7575 - acc: 0.4080 - perplexity2: 5.6783\n",
      "Epoch 7/500\n",
      " - 5s - loss: 1.7128 - acc: 0.4190 - perplexity2: 5.3828\n",
      "Epoch 8/500\n",
      " - 5s - loss: 1.6835 - acc: 0.4260 - perplexity2: 5.2291\n",
      "Epoch 9/500\n",
      " - 5s - loss: 1.6521 - acc: 0.4340 - perplexity2: 5.0297\n",
      "Epoch 10/500\n",
      " - 5s - loss: 1.6241 - acc: 0.4410 - perplexity2: 4.9049\n",
      "Epoch 11/500\n",
      " - 5s - loss: 1.6005 - acc: 0.4380 - perplexity2: 4.7620\n",
      "Epoch 12/500\n",
      " - 5s - loss: 1.5788 - acc: 0.4550 - perplexity2: 4.6052\n",
      "Epoch 13/500\n",
      " - 5s - loss: 1.5574 - acc: 0.4600 - perplexity2: 4.5749\n",
      "Epoch 14/500\n",
      " - 5s - loss: 1.5430 - acc: 0.4580 - perplexity2: 4.5508\n",
      "Epoch 15/500\n",
      " - 5s - loss: 1.5283 - acc: 0.4670 - perplexity2: 4.4453\n",
      "Epoch 16/500\n",
      " - 5s - loss: 1.5081 - acc: 0.4750 - perplexity2: 4.3328\n",
      "Epoch 17/500\n",
      " - 5s - loss: 1.4917 - acc: 0.4780 - perplexity2: 4.3204\n",
      "Epoch 18/500\n",
      " - 5s - loss: 1.4753 - acc: 0.4800 - perplexity2: 4.2257\n",
      "Epoch 19/500\n",
      " - 5s - loss: 1.4624 - acc: 0.4980 - perplexity2: 4.2174\n",
      "Epoch 20/500\n",
      " - 5s - loss: 1.4464 - acc: 0.4900 - perplexity2: 4.0974\n",
      "Epoch 21/500\n",
      " - 5s - loss: 1.4305 - acc: 0.4970 - perplexity2: 4.0585\n",
      "Epoch 22/500\n",
      " - 5s - loss: 1.4165 - acc: 0.4990 - perplexity2: 3.9468\n",
      "Epoch 23/500\n",
      " - 5s - loss: 1.4010 - acc: 0.5130 - perplexity2: 3.9677\n",
      "Epoch 24/500\n",
      " - 5s - loss: 1.3885 - acc: 0.5160 - perplexity2: 3.8710\n",
      "Epoch 25/500\n",
      " - 5s - loss: 1.3722 - acc: 0.5290 - perplexity2: 3.8117\n",
      "Epoch 26/500\n",
      " - 5s - loss: 1.3562 - acc: 0.5270 - perplexity2: 3.7061\n",
      "Epoch 27/500\n",
      " - 5s - loss: 1.3400 - acc: 0.5400 - perplexity2: 3.6589\n",
      "Epoch 28/500\n",
      " - 5s - loss: 1.3291 - acc: 0.5280 - perplexity2: 3.6513\n",
      "Epoch 29/500\n",
      " - 5s - loss: 1.3228 - acc: 0.5370 - perplexity2: 3.6687\n",
      "Epoch 30/500\n",
      " - 5s - loss: 1.2996 - acc: 0.5510 - perplexity2: 3.5155\n",
      "Epoch 31/500\n",
      " - 5s - loss: 1.2850 - acc: 0.5460 - perplexity2: 3.4880\n",
      "Epoch 32/500\n",
      " - 5s - loss: 1.2691 - acc: 0.5600 - perplexity2: 3.4031\n",
      "Epoch 33/500\n",
      " - 5s - loss: 1.2601 - acc: 0.5810 - perplexity2: 3.3829\n",
      "Epoch 34/500\n",
      " - 5s - loss: 1.2487 - acc: 0.5680 - perplexity2: 3.3334\n",
      "Epoch 35/500\n",
      " - 5s - loss: 1.2372 - acc: 0.5700 - perplexity2: 3.2905\n",
      "Epoch 36/500\n",
      " - 5s - loss: 1.2125 - acc: 0.5770 - perplexity2: 3.2117\n",
      "Epoch 37/500\n",
      " - 5s - loss: 1.2066 - acc: 0.5920 - perplexity2: 3.1798\n",
      "Epoch 38/500\n",
      " - 5s - loss: 1.1873 - acc: 0.5960 - perplexity2: 3.1215\n",
      "Epoch 39/500\n",
      " - 5s - loss: 1.2068 - acc: 0.5910 - perplexity2: 3.2373\n",
      "Epoch 40/500\n",
      " - 5s - loss: 1.1679 - acc: 0.5980 - perplexity2: 3.0491\n",
      "Epoch 41/500\n",
      " - 5s - loss: 1.1537 - acc: 0.6070 - perplexity2: 3.0341\n",
      "Epoch 42/500\n",
      " - 5s - loss: 1.1435 - acc: 0.6110 - perplexity2: 2.9813\n",
      "Epoch 43/500\n",
      " - 6s - loss: 1.1345 - acc: 0.6200 - perplexity2: 2.9673\n",
      "Epoch 44/500\n",
      " - 5s - loss: 1.1148 - acc: 0.6310 - perplexity2: 2.9014\n",
      "Epoch 45/500\n",
      " - 5s - loss: 1.1018 - acc: 0.6330 - perplexity2: 2.8282\n",
      "Epoch 46/500\n",
      " - 5s - loss: 1.0908 - acc: 0.6480 - perplexity2: 2.8078\n",
      "Epoch 47/500\n",
      " - 5s - loss: 1.0702 - acc: 0.6470 - perplexity2: 2.7764\n",
      "Epoch 48/500\n",
      " - 5s - loss: 1.0642 - acc: 0.6580 - perplexity2: 2.7642\n",
      "Epoch 49/500\n",
      " - 5s - loss: 1.0490 - acc: 0.6720 - perplexity2: 2.7021\n",
      "Epoch 50/500\n",
      " - 5s - loss: 1.0366 - acc: 0.6520 - perplexity2: 2.6656\n",
      "Epoch 51/500\n",
      " - 5s - loss: 1.0298 - acc: 0.6710 - perplexity2: 2.6630\n",
      "Epoch 52/500\n",
      " - 5s - loss: 1.0176 - acc: 0.6680 - perplexity2: 2.6266\n",
      "Epoch 53/500\n",
      " - 5s - loss: 0.9989 - acc: 0.6730 - perplexity2: 2.5825\n",
      "Epoch 54/500\n",
      " - 5s - loss: 0.9845 - acc: 0.6860 - perplexity2: 2.5615\n",
      "Epoch 55/500\n",
      " - 5s - loss: 0.9746 - acc: 0.6920 - perplexity2: 2.5069\n",
      "Epoch 56/500\n",
      " - 6s - loss: 0.9706 - acc: 0.6880 - perplexity2: 2.4836\n",
      "Epoch 57/500\n",
      " - 5s - loss: 0.9568 - acc: 0.6980 - perplexity2: 2.4993\n",
      "Epoch 58/500\n",
      " - 5s - loss: 0.9436 - acc: 0.6960 - perplexity2: 2.4322\n",
      "Epoch 59/500\n",
      " - 5s - loss: 0.9221 - acc: 0.7110 - perplexity2: 2.3798\n",
      "Epoch 60/500\n",
      " - 5s - loss: 0.9148 - acc: 0.7120 - perplexity2: 2.3562\n",
      "Epoch 61/500\n",
      " - 6s - loss: 0.9063 - acc: 0.7190 - perplexity2: 2.3427\n",
      "Epoch 62/500\n",
      " - 6s - loss: 0.8910 - acc: 0.7130 - perplexity2: 2.3133\n",
      "Epoch 63/500\n",
      " - 6s - loss: 0.8818 - acc: 0.7230 - perplexity2: 2.3128\n",
      "Epoch 64/500\n",
      " - 5s - loss: 0.8744 - acc: 0.7200 - perplexity2: 2.2692\n",
      "Epoch 65/500\n",
      " - 5s - loss: 0.8616 - acc: 0.7210 - perplexity2: 2.2594\n",
      "Epoch 66/500\n",
      " - 6s - loss: 0.8494 - acc: 0.7340 - perplexity2: 2.2251\n",
      "Epoch 67/500\n",
      " - 5s - loss: 0.8331 - acc: 0.7370 - perplexity2: 2.1904\n",
      "Epoch 68/500\n",
      " - 5s - loss: 0.8317 - acc: 0.7400 - perplexity2: 2.1841\n",
      "Epoch 69/500\n",
      " - 5s - loss: 0.8171 - acc: 0.7370 - perplexity2: 2.1600\n",
      "Epoch 70/500\n",
      " - 5s - loss: 0.8025 - acc: 0.7550 - perplexity2: 2.1241\n",
      "Epoch 71/500\n",
      " - 5s - loss: 0.7921 - acc: 0.7580 - perplexity2: 2.1015\n",
      "Epoch 72/500\n",
      " - 5s - loss: 0.7843 - acc: 0.7570 - perplexity2: 2.0783\n",
      "Epoch 73/500\n",
      " - 5s - loss: 0.7721 - acc: 0.7700 - perplexity2: 2.0721\n",
      "Epoch 74/500\n",
      " - 5s - loss: 0.7656 - acc: 0.7710 - perplexity2: 2.0440\n",
      "Epoch 75/500\n",
      " - 5s - loss: 0.7507 - acc: 0.7830 - perplexity2: 2.0531\n",
      "Epoch 76/500\n",
      " - 6s - loss: 0.7385 - acc: 0.7870 - perplexity2: 1.9943\n",
      "Epoch 77/500\n",
      " - 6s - loss: 0.7309 - acc: 0.7930 - perplexity2: 1.9964\n",
      "Epoch 78/500\n",
      " - 5s - loss: 0.7264 - acc: 0.7920 - perplexity2: 1.9859\n",
      "Epoch 79/500\n",
      " - 5s - loss: 0.7132 - acc: 0.8020 - perplexity2: 1.9592\n",
      "Epoch 80/500\n",
      " - 5s - loss: 0.7073 - acc: 0.7970 - perplexity2: 1.9488\n",
      "Epoch 81/500\n",
      " - 5s - loss: 0.6948 - acc: 0.8060 - perplexity2: 1.9159\n",
      "Epoch 82/500\n",
      " - 6s - loss: 0.6900 - acc: 0.8010 - perplexity2: 1.9040\n",
      "Epoch 83/500\n",
      " - 6s - loss: 0.6818 - acc: 0.8130 - perplexity2: 1.8973\n",
      "Epoch 84/500\n",
      " - 5s - loss: 0.6682 - acc: 0.8110 - perplexity2: 1.8847\n",
      "Epoch 85/500\n",
      " - 5s - loss: 0.6575 - acc: 0.8200 - perplexity2: 1.8578\n",
      "Epoch 86/500\n",
      " - 5s - loss: 0.6482 - acc: 0.8260 - perplexity2: 1.8502\n",
      "Epoch 87/500\n",
      " - 5s - loss: 0.6469 - acc: 0.8270 - perplexity2: 1.8511\n",
      "Epoch 88/500\n",
      " - 5s - loss: 0.6364 - acc: 0.8300 - perplexity2: 1.8300\n",
      "Epoch 89/500\n",
      " - 5s - loss: 0.6310 - acc: 0.8270 - perplexity2: 1.8111\n",
      "Epoch 90/500\n",
      " - 5s - loss: 0.6209 - acc: 0.8350 - perplexity2: 1.7962\n",
      "Epoch 91/500\n",
      " - 5s - loss: 0.6152 - acc: 0.8390 - perplexity2: 1.7863\n",
      "Epoch 92/500\n",
      " - 5s - loss: 0.6005 - acc: 0.8460 - perplexity2: 1.7722\n",
      "Epoch 93/500\n",
      " - 6s - loss: 0.5969 - acc: 0.8450 - perplexity2: 1.7711\n",
      "Epoch 94/500\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-420-9b2cd2163868>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy',perplexity2])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# fit network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1380\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1381\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1382\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1383\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1384\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# compile the network\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy',perplexity2])\n",
    "# fit the network\n",
    "model.fit(trainX, trainY, epochs=500, verbose=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#so this is the perplexity of the model on the data it observed\n",
    "model.evaluate(trainX, trainY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# generate a sequence from a language model\n",
    "def generate_seq(model, tokenizer, max_length, seed_text, n_words):\n",
    "\tin_text = seed_text\n",
    "\t# generate a fixed number of words\n",
    "\tfor _ in range(n_words):\n",
    "\t\t# encode the text as integer\n",
    "\t\tencoded = tokenizer.texts_to_sequences([in_text])[0]\n",
    "\t\t# pre-pad sequences to a fixed length\n",
    "\t\tencoded = pad_sequences([encoded], maxlen=max_length, padding='pre')\n",
    "\t\t# predict probabilities for each word\n",
    "\t\tyhat = model.predict_classes(encoded, verbose=0)\n",
    "\t\t# map predicted word index to word\n",
    "\t\tout_word = ''\n",
    "\t\tfor word, index in tokenizer.word_index.items():\n",
    "\t\t\tif index == yhat:\n",
    "\t\t\t\tout_word = word\n",
    "\t\t\t\tbreak\n",
    "\t\t# append to input\n",
    "\t\tin_text += ' ' + out_word\n",
    "\treturn in_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#show the 'quality' of the model\n",
    "generate_seq(model, tokenizer, max_length-1, 'and', 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [],
   "source": [
    "#or, again, we define a new sequence to evaluate\n",
    "data = \"\"\" Jill and Jack went up the hill\\n\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Sequences: 11\n",
      "Max Sequence Length: 252\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[3.765824794769287, 0.1818181872367859, 47.031124114990234]"
      ]
     },
     "execution_count": 422,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# eval sequences\n",
    "sequences = list()\n",
    "for line in data.split('\\n'):\n",
    "\tencoded = tokenizer.texts_to_sequences([line])[0]\n",
    "\tfor i in range(1, len(encoded)):\n",
    "\t\tsequence = encoded[:i+1]\n",
    "\t\tsequences.append(sequence)\n",
    "print('Total Sequences: %d' % len(sequences))\n",
    "\n",
    "# pad input sequences\n",
    "#max_length = max([len(seq) for seq in sequences])\n",
    "sequences = pad_sequences(sequences, maxlen=max_length, padding='pre')\n",
    "print('Max Sequence Length: %d' % max_length)\n",
    "\n",
    "# split into input and output elements\n",
    "sequences = array(sequences)\n",
    "X1, y1 = sequences[:,:-1],sequences[:,-1]\n",
    "y1 = to_categorical(y1, num_classes=vocab_size)\n",
    "\n",
    "model.evaluate(X1,y1, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.19470392167568207, 1.0, 1.1529335975646973]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#this is how perplexed the model is on the whole data (train and test)\n",
    "model.evaluate(X,y, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally there is one last possibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Two in one out : just another possibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 22\n",
      "Total Sequences: 23\n",
      "Max Sequence Length: 3\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 2, 10)             220       \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 50)                12200     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 22)                1122      \n",
      "=================================================================\n",
      "Total params: 13,542\n",
      "Trainable params: 13,542\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/500\n",
      " - 2s - loss: 3.0912 - acc: 0.0435\n",
      "Epoch 2/500\n",
      " - 0s - loss: 3.0906 - acc: 0.0000e+00\n",
      "Epoch 3/500\n",
      " - 0s - loss: 3.0899 - acc: 0.1304\n",
      "Epoch 4/500\n",
      " - 0s - loss: 3.0890 - acc: 0.1304\n",
      "Epoch 5/500\n",
      " - 0s - loss: 3.0881 - acc: 0.1304\n",
      "Epoch 6/500\n",
      " - 0s - loss: 3.0872 - acc: 0.1304\n",
      "Epoch 7/500\n",
      " - 0s - loss: 3.0864 - acc: 0.1304\n",
      "Epoch 8/500\n",
      " - 0s - loss: 3.0855 - acc: 0.1304\n",
      "Epoch 9/500\n",
      " - 0s - loss: 3.0846 - acc: 0.1304\n",
      "Epoch 10/500\n",
      " - 0s - loss: 3.0837 - acc: 0.1304\n",
      "Epoch 11/500\n",
      " - 0s - loss: 3.0827 - acc: 0.1304\n",
      "Epoch 12/500\n",
      " - 0s - loss: 3.0818 - acc: 0.1304\n",
      "Epoch 13/500\n",
      " - 0s - loss: 3.0808 - acc: 0.1304\n",
      "Epoch 14/500\n",
      " - 0s - loss: 3.0799 - acc: 0.1304\n",
      "Epoch 15/500\n",
      " - 0s - loss: 3.0789 - acc: 0.1304\n",
      "Epoch 16/500\n",
      " - 0s - loss: 3.0778 - acc: 0.1304\n",
      "Epoch 17/500\n",
      " - 0s - loss: 3.0768 - acc: 0.1304\n",
      "Epoch 18/500\n",
      " - 0s - loss: 3.0757 - acc: 0.1304\n",
      "Epoch 19/500\n",
      " - 0s - loss: 3.0746 - acc: 0.1304\n",
      "Epoch 20/500\n",
      " - 0s - loss: 3.0734 - acc: 0.1304\n",
      "Epoch 21/500\n",
      " - 0s - loss: 3.0722 - acc: 0.1304\n",
      "Epoch 22/500\n",
      " - 0s - loss: 3.0710 - acc: 0.1304\n",
      "Epoch 23/500\n",
      " - 0s - loss: 3.0697 - acc: 0.1304\n",
      "Epoch 24/500\n",
      " - 0s - loss: 3.0684 - acc: 0.1304\n",
      "Epoch 25/500\n",
      " - 0s - loss: 3.0671 - acc: 0.1304\n",
      "Epoch 26/500\n",
      " - 0s - loss: 3.0657 - acc: 0.1304\n",
      "Epoch 27/500\n",
      " - 0s - loss: 3.0642 - acc: 0.1304\n",
      "Epoch 28/500\n",
      " - 0s - loss: 3.0627 - acc: 0.1304\n",
      "Epoch 29/500\n",
      " - 0s - loss: 3.0611 - acc: 0.1304\n",
      "Epoch 30/500\n",
      " - 0s - loss: 3.0595 - acc: 0.1304\n",
      "Epoch 31/500\n",
      " - 0s - loss: 3.0578 - acc: 0.1304\n",
      "Epoch 32/500\n",
      " - 0s - loss: 3.0561 - acc: 0.1304\n",
      "Epoch 33/500\n",
      " - 0s - loss: 3.0542 - acc: 0.1304\n",
      "Epoch 34/500\n",
      " - 0s - loss: 3.0523 - acc: 0.1304\n",
      "Epoch 35/500\n",
      " - 0s - loss: 3.0504 - acc: 0.1304\n",
      "Epoch 36/500\n",
      " - 0s - loss: 3.0483 - acc: 0.1304\n",
      "Epoch 37/500\n",
      " - 0s - loss: 3.0462 - acc: 0.1304\n",
      "Epoch 38/500\n",
      " - 0s - loss: 3.0440 - acc: 0.1304\n",
      "Epoch 39/500\n",
      " - 0s - loss: 3.0418 - acc: 0.1304\n",
      "Epoch 40/500\n",
      " - 0s - loss: 3.0394 - acc: 0.1739\n",
      "Epoch 41/500\n",
      " - 0s - loss: 3.0369 - acc: 0.1739\n",
      "Epoch 42/500\n",
      " - 0s - loss: 3.0343 - acc: 0.1739\n",
      "Epoch 43/500\n",
      " - 0s - loss: 3.0317 - acc: 0.1739\n",
      "Epoch 44/500\n",
      " - 0s - loss: 3.0289 - acc: 0.1739\n",
      "Epoch 45/500\n",
      " - 0s - loss: 3.0260 - acc: 0.1739\n",
      "Epoch 46/500\n",
      " - 0s - loss: 3.0231 - acc: 0.1739\n",
      "Epoch 47/500\n",
      " - 0s - loss: 3.0200 - acc: 0.1739\n",
      "Epoch 48/500\n",
      " - 0s - loss: 3.0167 - acc: 0.1739\n",
      "Epoch 49/500\n",
      " - 0s - loss: 3.0134 - acc: 0.1739\n",
      "Epoch 50/500\n",
      " - 0s - loss: 3.0099 - acc: 0.1739\n",
      "Epoch 51/500\n",
      " - 0s - loss: 3.0063 - acc: 0.1739\n",
      "Epoch 52/500\n",
      " - 0s - loss: 3.0025 - acc: 0.1739\n",
      "Epoch 53/500\n",
      " - 0s - loss: 2.9986 - acc: 0.1739\n",
      "Epoch 54/500\n",
      " - 0s - loss: 2.9945 - acc: 0.1739\n",
      "Epoch 55/500\n",
      " - 0s - loss: 2.9903 - acc: 0.1739\n",
      "Epoch 56/500\n",
      " - 0s - loss: 2.9859 - acc: 0.1739\n",
      "Epoch 57/500\n",
      " - 0s - loss: 2.9814 - acc: 0.1739\n",
      "Epoch 58/500\n",
      " - 0s - loss: 2.9766 - acc: 0.1739\n",
      "Epoch 59/500\n",
      " - 0s - loss: 2.9717 - acc: 0.1739\n",
      "Epoch 60/500\n",
      " - 0s - loss: 2.9665 - acc: 0.1739\n",
      "Epoch 61/500\n",
      " - 0s - loss: 2.9612 - acc: 0.1739\n",
      "Epoch 62/500\n",
      " - 0s - loss: 2.9557 - acc: 0.1739\n",
      "Epoch 63/500\n",
      " - 0s - loss: 2.9500 - acc: 0.1739\n",
      "Epoch 64/500\n",
      " - 0s - loss: 2.9440 - acc: 0.1739\n",
      "Epoch 65/500\n",
      " - 0s - loss: 2.9378 - acc: 0.1739\n",
      "Epoch 66/500\n",
      " - 0s - loss: 2.9314 - acc: 0.1739\n",
      "Epoch 67/500\n",
      " - 0s - loss: 2.9248 - acc: 0.1739\n",
      "Epoch 68/500\n",
      " - 0s - loss: 2.9179 - acc: 0.1739\n",
      "Epoch 69/500\n",
      " - 0s - loss: 2.9107 - acc: 0.1739\n",
      "Epoch 70/500\n",
      " - 0s - loss: 2.9033 - acc: 0.1739\n",
      "Epoch 71/500\n",
      " - 0s - loss: 2.8955 - acc: 0.1739\n",
      "Epoch 72/500\n",
      " - 0s - loss: 2.8875 - acc: 0.1739\n",
      "Epoch 73/500\n",
      " - 0s - loss: 2.8793 - acc: 0.1739\n",
      "Epoch 74/500\n",
      " - 0s - loss: 2.8707 - acc: 0.1739\n",
      "Epoch 75/500\n",
      " - 0s - loss: 2.8618 - acc: 0.1739\n",
      "Epoch 76/500\n",
      " - 0s - loss: 2.8525 - acc: 0.1739\n",
      "Epoch 77/500\n",
      " - 0s - loss: 2.8430 - acc: 0.1739\n",
      "Epoch 78/500\n",
      " - 0s - loss: 2.8331 - acc: 0.1739\n",
      "Epoch 79/500\n",
      " - 0s - loss: 2.8229 - acc: 0.1739\n",
      "Epoch 80/500\n",
      " - 0s - loss: 2.8123 - acc: 0.1739\n",
      "Epoch 81/500\n",
      " - 0s - loss: 2.8013 - acc: 0.1739\n",
      "Epoch 82/500\n",
      " - 0s - loss: 2.7900 - acc: 0.1739\n",
      "Epoch 83/500\n",
      " - 0s - loss: 2.7783 - acc: 0.1739\n",
      "Epoch 84/500\n",
      " - 0s - loss: 2.7662 - acc: 0.1739\n",
      "Epoch 85/500\n",
      " - 0s - loss: 2.7537 - acc: 0.1739\n",
      "Epoch 86/500\n",
      " - 0s - loss: 2.7408 - acc: 0.1739\n",
      "Epoch 87/500\n",
      " - 0s - loss: 2.7276 - acc: 0.1739\n",
      "Epoch 88/500\n",
      " - 0s - loss: 2.7139 - acc: 0.1739\n",
      "Epoch 89/500\n",
      " - 0s - loss: 2.6998 - acc: 0.1739\n",
      "Epoch 90/500\n",
      " - 0s - loss: 2.6853 - acc: 0.1739\n",
      "Epoch 91/500\n",
      " - 0s - loss: 2.6703 - acc: 0.1739\n",
      "Epoch 92/500\n",
      " - 0s - loss: 2.6549 - acc: 0.1739\n",
      "Epoch 93/500\n",
      " - 0s - loss: 2.6390 - acc: 0.1739\n",
      "Epoch 94/500\n",
      " - 0s - loss: 2.6228 - acc: 0.1739\n",
      "Epoch 95/500\n",
      " - 0s - loss: 2.6061 - acc: 0.1739\n",
      "Epoch 96/500\n",
      " - 0s - loss: 2.5889 - acc: 0.2609\n",
      "Epoch 97/500\n",
      " - 0s - loss: 2.5714 - acc: 0.2609\n",
      "Epoch 98/500\n",
      " - 0s - loss: 2.5533 - acc: 0.2609\n",
      "Epoch 99/500\n",
      " - 0s - loss: 2.5349 - acc: 0.2609\n",
      "Epoch 100/500\n",
      " - 0s - loss: 2.5160 - acc: 0.2609\n",
      "Epoch 101/500\n",
      " - 0s - loss: 2.4966 - acc: 0.3043\n",
      "Epoch 102/500\n",
      " - 0s - loss: 2.4768 - acc: 0.3043\n",
      "Epoch 103/500\n",
      " - 0s - loss: 2.4567 - acc: 0.3043\n",
      "Epoch 104/500\n",
      " - 0s - loss: 2.4361 - acc: 0.3913\n",
      "Epoch 105/500\n",
      " - 0s - loss: 2.4151 - acc: 0.3913\n",
      "Epoch 106/500\n",
      " - 0s - loss: 2.3937 - acc: 0.3913\n",
      "Epoch 107/500\n",
      " - 0s - loss: 2.3720 - acc: 0.3913\n",
      "Epoch 108/500\n",
      " - 0s - loss: 2.3498 - acc: 0.3913\n",
      "Epoch 109/500\n",
      " - 0s - loss: 2.3273 - acc: 0.3913\n",
      "Epoch 110/500\n",
      " - 0s - loss: 2.3044 - acc: 0.3913\n",
      "Epoch 111/500\n",
      " - 0s - loss: 2.2813 - acc: 0.3913\n",
      "Epoch 112/500\n",
      " - 0s - loss: 2.2579 - acc: 0.3913\n",
      "Epoch 113/500\n",
      " - 0s - loss: 2.2342 - acc: 0.4348\n",
      "Epoch 114/500\n",
      " - 0s - loss: 2.2102 - acc: 0.4348\n",
      "Epoch 115/500\n",
      " - 0s - loss: 2.1860 - acc: 0.4348\n",
      "Epoch 116/500\n",
      " - 0s - loss: 2.1615 - acc: 0.4348\n",
      "Epoch 117/500\n",
      " - 0s - loss: 2.1368 - acc: 0.4348\n",
      "Epoch 118/500\n",
      " - 0s - loss: 2.1118 - acc: 0.4348\n",
      "Epoch 119/500\n",
      " - 0s - loss: 2.0867 - acc: 0.4348\n",
      "Epoch 120/500\n",
      " - 0s - loss: 2.0614 - acc: 0.4348\n",
      "Epoch 121/500\n",
      " - 0s - loss: 2.0360 - acc: 0.4348\n",
      "Epoch 122/500\n",
      " - 0s - loss: 2.0103 - acc: 0.4348\n",
      "Epoch 123/500\n",
      " - 0s - loss: 1.9846 - acc: 0.4348\n",
      "Epoch 124/500\n",
      " - 0s - loss: 1.9587 - acc: 0.4348\n",
      "Epoch 125/500\n",
      " - 0s - loss: 1.9327 - acc: 0.5217\n",
      "Epoch 126/500\n",
      " - 0s - loss: 1.9066 - acc: 0.5652\n",
      "Epoch 127/500\n",
      " - 0s - loss: 1.8805 - acc: 0.6087\n",
      "Epoch 128/500\n",
      " - 0s - loss: 1.8542 - acc: 0.6087\n",
      "Epoch 129/500\n",
      " - 0s - loss: 1.8280 - acc: 0.6087\n",
      "Epoch 130/500\n",
      " - 0s - loss: 1.8017 - acc: 0.6087\n",
      "Epoch 131/500\n",
      " - 0s - loss: 1.7753 - acc: 0.6087\n",
      "Epoch 132/500\n",
      " - 0s - loss: 1.7488 - acc: 0.6087\n",
      "Epoch 133/500\n",
      " - 0s - loss: 1.7225 - acc: 0.6957\n",
      "Epoch 134/500\n",
      " - 0s - loss: 1.6961 - acc: 0.6957\n",
      "Epoch 135/500\n",
      " - 0s - loss: 1.6697 - acc: 0.6957\n",
      "Epoch 136/500\n",
      " - 0s - loss: 1.6434 - acc: 0.6957\n",
      "Epoch 137/500\n",
      " - 0s - loss: 1.6171 - acc: 0.6957\n",
      "Epoch 138/500\n",
      " - 0s - loss: 1.5908 - acc: 0.6957\n",
      "Epoch 139/500\n",
      " - 0s - loss: 1.5647 - acc: 0.6957\n",
      "Epoch 140/500\n",
      " - 0s - loss: 1.5386 - acc: 0.7391\n",
      "Epoch 141/500\n",
      " - 0s - loss: 1.5126 - acc: 0.7391\n",
      "Epoch 142/500\n",
      " - 0s - loss: 1.4866 - acc: 0.7826\n",
      "Epoch 143/500\n",
      " - 0s - loss: 1.4609 - acc: 0.8261\n",
      "Epoch 144/500\n",
      " - 0s - loss: 1.4352 - acc: 0.8261\n",
      "Epoch 145/500\n",
      " - 0s - loss: 1.4097 - acc: 0.8261\n",
      "Epoch 146/500\n",
      " - 0s - loss: 1.3843 - acc: 0.8261\n",
      "Epoch 147/500\n",
      " - 0s - loss: 1.3591 - acc: 0.8261\n",
      "Epoch 148/500\n",
      " - 0s - loss: 1.3341 - acc: 0.8696\n",
      "Epoch 149/500\n",
      " - 0s - loss: 1.3092 - acc: 0.8696\n",
      "Epoch 150/500\n",
      " - 0s - loss: 1.2845 - acc: 0.8696\n",
      "Epoch 151/500\n",
      " - 0s - loss: 1.2601 - acc: 0.8696\n",
      "Epoch 152/500\n",
      " - 0s - loss: 1.2359 - acc: 0.8696\n",
      "Epoch 153/500\n",
      " - 0s - loss: 1.2119 - acc: 0.8696\n",
      "Epoch 154/500\n",
      " - 0s - loss: 1.1881 - acc: 0.8696\n",
      "Epoch 155/500\n",
      " - 0s - loss: 1.1646 - acc: 0.8696\n",
      "Epoch 156/500\n",
      " - 0s - loss: 1.1413 - acc: 0.8696\n",
      "Epoch 157/500\n",
      " - 0s - loss: 1.1182 - acc: 0.8696\n",
      "Epoch 158/500\n",
      " - 0s - loss: 1.0955 - acc: 0.8696\n",
      "Epoch 159/500\n",
      " - 0s - loss: 1.0731 - acc: 0.8696\n",
      "Epoch 160/500\n",
      " - 0s - loss: 1.0509 - acc: 0.8696\n",
      "Epoch 161/500\n",
      " - 0s - loss: 1.0290 - acc: 0.8696\n",
      "Epoch 162/500\n",
      " - 0s - loss: 1.0074 - acc: 0.8696\n",
      "Epoch 163/500\n",
      " - 0s - loss: 0.9861 - acc: 0.8696\n",
      "Epoch 164/500\n",
      " - 0s - loss: 0.9651 - acc: 0.8696\n",
      "Epoch 165/500\n",
      " - 0s - loss: 0.9444 - acc: 0.8696\n",
      "Epoch 166/500\n",
      " - 0s - loss: 0.9239 - acc: 0.8696\n",
      "Epoch 167/500\n",
      " - 0s - loss: 0.9038 - acc: 0.8696\n",
      "Epoch 168/500\n",
      " - 0s - loss: 0.8839 - acc: 0.8696\n",
      "Epoch 169/500\n",
      " - 0s - loss: 0.8644 - acc: 0.9130\n",
      "Epoch 170/500\n",
      " - 0s - loss: 0.8451 - acc: 0.9130\n",
      "Epoch 171/500\n",
      " - 0s - loss: 0.8261 - acc: 0.9130\n",
      "Epoch 172/500\n",
      " - 0s - loss: 0.8074 - acc: 0.9565\n",
      "Epoch 173/500\n",
      " - 0s - loss: 0.7891 - acc: 0.9565\n",
      "Epoch 174/500\n",
      " - 0s - loss: 0.7710 - acc: 0.9565\n",
      "Epoch 175/500\n",
      " - 0s - loss: 0.7533 - acc: 0.9565\n",
      "Epoch 176/500\n",
      " - 0s - loss: 0.7359 - acc: 0.9565\n",
      "Epoch 177/500\n",
      " - 0s - loss: 0.7188 - acc: 0.9565\n",
      "Epoch 178/500\n",
      " - 0s - loss: 0.7021 - acc: 0.9565\n",
      "Epoch 179/500\n",
      " - 0s - loss: 0.6856 - acc: 0.9565\n",
      "Epoch 180/500\n",
      " - 0s - loss: 0.6695 - acc: 0.9565\n",
      "Epoch 181/500\n",
      " - 0s - loss: 0.6537 - acc: 0.9565\n",
      "Epoch 182/500\n",
      " - 0s - loss: 0.6381 - acc: 0.9565\n",
      "Epoch 183/500\n",
      " - 0s - loss: 0.6229 - acc: 0.9565\n",
      "Epoch 184/500\n",
      " - 0s - loss: 0.6079 - acc: 0.9565\n",
      "Epoch 185/500\n",
      " - 0s - loss: 0.5933 - acc: 0.9565\n",
      "Epoch 186/500\n",
      " - 0s - loss: 0.5790 - acc: 0.9565\n",
      "Epoch 187/500\n",
      " - 0s - loss: 0.5651 - acc: 0.9565\n",
      "Epoch 188/500\n",
      " - 0s - loss: 0.5514 - acc: 0.9565\n",
      "Epoch 189/500\n",
      " - 0s - loss: 0.5380 - acc: 0.9565\n",
      "Epoch 190/500\n",
      " - 0s - loss: 0.5249 - acc: 0.9565\n",
      "Epoch 191/500\n",
      " - 0s - loss: 0.5122 - acc: 0.9565\n",
      "Epoch 192/500\n",
      " - 0s - loss: 0.4997 - acc: 0.9565\n",
      "Epoch 193/500\n",
      " - 0s - loss: 0.4876 - acc: 0.9565\n",
      "Epoch 194/500\n",
      " - 0s - loss: 0.4757 - acc: 0.9565\n",
      "Epoch 195/500\n",
      " - 0s - loss: 0.4640 - acc: 0.9565\n",
      "Epoch 196/500\n",
      " - 0s - loss: 0.4527 - acc: 0.9565\n",
      "Epoch 197/500\n",
      " - 0s - loss: 0.4416 - acc: 0.9565\n",
      "Epoch 198/500\n",
      " - 0s - loss: 0.4308 - acc: 0.9565\n",
      "Epoch 199/500\n",
      " - 0s - loss: 0.4203 - acc: 0.9565\n",
      "Epoch 200/500\n",
      " - 0s - loss: 0.4100 - acc: 0.9565\n",
      "Epoch 201/500\n",
      " - 0s - loss: 0.4000 - acc: 0.9565\n",
      "Epoch 202/500\n",
      " - 0s - loss: 0.3903 - acc: 0.9565\n",
      "Epoch 203/500\n",
      " - 0s - loss: 0.3808 - acc: 0.9565\n",
      "Epoch 204/500\n",
      " - 0s - loss: 0.3716 - acc: 0.9565\n",
      "Epoch 205/500\n",
      " - 0s - loss: 0.3626 - acc: 0.9565\n",
      "Epoch 206/500\n",
      " - 0s - loss: 0.3540 - acc: 0.9565\n",
      "Epoch 207/500\n",
      " - 0s - loss: 0.3456 - acc: 0.9565\n",
      "Epoch 208/500\n",
      " - 0s - loss: 0.3374 - acc: 0.9565\n",
      "Epoch 209/500\n",
      " - 0s - loss: 0.3294 - acc: 0.9565\n",
      "Epoch 210/500\n",
      " - 0s - loss: 0.3217 - acc: 0.9565\n",
      "Epoch 211/500\n",
      " - 0s - loss: 0.3142 - acc: 0.9565\n",
      "Epoch 212/500\n",
      " - 0s - loss: 0.3069 - acc: 0.9565\n",
      "Epoch 213/500\n",
      " - 0s - loss: 0.2999 - acc: 0.9565\n",
      "Epoch 214/500\n",
      " - 0s - loss: 0.2931 - acc: 0.9565\n",
      "Epoch 215/500\n",
      " - 0s - loss: 0.2865 - acc: 0.9565\n",
      "Epoch 216/500\n",
      " - 0s - loss: 0.2801 - acc: 0.9565\n",
      "Epoch 217/500\n",
      " - 0s - loss: 0.2740 - acc: 0.9565\n",
      "Epoch 218/500\n",
      " - 0s - loss: 0.2680 - acc: 0.9565\n",
      "Epoch 219/500\n",
      " - 0s - loss: 0.2622 - acc: 0.9565\n",
      "Epoch 220/500\n",
      " - 0s - loss: 0.2567 - acc: 0.9565\n",
      "Epoch 221/500\n",
      " - 0s - loss: 0.2513 - acc: 0.9565\n",
      "Epoch 222/500\n",
      " - 0s - loss: 0.2460 - acc: 0.9565\n",
      "Epoch 223/500\n",
      " - 0s - loss: 0.2410 - acc: 0.9565\n",
      "Epoch 224/500\n",
      " - 0s - loss: 0.2361 - acc: 0.9565\n",
      "Epoch 225/500\n",
      " - 0s - loss: 0.2314 - acc: 0.9565\n",
      "Epoch 226/500\n",
      " - 0s - loss: 0.2269 - acc: 0.9565\n",
      "Epoch 227/500\n",
      " - 0s - loss: 0.2224 - acc: 0.9565\n",
      "Epoch 228/500\n",
      " - 0s - loss: 0.2182 - acc: 0.9565\n",
      "Epoch 229/500\n",
      " - 0s - loss: 0.2140 - acc: 0.9565\n",
      "Epoch 230/500\n",
      " - 0s - loss: 0.2100 - acc: 0.9565\n",
      "Epoch 231/500\n",
      " - 0s - loss: 0.2062 - acc: 0.9565\n",
      "Epoch 232/500\n",
      " - 0s - loss: 0.2025 - acc: 0.9565\n",
      "Epoch 233/500\n",
      " - 0s - loss: 0.1989 - acc: 0.9565\n",
      "Epoch 234/500\n",
      " - 0s - loss: 0.1954 - acc: 0.9565\n",
      "Epoch 235/500\n",
      " - 0s - loss: 0.1921 - acc: 0.9565\n",
      "Epoch 236/500\n",
      " - 0s - loss: 0.1888 - acc: 0.9565\n",
      "Epoch 237/500\n",
      " - 0s - loss: 0.1857 - acc: 0.9565\n",
      "Epoch 238/500\n",
      " - 0s - loss: 0.1826 - acc: 0.9565\n",
      "Epoch 239/500\n",
      " - 0s - loss: 0.1797 - acc: 0.9565\n",
      "Epoch 240/500\n",
      " - 0s - loss: 0.1768 - acc: 0.9565\n",
      "Epoch 241/500\n",
      " - 0s - loss: 0.1741 - acc: 0.9565\n",
      "Epoch 242/500\n",
      " - 0s - loss: 0.1714 - acc: 0.9565\n",
      "Epoch 243/500\n",
      " - 0s - loss: 0.1689 - acc: 0.9565\n",
      "Epoch 244/500\n",
      " - 0s - loss: 0.1664 - acc: 0.9565\n",
      "Epoch 245/500\n",
      " - 0s - loss: 0.1640 - acc: 0.9565\n",
      "Epoch 246/500\n",
      " - 0s - loss: 0.1616 - acc: 0.9565\n",
      "Epoch 247/500\n",
      " - 0s - loss: 0.1594 - acc: 0.9565\n",
      "Epoch 248/500\n",
      " - 0s - loss: 0.1572 - acc: 0.9565\n",
      "Epoch 249/500\n",
      " - 0s - loss: 0.1551 - acc: 0.9565\n",
      "Epoch 250/500\n",
      " - 0s - loss: 0.1530 - acc: 0.9565\n",
      "Epoch 251/500\n",
      " - 0s - loss: 0.1511 - acc: 0.9565\n",
      "Epoch 252/500\n",
      " - 0s - loss: 0.1491 - acc: 0.9565\n",
      "Epoch 253/500\n",
      " - 0s - loss: 0.1473 - acc: 0.9565\n",
      "Epoch 254/500\n",
      " - 0s - loss: 0.1455 - acc: 0.9565\n",
      "Epoch 255/500\n",
      " - 0s - loss: 0.1438 - acc: 0.9565\n",
      "Epoch 256/500\n",
      " - 0s - loss: 0.1421 - acc: 0.9565\n",
      "Epoch 257/500\n",
      " - 0s - loss: 0.1405 - acc: 0.9565\n",
      "Epoch 258/500\n",
      " - 0s - loss: 0.1390 - acc: 0.9565\n",
      "Epoch 259/500\n",
      " - 0s - loss: 0.1374 - acc: 0.9565\n",
      "Epoch 260/500\n",
      " - 0s - loss: 0.1359 - acc: 0.9565\n",
      "Epoch 261/500\n",
      " - 0s - loss: 0.1345 - acc: 0.9565\n",
      "Epoch 262/500\n",
      " - 0s - loss: 0.1331 - acc: 0.9565\n",
      "Epoch 263/500\n",
      " - 0s - loss: 0.1318 - acc: 0.9565\n",
      "Epoch 264/500\n",
      " - 0s - loss: 0.1305 - acc: 0.9565\n",
      "Epoch 265/500\n",
      " - 0s - loss: 0.1292 - acc: 0.9565\n",
      "Epoch 266/500\n",
      " - 0s - loss: 0.1280 - acc: 0.9565\n",
      "Epoch 267/500\n",
      " - 0s - loss: 0.1268 - acc: 0.9565\n",
      "Epoch 268/500\n",
      " - 0s - loss: 0.1256 - acc: 0.9565\n",
      "Epoch 269/500\n",
      " - 0s - loss: 0.1245 - acc: 0.9565\n",
      "Epoch 270/500\n",
      " - 0s - loss: 0.1234 - acc: 0.9565\n",
      "Epoch 271/500\n",
      " - 0s - loss: 0.1224 - acc: 0.9565\n",
      "Epoch 272/500\n",
      " - 0s - loss: 0.1213 - acc: 0.9565\n",
      "Epoch 273/500\n",
      " - 0s - loss: 0.1203 - acc: 0.9565\n",
      "Epoch 274/500\n",
      " - 0s - loss: 0.1193 - acc: 0.9565\n",
      "Epoch 275/500\n",
      " - 0s - loss: 0.1184 - acc: 0.9565\n",
      "Epoch 276/500\n",
      " - 0s - loss: 0.1175 - acc: 0.9565\n",
      "Epoch 277/500\n",
      " - 0s - loss: 0.1166 - acc: 0.9565\n",
      "Epoch 278/500\n",
      " - 0s - loss: 0.1157 - acc: 0.9565\n",
      "Epoch 279/500\n",
      " - 0s - loss: 0.1149 - acc: 0.9565\n",
      "Epoch 280/500\n",
      " - 0s - loss: 0.1140 - acc: 0.9565\n",
      "Epoch 281/500\n",
      " - 0s - loss: 0.1132 - acc: 0.9565\n",
      "Epoch 282/500\n",
      " - 0s - loss: 0.1124 - acc: 0.9565\n",
      "Epoch 283/500\n",
      " - 0s - loss: 0.1117 - acc: 0.9565\n",
      "Epoch 284/500\n",
      " - 0s - loss: 0.1109 - acc: 0.9565\n",
      "Epoch 285/500\n",
      " - 0s - loss: 0.1102 - acc: 0.9565\n",
      "Epoch 286/500\n",
      " - 0s - loss: 0.1095 - acc: 0.9565\n",
      "Epoch 287/500\n",
      " - 0s - loss: 0.1088 - acc: 0.9565\n",
      "Epoch 288/500\n",
      " - 0s - loss: 0.1081 - acc: 0.9565\n",
      "Epoch 289/500\n",
      " - 0s - loss: 0.1075 - acc: 0.9565\n",
      "Epoch 290/500\n",
      " - 0s - loss: 0.1068 - acc: 0.9565\n",
      "Epoch 291/500\n",
      " - 0s - loss: 0.1062 - acc: 0.9565\n",
      "Epoch 292/500\n",
      " - 0s - loss: 0.1056 - acc: 0.9565\n",
      "Epoch 293/500\n",
      " - 0s - loss: 0.1050 - acc: 0.9565\n",
      "Epoch 294/500\n",
      " - 0s - loss: 0.1044 - acc: 0.9565\n",
      "Epoch 295/500\n",
      " - 0s - loss: 0.1038 - acc: 0.9565\n",
      "Epoch 296/500\n",
      " - 0s - loss: 0.1033 - acc: 0.9565\n",
      "Epoch 297/500\n",
      " - 0s - loss: 0.1027 - acc: 0.9565\n",
      "Epoch 298/500\n",
      " - 0s - loss: 0.1022 - acc: 0.9565\n",
      "Epoch 299/500\n",
      " - 0s - loss: 0.1016 - acc: 0.9565\n",
      "Epoch 300/500\n",
      " - 0s - loss: 0.1011 - acc: 0.9565\n",
      "Epoch 301/500\n",
      " - 0s - loss: 0.1006 - acc: 0.9565\n",
      "Epoch 302/500\n",
      " - 0s - loss: 0.1001 - acc: 0.9565\n",
      "Epoch 303/500\n",
      " - 0s - loss: 0.0996 - acc: 0.9565\n",
      "Epoch 304/500\n",
      " - 0s - loss: 0.0992 - acc: 0.9565\n",
      "Epoch 305/500\n",
      " - 0s - loss: 0.0987 - acc: 0.9565\n",
      "Epoch 306/500\n",
      " - 0s - loss: 0.0983 - acc: 0.9565\n",
      "Epoch 307/500\n",
      " - 0s - loss: 0.0978 - acc: 0.9565\n",
      "Epoch 308/500\n",
      " - 0s - loss: 0.0974 - acc: 0.9565\n",
      "Epoch 309/500\n",
      " - 0s - loss: 0.0970 - acc: 0.9565\n",
      "Epoch 310/500\n",
      " - 0s - loss: 0.0965 - acc: 0.9565\n",
      "Epoch 311/500\n",
      " - 0s - loss: 0.0961 - acc: 0.9565\n",
      "Epoch 312/500\n",
      " - 0s - loss: 0.0957 - acc: 0.9565\n",
      "Epoch 313/500\n",
      " - 0s - loss: 0.0953 - acc: 0.9565\n",
      "Epoch 314/500\n",
      " - 0s - loss: 0.0949 - acc: 0.9565\n",
      "Epoch 315/500\n",
      " - 0s - loss: 0.0946 - acc: 0.9565\n",
      "Epoch 316/500\n",
      " - 0s - loss: 0.0942 - acc: 0.9565\n",
      "Epoch 317/500\n",
      " - 0s - loss: 0.0938 - acc: 0.9565\n",
      "Epoch 318/500\n",
      " - 0s - loss: 0.0935 - acc: 0.9565\n",
      "Epoch 319/500\n",
      " - 0s - loss: 0.0931 - acc: 0.9565\n",
      "Epoch 320/500\n",
      " - 0s - loss: 0.0928 - acc: 0.9565\n",
      "Epoch 321/500\n",
      " - 0s - loss: 0.0924 - acc: 0.9565\n",
      "Epoch 322/500\n",
      " - 0s - loss: 0.0921 - acc: 0.9565\n",
      "Epoch 323/500\n",
      " - 0s - loss: 0.0918 - acc: 0.9565\n",
      "Epoch 324/500\n",
      " - 0s - loss: 0.0915 - acc: 0.9565\n",
      "Epoch 325/500\n",
      " - 0s - loss: 0.0911 - acc: 0.9565\n",
      "Epoch 326/500\n",
      " - 0s - loss: 0.0908 - acc: 0.9565\n",
      "Epoch 327/500\n",
      " - 0s - loss: 0.0905 - acc: 0.9565\n",
      "Epoch 328/500\n",
      " - 0s - loss: 0.0902 - acc: 0.9565\n",
      "Epoch 329/500\n",
      " - 0s - loss: 0.0899 - acc: 0.9565\n",
      "Epoch 330/500\n",
      " - 0s - loss: 0.0896 - acc: 0.9565\n",
      "Epoch 331/500\n",
      " - 0s - loss: 0.0894 - acc: 0.9565\n",
      "Epoch 332/500\n",
      " - 0s - loss: 0.0891 - acc: 0.9565\n",
      "Epoch 333/500\n",
      " - 0s - loss: 0.0888 - acc: 0.9565\n",
      "Epoch 334/500\n",
      " - 0s - loss: 0.0885 - acc: 0.9565\n",
      "Epoch 335/500\n",
      " - 0s - loss: 0.0883 - acc: 0.9565\n",
      "Epoch 336/500\n",
      " - 0s - loss: 0.0880 - acc: 0.9565\n",
      "Epoch 337/500\n",
      " - 0s - loss: 0.0877 - acc: 0.9565\n",
      "Epoch 338/500\n",
      " - 0s - loss: 0.0875 - acc: 0.9565\n",
      "Epoch 339/500\n",
      " - 0s - loss: 0.0872 - acc: 0.9565\n",
      "Epoch 340/500\n",
      " - 0s - loss: 0.0870 - acc: 0.9565\n",
      "Epoch 341/500\n",
      " - 0s - loss: 0.0867 - acc: 0.9565\n",
      "Epoch 342/500\n",
      " - 0s - loss: 0.0865 - acc: 0.9565\n",
      "Epoch 343/500\n",
      " - 0s - loss: 0.0863 - acc: 0.9565\n",
      "Epoch 344/500\n",
      " - 0s - loss: 0.0860 - acc: 0.9565\n",
      "Epoch 345/500\n",
      " - 0s - loss: 0.0858 - acc: 0.9565\n",
      "Epoch 346/500\n",
      " - 0s - loss: 0.0856 - acc: 0.9565\n",
      "Epoch 347/500\n",
      " - 0s - loss: 0.0854 - acc: 0.9565\n",
      "Epoch 348/500\n",
      " - 0s - loss: 0.0851 - acc: 0.9565\n",
      "Epoch 349/500\n",
      " - 0s - loss: 0.0849 - acc: 0.9565\n",
      "Epoch 350/500\n",
      " - 0s - loss: 0.0847 - acc: 0.9565\n",
      "Epoch 351/500\n",
      " - 0s - loss: 0.0845 - acc: 0.9565\n",
      "Epoch 352/500\n",
      " - 0s - loss: 0.0843 - acc: 0.9565\n",
      "Epoch 353/500\n",
      " - 0s - loss: 0.0841 - acc: 0.9565\n",
      "Epoch 354/500\n",
      " - 0s - loss: 0.0839 - acc: 0.9565\n",
      "Epoch 355/500\n",
      " - 0s - loss: 0.0837 - acc: 0.9565\n",
      "Epoch 356/500\n",
      " - 0s - loss: 0.0835 - acc: 0.9565\n",
      "Epoch 357/500\n",
      " - 0s - loss: 0.0833 - acc: 0.9565\n",
      "Epoch 358/500\n",
      " - 0s - loss: 0.0831 - acc: 0.9565\n",
      "Epoch 359/500\n",
      " - 0s - loss: 0.0829 - acc: 0.9565\n",
      "Epoch 360/500\n",
      " - 0s - loss: 0.0827 - acc: 0.9565\n",
      "Epoch 361/500\n",
      " - 0s - loss: 0.0825 - acc: 0.9565\n",
      "Epoch 362/500\n",
      " - 0s - loss: 0.0824 - acc: 0.9565\n",
      "Epoch 363/500\n",
      " - 0s - loss: 0.0822 - acc: 0.9565\n",
      "Epoch 364/500\n",
      " - 0s - loss: 0.0820 - acc: 0.9565\n",
      "Epoch 365/500\n",
      " - 0s - loss: 0.0818 - acc: 0.9565\n",
      "Epoch 366/500\n",
      " - 0s - loss: 0.0817 - acc: 0.9565\n",
      "Epoch 367/500\n",
      " - 0s - loss: 0.0815 - acc: 0.9565\n",
      "Epoch 368/500\n",
      " - 0s - loss: 0.0813 - acc: 0.9565\n",
      "Epoch 369/500\n",
      " - 0s - loss: 0.0812 - acc: 0.9565\n",
      "Epoch 370/500\n",
      " - 0s - loss: 0.0810 - acc: 0.9565\n",
      "Epoch 371/500\n",
      " - 0s - loss: 0.0808 - acc: 0.9565\n",
      "Epoch 372/500\n",
      " - 0s - loss: 0.0807 - acc: 0.9565\n",
      "Epoch 373/500\n",
      " - 0s - loss: 0.0805 - acc: 0.9565\n",
      "Epoch 374/500\n",
      " - 0s - loss: 0.0804 - acc: 0.9565\n",
      "Epoch 375/500\n",
      " - 0s - loss: 0.0802 - acc: 0.9565\n",
      "Epoch 376/500\n",
      " - 0s - loss: 0.0801 - acc: 0.9565\n",
      "Epoch 377/500\n",
      " - 0s - loss: 0.0799 - acc: 0.9565\n",
      "Epoch 378/500\n",
      " - 0s - loss: 0.0798 - acc: 0.9565\n",
      "Epoch 379/500\n",
      " - 0s - loss: 0.0796 - acc: 0.9565\n",
      "Epoch 380/500\n",
      " - 0s - loss: 0.0795 - acc: 0.9565\n",
      "Epoch 381/500\n",
      " - 0s - loss: 0.0793 - acc: 0.9565\n",
      "Epoch 382/500\n",
      " - 0s - loss: 0.0792 - acc: 0.9565\n",
      "Epoch 383/500\n",
      " - 0s - loss: 0.0790 - acc: 0.9565\n",
      "Epoch 384/500\n",
      " - 0s - loss: 0.0789 - acc: 0.9565\n",
      "Epoch 385/500\n",
      " - 0s - loss: 0.0788 - acc: 0.9565\n",
      "Epoch 386/500\n",
      " - 0s - loss: 0.0786 - acc: 0.9565\n",
      "Epoch 387/500\n",
      " - 0s - loss: 0.0785 - acc: 0.9565\n",
      "Epoch 388/500\n",
      " - 0s - loss: 0.0784 - acc: 0.9565\n",
      "Epoch 389/500\n",
      " - 0s - loss: 0.0783 - acc: 0.9565\n",
      "Epoch 390/500\n",
      " - 0s - loss: 0.0781 - acc: 0.9565\n",
      "Epoch 391/500\n",
      " - 0s - loss: 0.0780 - acc: 0.9565\n",
      "Epoch 392/500\n",
      " - 0s - loss: 0.0779 - acc: 0.9565\n",
      "Epoch 393/500\n",
      " - 0s - loss: 0.0778 - acc: 0.9565\n",
      "Epoch 394/500\n",
      " - 0s - loss: 0.0776 - acc: 0.9565\n",
      "Epoch 395/500\n",
      " - 0s - loss: 0.0775 - acc: 0.9565\n",
      "Epoch 396/500\n",
      " - 0s - loss: 0.0774 - acc: 0.9565\n",
      "Epoch 397/500\n",
      " - 0s - loss: 0.0773 - acc: 0.9565\n",
      "Epoch 398/500\n",
      " - 0s - loss: 0.0772 - acc: 0.9565\n",
      "Epoch 399/500\n",
      " - 0s - loss: 0.0770 - acc: 0.9565\n",
      "Epoch 400/500\n",
      " - 0s - loss: 0.0769 - acc: 0.9565\n",
      "Epoch 401/500\n",
      " - 0s - loss: 0.0768 - acc: 0.9565\n",
      "Epoch 402/500\n",
      " - 0s - loss: 0.0767 - acc: 0.9565\n",
      "Epoch 403/500\n",
      " - 0s - loss: 0.0766 - acc: 0.9565\n",
      "Epoch 404/500\n",
      " - 0s - loss: 0.0765 - acc: 0.9565\n",
      "Epoch 405/500\n",
      " - 0s - loss: 0.0764 - acc: 0.9565\n",
      "Epoch 406/500\n",
      " - 0s - loss: 0.0763 - acc: 0.9565\n",
      "Epoch 407/500\n",
      " - 0s - loss: 0.0762 - acc: 0.9565\n",
      "Epoch 408/500\n",
      " - 0s - loss: 0.0761 - acc: 0.9565\n",
      "Epoch 409/500\n",
      " - 0s - loss: 0.0760 - acc: 0.9565\n",
      "Epoch 410/500\n",
      " - 0s - loss: 0.0759 - acc: 0.9565\n",
      "Epoch 411/500\n",
      " - 0s - loss: 0.0758 - acc: 0.9565\n",
      "Epoch 412/500\n",
      " - 0s - loss: 0.0757 - acc: 0.9565\n",
      "Epoch 413/500\n",
      " - 0s - loss: 0.0756 - acc: 0.9565\n",
      "Epoch 414/500\n",
      " - 0s - loss: 0.0755 - acc: 0.9565\n",
      "Epoch 415/500\n",
      " - 0s - loss: 0.0754 - acc: 0.9565\n",
      "Epoch 416/500\n",
      " - 0s - loss: 0.0753 - acc: 0.9565\n",
      "Epoch 417/500\n",
      " - 0s - loss: 0.0752 - acc: 0.9565\n",
      "Epoch 418/500\n",
      " - 0s - loss: 0.0751 - acc: 0.9565\n",
      "Epoch 419/500\n",
      " - 0s - loss: 0.0750 - acc: 0.9565\n",
      "Epoch 420/500\n",
      " - 0s - loss: 0.0749 - acc: 0.9565\n",
      "Epoch 421/500\n",
      " - 0s - loss: 0.0748 - acc: 0.9565\n",
      "Epoch 422/500\n",
      " - 0s - loss: 0.0747 - acc: 0.9565\n",
      "Epoch 423/500\n",
      " - 0s - loss: 0.0746 - acc: 0.9565\n",
      "Epoch 424/500\n",
      " - 0s - loss: 0.0746 - acc: 0.9565\n",
      "Epoch 425/500\n",
      " - 0s - loss: 0.0745 - acc: 0.9565\n",
      "Epoch 426/500\n",
      " - 0s - loss: 0.0744 - acc: 0.9565\n",
      "Epoch 427/500\n",
      " - 0s - loss: 0.0743 - acc: 0.9565\n",
      "Epoch 428/500\n",
      " - 0s - loss: 0.0742 - acc: 0.9565\n",
      "Epoch 429/500\n",
      " - 0s - loss: 0.0741 - acc: 0.9565\n",
      "Epoch 430/500\n",
      " - 0s - loss: 0.0740 - acc: 0.9565\n",
      "Epoch 431/500\n",
      " - 0s - loss: 0.0740 - acc: 0.9565\n",
      "Epoch 432/500\n",
      " - 0s - loss: 0.0739 - acc: 0.9565\n",
      "Epoch 433/500\n",
      " - 0s - loss: 0.0738 - acc: 0.9565\n",
      "Epoch 434/500\n",
      " - 0s - loss: 0.0737 - acc: 0.9565\n",
      "Epoch 435/500\n",
      " - 0s - loss: 0.0736 - acc: 0.9565\n",
      "Epoch 436/500\n",
      " - 0s - loss: 0.0736 - acc: 0.9565\n",
      "Epoch 437/500\n",
      " - 0s - loss: 0.0735 - acc: 0.9565\n",
      "Epoch 438/500\n",
      " - 0s - loss: 0.0734 - acc: 0.9565\n",
      "Epoch 439/500\n",
      " - 0s - loss: 0.0733 - acc: 0.9565\n",
      "Epoch 440/500\n",
      " - 0s - loss: 0.0733 - acc: 0.9565\n",
      "Epoch 441/500\n",
      " - 0s - loss: 0.0732 - acc: 0.9565\n",
      "Epoch 442/500\n",
      " - 0s - loss: 0.0731 - acc: 0.9565\n",
      "Epoch 443/500\n",
      " - 0s - loss: 0.0730 - acc: 0.9565\n",
      "Epoch 444/500\n",
      " - 0s - loss: 0.0730 - acc: 0.9565\n",
      "Epoch 445/500\n",
      " - 0s - loss: 0.0729 - acc: 0.9565\n",
      "Epoch 446/500\n",
      " - 0s - loss: 0.0728 - acc: 0.9565\n",
      "Epoch 447/500\n",
      " - 0s - loss: 0.0728 - acc: 0.9565\n",
      "Epoch 448/500\n",
      " - 0s - loss: 0.0727 - acc: 0.9565\n",
      "Epoch 449/500\n",
      " - 0s - loss: 0.0726 - acc: 0.9565\n",
      "Epoch 450/500\n",
      " - 0s - loss: 0.0726 - acc: 0.9565\n",
      "Epoch 451/500\n",
      " - 0s - loss: 0.0725 - acc: 0.9565\n",
      "Epoch 452/500\n",
      " - 0s - loss: 0.0724 - acc: 0.9565\n",
      "Epoch 453/500\n",
      " - 0s - loss: 0.0724 - acc: 0.9565\n",
      "Epoch 454/500\n",
      " - 0s - loss: 0.0723 - acc: 0.9565\n",
      "Epoch 455/500\n",
      " - 0s - loss: 0.0722 - acc: 0.9565\n",
      "Epoch 456/500\n",
      " - 0s - loss: 0.0722 - acc: 0.9565\n",
      "Epoch 457/500\n",
      " - 0s - loss: 0.0721 - acc: 0.9565\n",
      "Epoch 458/500\n",
      " - 0s - loss: 0.0720 - acc: 0.9565\n",
      "Epoch 459/500\n",
      " - 0s - loss: 0.0720 - acc: 0.9565\n",
      "Epoch 460/500\n",
      " - 0s - loss: 0.0719 - acc: 0.9565\n",
      "Epoch 461/500\n",
      " - 0s - loss: 0.0718 - acc: 0.9565\n",
      "Epoch 462/500\n",
      " - 0s - loss: 0.0718 - acc: 0.9565\n",
      "Epoch 463/500\n",
      " - 0s - loss: 0.0717 - acc: 0.9565\n",
      "Epoch 464/500\n",
      " - 0s - loss: 0.0717 - acc: 0.9565\n",
      "Epoch 465/500\n",
      " - 0s - loss: 0.0716 - acc: 0.9565\n",
      "Epoch 466/500\n",
      " - 0s - loss: 0.0715 - acc: 0.9565\n",
      "Epoch 467/500\n",
      " - 0s - loss: 0.0715 - acc: 0.9565\n",
      "Epoch 468/500\n",
      " - 0s - loss: 0.0714 - acc: 0.9565\n",
      "Epoch 469/500\n",
      " - 0s - loss: 0.0714 - acc: 0.9565\n",
      "Epoch 470/500\n",
      " - 0s - loss: 0.0713 - acc: 0.9565\n",
      "Epoch 471/500\n",
      " - 0s - loss: 0.0713 - acc: 0.9565\n",
      "Epoch 472/500\n",
      " - 0s - loss: 0.0712 - acc: 0.9565\n",
      "Epoch 473/500\n",
      " - 0s - loss: 0.0711 - acc: 0.9565\n",
      "Epoch 474/500\n",
      " - 0s - loss: 0.0711 - acc: 0.9565\n",
      "Epoch 475/500\n",
      " - 0s - loss: 0.0710 - acc: 0.9565\n",
      "Epoch 476/500\n",
      " - 0s - loss: 0.0710 - acc: 0.9565\n",
      "Epoch 477/500\n",
      " - 0s - loss: 0.0709 - acc: 0.9565\n",
      "Epoch 478/500\n",
      " - 0s - loss: 0.0709 - acc: 0.9565\n",
      "Epoch 479/500\n",
      " - 0s - loss: 0.0708 - acc: 0.9565\n",
      "Epoch 480/500\n",
      " - 0s - loss: 0.0708 - acc: 0.9565\n",
      "Epoch 481/500\n",
      " - 0s - loss: 0.0707 - acc: 0.9565\n",
      "Epoch 482/500\n",
      " - 0s - loss: 0.0707 - acc: 0.9565\n",
      "Epoch 483/500\n",
      " - 0s - loss: 0.0706 - acc: 0.9565\n",
      "Epoch 484/500\n",
      " - 0s - loss: 0.0706 - acc: 0.9565\n",
      "Epoch 485/500\n",
      " - 0s - loss: 0.0705 - acc: 0.9565\n",
      "Epoch 486/500\n",
      " - 0s - loss: 0.0705 - acc: 0.9565\n",
      "Epoch 487/500\n",
      " - 0s - loss: 0.0704 - acc: 0.9565\n",
      "Epoch 488/500\n",
      " - 0s - loss: 0.0704 - acc: 0.9565\n",
      "Epoch 489/500\n",
      " - 0s - loss: 0.0703 - acc: 0.9565\n",
      "Epoch 490/500\n",
      " - 0s - loss: 0.0703 - acc: 0.9565\n",
      "Epoch 491/500\n",
      " - 0s - loss: 0.0702 - acc: 0.9565\n",
      "Epoch 492/500\n",
      " - 0s - loss: 0.0702 - acc: 0.9565\n",
      "Epoch 493/500\n",
      " - 0s - loss: 0.0701 - acc: 0.9565\n",
      "Epoch 494/500\n",
      " - 0s - loss: 0.0701 - acc: 0.9565\n",
      "Epoch 495/500\n",
      " - 0s - loss: 0.0700 - acc: 0.9565\n",
      "Epoch 496/500\n",
      " - 0s - loss: 0.0700 - acc: 0.9565\n",
      "Epoch 497/500\n",
      " - 0s - loss: 0.0699 - acc: 0.9565\n",
      "Epoch 498/500\n",
      " - 0s - loss: 0.0699 - acc: 0.9565\n",
      "Epoch 499/500\n",
      " - 0s - loss: 0.0699 - acc: 0.9565\n",
      "Epoch 500/500\n",
      " - 0s - loss: 0.0698 - acc: 0.9565\n",
      "Jack and jill came tumbling after his\n",
      "And Jill came tumbling after\n",
      "fell down and broke his crown and\n",
      "pail of water jack fell down and\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# generate a sequence from a language model\n",
    "def generate_seq(model, tokenizer, max_length, seed_text, n_words):\n",
    "\tin_text = seed_text\n",
    "\t# generate a fixed number of words\n",
    "\tfor _ in range(n_words):\n",
    "\t\t# encode the text as integer\n",
    "\t\tencoded = tokenizer.texts_to_sequences([in_text])[0]\n",
    "\t\t# pre-pad sequences to a fixed length\n",
    "\t\tencoded = pad_sequences([encoded], maxlen=max_length, padding='pre')\n",
    "\t\t# predict probabilities for each word\n",
    "\t\tyhat = model.predict_classes(encoded, verbose=0)\n",
    "\t\t# map predicted word index to word\n",
    "\t\tout_word = ''\n",
    "\t\tfor word, index in tokenizer.word_index.items():\n",
    "\t\t\tif index == yhat:\n",
    "\t\t\t\tout_word = word\n",
    "\t\t\t\tbreak\n",
    "\t\t# append to input\n",
    "\t\tin_text += ' ' + out_word\n",
    "\treturn in_text\n",
    "\n",
    "# source text\n",
    "data = \"\"\" Jack and Jill went up the hill\\n\n",
    "\t\tTo fetch a pail of water\\n\n",
    "\t\tJack fell down and broke his crown\\n\n",
    "\t\tAnd Jill came tumbling after\\n \"\"\"\n",
    "# integer encode sequences of words\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts([data])\n",
    "encoded = tokenizer.texts_to_sequences([data])[0]\n",
    "# retrieve vocabulary size\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print('Vocabulary Size: %d' % vocab_size)\n",
    "\n",
    "\n",
    "# encode 2 words -> 1 word\n",
    "sequences = list()\n",
    "for i in range(2, len(encoded)):\n",
    "\tsequence = encoded[i-2:i+1]\n",
    "\tsequences.append(sequence)\n",
    "print('Total Sequences: %d' % len(sequences))\n",
    "\n",
    "\n",
    "# pad sequences\n",
    "max_length = max([len(seq) for seq in sequences])\n",
    "sequences = pad_sequences(sequences, maxlen=max_length, padding='pre')\n",
    "print('Max Sequence Length: %d' % max_length)\n",
    "\n",
    "\n",
    "# split into input and output elements\n",
    "sequences = array(sequences)\n",
    "X, y = sequences[:,:-1],sequences[:,-1]\n",
    "y = to_categorical(y, num_classes=vocab_size)\n",
    "\n",
    "\n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 10, input_length=max_length-1))\n",
    "model.add(LSTM(50))\n",
    "model.add(Dense(vocab_size, activation='softmax'))\n",
    "print(model.summary())\n",
    "\n",
    "\n",
    "# compile network\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# fit network\n",
    "model.fit(X, y, epochs=500, verbose=2)\n",
    "\n",
    "# evaluate model\n",
    "print(generate_seq(model, tokenizer, max_length-1, 'Jack and', 5))\n",
    "print(generate_seq(model, tokenizer, max_length-1, 'And Jill', 3))\n",
    "print(generate_seq(model, tokenizer, max_length-1, 'fell down', 5))\n",
    "print(generate_seq(model, tokenizer, max_length-1, 'pail of', 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Data\n",
    "data = [\"Two little dicky birds\",\n",
    "        \"Sat on a wall,\",\n",
    "        \"One called Peter,\",\n",
    "        \"One called Paul.\",\n",
    "        \"Fly away, Peter,\",\n",
    "        \"Fly away, Paul!\",\n",
    "        \"Come back, Peter,\",\n",
    "        \"Come back, Paul.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# tokenize data\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(data)\n",
    "vocab = tokenizer.word_index\n",
    "seqs = tokenizer.texts_to_sequences(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#prepare sentence\n",
    "\n",
    "import numpy as np\n",
    "def prepare_sentence(seq, maxlen):\n",
    "    # Pads seq and slides windows\n",
    "    x = []\n",
    "    y = []\n",
    "    for i, w in enumerate(seq):\n",
    "        x_padded = pad_sequences([seq[:i]],\n",
    "                                 maxlen=maxlen - 1,\n",
    "                                 padding='pre')[0]  # Pads before each sequence\n",
    "        x.append(x_padded)\n",
    "        y.append(w)\n",
    "    return x, y\n",
    "\n",
    "# Pad sequences and slide windows\n",
    "maxlen = max([len(seq) for seq in seqs])\n",
    "x = []\n",
    "y = []\n",
    "for seq in seqs:\n",
    "    x_windows, y_windows = prepare_sentence(seq, maxlen)\n",
    "    x += x_windows\n",
    "    y += y_windows\n",
    "x = np.array(x)\n",
    "y = np.array(y) - 1  # The word <PAD> does not constitute a class\n",
    "y = np.eye(len(vocab))[y]  # One hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "26/26 [==============================] - 2s 79ms/step - loss: 2.7721\n",
      "Epoch 2/1000\n",
      "26/26 [==============================] - 0s 215us/step - loss: 2.7698\n",
      "Epoch 3/1000\n",
      "26/26 [==============================] - 0s 244us/step - loss: 2.7682\n",
      "Epoch 4/1000\n",
      "26/26 [==============================] - 0s 285us/step - loss: 2.7668\n",
      "Epoch 5/1000\n",
      "26/26 [==============================] - 0s 263us/step - loss: 2.7655\n",
      "Epoch 6/1000\n",
      "26/26 [==============================] - 0s 210us/step - loss: 2.7643\n",
      "Epoch 7/1000\n",
      "26/26 [==============================] - 0s 327us/step - loss: 2.7631\n",
      "Epoch 8/1000\n",
      "26/26 [==============================] - 0s 245us/step - loss: 2.7620\n",
      "Epoch 9/1000\n",
      "26/26 [==============================] - 0s 317us/step - loss: 2.7610\n",
      "Epoch 10/1000\n",
      "26/26 [==============================] - 0s 230us/step - loss: 2.7599\n",
      "Epoch 11/1000\n",
      "26/26 [==============================] - 0s 285us/step - loss: 2.7589\n",
      "Epoch 12/1000\n",
      "26/26 [==============================] - 0s 238us/step - loss: 2.7578\n",
      "Epoch 13/1000\n",
      "26/26 [==============================] - 0s 218us/step - loss: 2.7568\n",
      "Epoch 14/1000\n",
      "26/26 [==============================] - 0s 250us/step - loss: 2.7558\n",
      "Epoch 15/1000\n",
      "26/26 [==============================] - 0s 291us/step - loss: 2.7547\n",
      "Epoch 16/1000\n",
      "26/26 [==============================] - 0s 267us/step - loss: 2.7537\n",
      "Epoch 17/1000\n",
      "26/26 [==============================] - 0s 268us/step - loss: 2.7527\n",
      "Epoch 18/1000\n",
      "26/26 [==============================] - 0s 227us/step - loss: 2.7516\n",
      "Epoch 19/1000\n",
      "26/26 [==============================] - 0s 264us/step - loss: 2.7506\n",
      "Epoch 20/1000\n",
      "26/26 [==============================] - 0s 226us/step - loss: 2.7495\n",
      "Epoch 21/1000\n",
      "26/26 [==============================] - 0s 257us/step - loss: 2.7484\n",
      "Epoch 22/1000\n",
      "26/26 [==============================] - 0s 220us/step - loss: 2.7474\n",
      "Epoch 23/1000\n",
      "26/26 [==============================] - 0s 394us/step - loss: 2.7463\n",
      "Epoch 24/1000\n",
      "26/26 [==============================] - 0s 241us/step - loss: 2.7452\n",
      "Epoch 25/1000\n",
      "26/26 [==============================] - 0s 280us/step - loss: 2.7441\n",
      "Epoch 26/1000\n",
      "26/26 [==============================] - 0s 277us/step - loss: 2.7430\n",
      "Epoch 27/1000\n",
      "26/26 [==============================] - 0s 293us/step - loss: 2.7419\n",
      "Epoch 28/1000\n",
      "26/26 [==============================] - 0s 303us/step - loss: 2.7408\n",
      "Epoch 29/1000\n",
      "26/26 [==============================] - 0s 280us/step - loss: 2.7396\n",
      "Epoch 30/1000\n",
      "26/26 [==============================] - 0s 314us/step - loss: 2.7385\n",
      "Epoch 31/1000\n",
      "26/26 [==============================] - 0s 248us/step - loss: 2.7373\n",
      "Epoch 32/1000\n",
      "26/26 [==============================] - 0s 293us/step - loss: 2.7361\n",
      "Epoch 33/1000\n",
      "26/26 [==============================] - 0s 331us/step - loss: 2.7349\n",
      "Epoch 34/1000\n",
      "26/26 [==============================] - 0s 216us/step - loss: 2.7337\n",
      "Epoch 35/1000\n",
      "26/26 [==============================] - 0s 258us/step - loss: 2.7325\n",
      "Epoch 36/1000\n",
      "26/26 [==============================] - 0s 324us/step - loss: 2.7313\n",
      "Epoch 37/1000\n",
      "26/26 [==============================] - 0s 221us/step - loss: 2.7300\n",
      "Epoch 38/1000\n",
      "26/26 [==============================] - 0s 220us/step - loss: 2.7288\n",
      "Epoch 39/1000\n",
      "26/26 [==============================] - 0s 284us/step - loss: 2.7275\n",
      "Epoch 40/1000\n",
      "26/26 [==============================] - 0s 261us/step - loss: 2.7263\n",
      "Epoch 41/1000\n",
      "26/26 [==============================] - 0s 258us/step - loss: 2.7250\n",
      "Epoch 42/1000\n",
      "26/26 [==============================] - 0s 345us/step - loss: 2.7237\n",
      "Epoch 43/1000\n",
      "26/26 [==============================] - 0s 263us/step - loss: 2.7223\n",
      "Epoch 44/1000\n",
      "26/26 [==============================] - 0s 329us/step - loss: 2.7210\n",
      "Epoch 45/1000\n",
      "26/26 [==============================] - 0s 327us/step - loss: 2.7197\n",
      "Epoch 46/1000\n",
      "26/26 [==============================] - 0s 306us/step - loss: 2.7183\n",
      "Epoch 47/1000\n",
      "26/26 [==============================] - 0s 303us/step - loss: 2.7169\n",
      "Epoch 48/1000\n",
      "26/26 [==============================] - 0s 266us/step - loss: 2.7155\n",
      "Epoch 49/1000\n",
      "26/26 [==============================] - 0s 248us/step - loss: 2.7142\n",
      "Epoch 50/1000\n",
      "26/26 [==============================] - 0s 285us/step - loss: 2.7127\n",
      "Epoch 51/1000\n",
      "26/26 [==============================] - 0s 344us/step - loss: 2.7113\n",
      "Epoch 52/1000\n",
      "26/26 [==============================] - 0s 287us/step - loss: 2.7099\n",
      "Epoch 53/1000\n",
      "26/26 [==============================] - 0s 262us/step - loss: 2.7085\n",
      "Epoch 54/1000\n",
      "26/26 [==============================] - 0s 264us/step - loss: 2.7070\n",
      "Epoch 55/1000\n",
      "26/26 [==============================] - 0s 244us/step - loss: 2.7055\n",
      "Epoch 56/1000\n",
      "26/26 [==============================] - 0s 364us/step - loss: 2.7040\n",
      "Epoch 57/1000\n",
      "26/26 [==============================] - 0s 241us/step - loss: 2.7026\n",
      "Epoch 58/1000\n",
      "26/26 [==============================] - 0s 280us/step - loss: 2.7010\n",
      "Epoch 59/1000\n",
      "26/26 [==============================] - 0s 315us/step - loss: 2.6995\n",
      "Epoch 60/1000\n",
      "26/26 [==============================] - 0s 249us/step - loss: 2.6980\n",
      "Epoch 61/1000\n",
      "26/26 [==============================] - 0s 318us/step - loss: 2.6965\n",
      "Epoch 62/1000\n",
      "26/26 [==============================] - 0s 288us/step - loss: 2.6949\n",
      "Epoch 63/1000\n",
      "26/26 [==============================] - 0s 284us/step - loss: 2.6933\n",
      "Epoch 64/1000\n",
      "26/26 [==============================] - 0s 235us/step - loss: 2.6918\n",
      "Epoch 65/1000\n",
      "26/26 [==============================] - 0s 357us/step - loss: 2.6902\n",
      "Epoch 66/1000\n",
      "26/26 [==============================] - 0s 313us/step - loss: 2.6886\n",
      "Epoch 67/1000\n",
      "26/26 [==============================] - 0s 255us/step - loss: 2.6870\n",
      "Epoch 68/1000\n",
      "26/26 [==============================] - 0s 273us/step - loss: 2.6854\n",
      "Epoch 69/1000\n",
      "26/26 [==============================] - 0s 308us/step - loss: 2.6838\n",
      "Epoch 70/1000\n",
      "26/26 [==============================] - 0s 299us/step - loss: 2.6821\n",
      "Epoch 71/1000\n",
      "26/26 [==============================] - 0s 282us/step - loss: 2.6805\n",
      "Epoch 72/1000\n",
      "26/26 [==============================] - 0s 250us/step - loss: 2.6788\n",
      "Epoch 73/1000\n",
      "26/26 [==============================] - 0s 243us/step - loss: 2.6772\n",
      "Epoch 74/1000\n",
      "26/26 [==============================] - 0s 327us/step - loss: 2.6755\n",
      "Epoch 75/1000\n",
      "26/26 [==============================] - 0s 285us/step - loss: 2.6739\n",
      "Epoch 76/1000\n",
      "26/26 [==============================] - 0s 271us/step - loss: 2.6722\n",
      "Epoch 77/1000\n",
      "26/26 [==============================] - 0s 298us/step - loss: 2.6705\n",
      "Epoch 78/1000\n",
      "26/26 [==============================] - 0s 287us/step - loss: 2.6689\n",
      "Epoch 79/1000\n",
      "26/26 [==============================] - 0s 265us/step - loss: 2.6672\n",
      "Epoch 80/1000\n",
      "26/26 [==============================] - 0s 312us/step - loss: 2.6655\n",
      "Epoch 81/1000\n",
      "26/26 [==============================] - 0s 295us/step - loss: 2.6638\n",
      "Epoch 82/1000\n",
      "26/26 [==============================] - 0s 306us/step - loss: 2.6621\n",
      "Epoch 83/1000\n",
      "26/26 [==============================] - 0s 318us/step - loss: 2.6604\n",
      "Epoch 84/1000\n",
      "26/26 [==============================] - 0s 272us/step - loss: 2.6587\n",
      "Epoch 85/1000\n",
      "26/26 [==============================] - 0s 251us/step - loss: 2.6570\n",
      "Epoch 86/1000\n",
      "26/26 [==============================] - 0s 310us/step - loss: 2.6553\n",
      "Epoch 87/1000\n",
      "26/26 [==============================] - 0s 321us/step - loss: 2.6536\n",
      "Epoch 88/1000\n",
      "26/26 [==============================] - 0s 291us/step - loss: 2.6519\n",
      "Epoch 89/1000\n",
      "26/26 [==============================] - 0s 271us/step - loss: 2.6502\n",
      "Epoch 90/1000\n",
      "26/26 [==============================] - 0s 360us/step - loss: 2.6485\n",
      "Epoch 91/1000\n",
      "26/26 [==============================] - 0s 292us/step - loss: 2.6468\n",
      "Epoch 92/1000\n",
      "26/26 [==============================] - 0s 258us/step - loss: 2.6451\n",
      "Epoch 93/1000\n",
      "26/26 [==============================] - 0s 242us/step - loss: 2.6434\n",
      "Epoch 94/1000\n",
      "26/26 [==============================] - 0s 312us/step - loss: 2.6417\n",
      "Epoch 95/1000\n",
      "26/26 [==============================] - 0s 282us/step - loss: 2.6399\n",
      "Epoch 96/1000\n",
      "26/26 [==============================] - 0s 253us/step - loss: 2.6382\n",
      "Epoch 97/1000\n",
      "26/26 [==============================] - 0s 250us/step - loss: 2.6365\n",
      "Epoch 98/1000\n",
      "26/26 [==============================] - 0s 281us/step - loss: 2.6347\n",
      "Epoch 99/1000\n",
      "26/26 [==============================] - 0s 334us/step - loss: 2.6330\n",
      "Epoch 100/1000\n",
      "26/26 [==============================] - 0s 260us/step - loss: 2.6312\n",
      "Epoch 101/1000\n",
      "26/26 [==============================] - 0s 272us/step - loss: 2.6294\n",
      "Epoch 102/1000\n",
      "26/26 [==============================] - 0s 269us/step - loss: 2.6277\n",
      "Epoch 103/1000\n",
      "26/26 [==============================] - 0s 290us/step - loss: 2.6259\n",
      "Epoch 104/1000\n",
      "26/26 [==============================] - 0s 267us/step - loss: 2.6241\n",
      "Epoch 105/1000\n",
      "26/26 [==============================] - 0s 290us/step - loss: 2.6223\n",
      "Epoch 106/1000\n",
      "26/26 [==============================] - 0s 242us/step - loss: 2.6205\n",
      "Epoch 107/1000\n",
      "26/26 [==============================] - 0s 277us/step - loss: 2.6186\n",
      "Epoch 108/1000\n",
      "26/26 [==============================] - 0s 235us/step - loss: 2.6168\n",
      "Epoch 109/1000\n",
      "26/26 [==============================] - 0s 290us/step - loss: 2.6150\n",
      "Epoch 110/1000\n",
      "26/26 [==============================] - 0s 395us/step - loss: 2.6131\n",
      "Epoch 111/1000\n",
      "26/26 [==============================] - 0s 271us/step - loss: 2.6112\n",
      "Epoch 112/1000\n",
      "26/26 [==============================] - 0s 249us/step - loss: 2.6093\n",
      "Epoch 113/1000\n",
      "26/26 [==============================] - 0s 304us/step - loss: 2.6074\n",
      "Epoch 114/1000\n",
      "26/26 [==============================] - 0s 272us/step - loss: 2.6055\n",
      "Epoch 115/1000\n",
      "26/26 [==============================] - 0s 234us/step - loss: 2.6036\n",
      "Epoch 116/1000\n",
      "26/26 [==============================] - 0s 236us/step - loss: 2.6016\n",
      "Epoch 117/1000\n",
      "26/26 [==============================] - 0s 288us/step - loss: 2.5997\n",
      "Epoch 118/1000\n",
      "26/26 [==============================] - 0s 234us/step - loss: 2.5977\n",
      "Epoch 119/1000\n",
      "26/26 [==============================] - 0s 278us/step - loss: 2.5957\n",
      "Epoch 120/1000\n",
      "26/26 [==============================] - 0s 247us/step - loss: 2.5937\n",
      "Epoch 121/1000\n",
      "26/26 [==============================] - 0s 295us/step - loss: 2.5917\n",
      "Epoch 122/1000\n",
      "26/26 [==============================] - 0s 246us/step - loss: 2.5897\n",
      "Epoch 123/1000\n",
      "26/26 [==============================] - 0s 246us/step - loss: 2.5876\n",
      "Epoch 124/1000\n",
      "26/26 [==============================] - 0s 317us/step - loss: 2.5856\n",
      "Epoch 125/1000\n",
      "26/26 [==============================] - 0s 245us/step - loss: 2.5835\n",
      "Epoch 126/1000\n",
      "26/26 [==============================] - 0s 269us/step - loss: 2.5814\n",
      "Epoch 127/1000\n",
      "26/26 [==============================] - 0s 278us/step - loss: 2.5793\n",
      "Epoch 128/1000\n",
      "26/26 [==============================] - 0s 247us/step - loss: 2.5772\n",
      "Epoch 129/1000\n",
      "26/26 [==============================] - 0s 295us/step - loss: 2.5751\n",
      "Epoch 130/1000\n",
      "26/26 [==============================] - 0s 294us/step - loss: 2.5729\n",
      "Epoch 131/1000\n",
      "26/26 [==============================] - 0s 224us/step - loss: 2.5708\n",
      "Epoch 132/1000\n",
      "26/26 [==============================] - 0s 309us/step - loss: 2.5686\n",
      "Epoch 133/1000\n",
      "26/26 [==============================] - 0s 236us/step - loss: 2.5664\n",
      "Epoch 134/1000\n",
      "26/26 [==============================] - 0s 266us/step - loss: 2.5642\n",
      "Epoch 135/1000\n",
      "26/26 [==============================] - 0s 261us/step - loss: 2.5620\n",
      "Epoch 136/1000\n",
      "26/26 [==============================] - 0s 312us/step - loss: 2.5597\n",
      "Epoch 137/1000\n",
      "26/26 [==============================] - 0s 258us/step - loss: 2.5575\n",
      "Epoch 138/1000\n",
      "26/26 [==============================] - 0s 308us/step - loss: 2.5552\n",
      "Epoch 139/1000\n",
      "26/26 [==============================] - 0s 242us/step - loss: 2.5529\n",
      "Epoch 140/1000\n",
      "26/26 [==============================] - 0s 261us/step - loss: 2.5506\n",
      "Epoch 141/1000\n",
      "26/26 [==============================] - 0s 314us/step - loss: 2.5483\n",
      "Epoch 142/1000\n",
      "26/26 [==============================] - 0s 299us/step - loss: 2.5460\n",
      "Epoch 143/1000\n",
      "26/26 [==============================] - 0s 333us/step - loss: 2.5437\n",
      "Epoch 144/1000\n",
      "26/26 [==============================] - 0s 260us/step - loss: 2.5413\n",
      "Epoch 145/1000\n",
      "26/26 [==============================] - 0s 282us/step - loss: 2.5389\n",
      "Epoch 146/1000\n",
      "26/26 [==============================] - 0s 286us/step - loss: 2.5365\n",
      "Epoch 147/1000\n",
      "26/26 [==============================] - 0s 273us/step - loss: 2.5341\n",
      "Epoch 148/1000\n",
      "26/26 [==============================] - 0s 282us/step - loss: 2.5317\n",
      "Epoch 149/1000\n",
      "26/26 [==============================] - 0s 268us/step - loss: 2.5293\n",
      "Epoch 150/1000\n",
      "26/26 [==============================] - 0s 342us/step - loss: 2.5268\n",
      "Epoch 151/1000\n",
      "26/26 [==============================] - 0s 308us/step - loss: 2.5244\n",
      "Epoch 152/1000\n",
      "26/26 [==============================] - 0s 252us/step - loss: 2.5219\n",
      "Epoch 153/1000\n",
      "26/26 [==============================] - 0s 272us/step - loss: 2.5194\n",
      "Epoch 154/1000\n",
      "26/26 [==============================] - 0s 314us/step - loss: 2.5169\n",
      "Epoch 155/1000\n",
      "26/26 [==============================] - 0s 283us/step - loss: 2.5144\n",
      "Epoch 156/1000\n",
      "26/26 [==============================] - 0s 249us/step - loss: 2.5119\n",
      "Epoch 157/1000\n",
      "26/26 [==============================] - 0s 284us/step - loss: 2.5094\n",
      "Epoch 158/1000\n",
      "26/26 [==============================] - 0s 300us/step - loss: 2.5068\n",
      "Epoch 159/1000\n",
      "26/26 [==============================] - 0s 286us/step - loss: 2.5042\n",
      "Epoch 160/1000\n",
      "26/26 [==============================] - 0s 294us/step - loss: 2.5017\n",
      "Epoch 161/1000\n",
      "26/26 [==============================] - 0s 301us/step - loss: 2.4991\n",
      "Epoch 162/1000\n",
      "26/26 [==============================] - 0s 277us/step - loss: 2.4965\n",
      "Epoch 163/1000\n",
      "26/26 [==============================] - 0s 294us/step - loss: 2.4939\n",
      "Epoch 164/1000\n",
      "26/26 [==============================] - 0s 415us/step - loss: 2.4912\n",
      "Epoch 165/1000\n",
      "26/26 [==============================] - 0s 370us/step - loss: 2.4886\n",
      "Epoch 166/1000\n",
      "26/26 [==============================] - 0s 315us/step - loss: 2.4859\n",
      "Epoch 167/1000\n",
      "26/26 [==============================] - 0s 320us/step - loss: 2.4833\n",
      "Epoch 168/1000\n",
      "26/26 [==============================] - 0s 371us/step - loss: 2.4806\n",
      "Epoch 169/1000\n",
      "26/26 [==============================] - 0s 384us/step - loss: 2.4779\n",
      "Epoch 170/1000\n",
      "26/26 [==============================] - 0s 305us/step - loss: 2.4752\n",
      "Epoch 171/1000\n",
      "26/26 [==============================] - 0s 358us/step - loss: 2.4725\n",
      "Epoch 172/1000\n",
      "26/26 [==============================] - 0s 352us/step - loss: 2.4697\n",
      "Epoch 173/1000\n",
      "26/26 [==============================] - 0s 343us/step - loss: 2.4670\n",
      "Epoch 174/1000\n",
      "26/26 [==============================] - 0s 316us/step - loss: 2.4642\n",
      "Epoch 175/1000\n",
      "26/26 [==============================] - 0s 253us/step - loss: 2.4615\n",
      "Epoch 176/1000\n",
      "26/26 [==============================] - 0s 291us/step - loss: 2.4587\n",
      "Epoch 177/1000\n",
      "26/26 [==============================] - 0s 229us/step - loss: 2.4559\n",
      "Epoch 178/1000\n",
      "26/26 [==============================] - 0s 312us/step - loss: 2.4531\n",
      "Epoch 179/1000\n",
      "26/26 [==============================] - 0s 280us/step - loss: 2.4503\n",
      "Epoch 180/1000\n",
      "26/26 [==============================] - 0s 245us/step - loss: 2.4475\n",
      "Epoch 181/1000\n",
      "26/26 [==============================] - 0s 365us/step - loss: 2.4447\n",
      "Epoch 182/1000\n",
      "26/26 [==============================] - 0s 240us/step - loss: 2.4418\n",
      "Epoch 183/1000\n",
      "26/26 [==============================] - 0s 346us/step - loss: 2.4390\n",
      "Epoch 184/1000\n",
      "26/26 [==============================] - 0s 296us/step - loss: 2.4361\n",
      "Epoch 185/1000\n",
      "26/26 [==============================] - 0s 244us/step - loss: 2.4332\n",
      "Epoch 186/1000\n",
      "26/26 [==============================] - 0s 290us/step - loss: 2.4303\n",
      "Epoch 187/1000\n",
      "26/26 [==============================] - 0s 291us/step - loss: 2.4274\n",
      "Epoch 188/1000\n",
      "26/26 [==============================] - 0s 290us/step - loss: 2.4245\n",
      "Epoch 189/1000\n",
      "26/26 [==============================] - 0s 285us/step - loss: 2.4216\n",
      "Epoch 190/1000\n",
      "26/26 [==============================] - 0s 237us/step - loss: 2.4187\n",
      "Epoch 191/1000\n",
      "26/26 [==============================] - 0s 360us/step - loss: 2.4157\n",
      "Epoch 192/1000\n",
      "26/26 [==============================] - 0s 279us/step - loss: 2.4128\n",
      "Epoch 193/1000\n",
      "26/26 [==============================] - 0s 261us/step - loss: 2.4098\n",
      "Epoch 194/1000\n",
      "26/26 [==============================] - 0s 228us/step - loss: 2.4068\n",
      "Epoch 195/1000\n",
      "26/26 [==============================] - 0s 328us/step - loss: 2.4038\n",
      "Epoch 196/1000\n",
      "26/26 [==============================] - 0s 310us/step - loss: 2.4008\n",
      "Epoch 197/1000\n",
      "26/26 [==============================] - 0s 243us/step - loss: 2.3978\n",
      "Epoch 198/1000\n",
      "26/26 [==============================] - 0s 256us/step - loss: 2.3948\n",
      "Epoch 199/1000\n",
      "26/26 [==============================] - 0s 278us/step - loss: 2.3917\n",
      "Epoch 200/1000\n",
      "26/26 [==============================] - 0s 236us/step - loss: 2.3887\n",
      "Epoch 201/1000\n",
      "26/26 [==============================] - 0s 299us/step - loss: 2.3856\n",
      "Epoch 202/1000\n",
      "26/26 [==============================] - 0s 300us/step - loss: 2.3825\n",
      "Epoch 203/1000\n",
      "26/26 [==============================] - 0s 234us/step - loss: 2.3795\n",
      "Epoch 204/1000\n",
      "26/26 [==============================] - 0s 261us/step - loss: 2.3763\n",
      "Epoch 205/1000\n",
      "26/26 [==============================] - 0s 257us/step - loss: 2.3732\n",
      "Epoch 206/1000\n",
      "26/26 [==============================] - 0s 272us/step - loss: 2.3701\n",
      "Epoch 207/1000\n",
      "26/26 [==============================] - 0s 301us/step - loss: 2.3670\n",
      "Epoch 208/1000\n",
      "26/26 [==============================] - 0s 263us/step - loss: 2.3638\n",
      "Epoch 209/1000\n",
      "26/26 [==============================] - 0s 274us/step - loss: 2.3607\n",
      "Epoch 210/1000\n",
      "26/26 [==============================] - 0s 325us/step - loss: 2.3575\n",
      "Epoch 211/1000\n",
      "26/26 [==============================] - 0s 280us/step - loss: 2.3543\n",
      "Epoch 212/1000\n",
      "26/26 [==============================] - 0s 265us/step - loss: 2.3511\n",
      "Epoch 213/1000\n",
      "26/26 [==============================] - 0s 390us/step - loss: 2.3479\n",
      "Epoch 214/1000\n",
      "26/26 [==============================] - 0s 318us/step - loss: 2.3446\n",
      "Epoch 215/1000\n",
      "26/26 [==============================] - 0s 285us/step - loss: 2.3414\n",
      "Epoch 216/1000\n",
      "26/26 [==============================] - 0s 280us/step - loss: 2.3382\n",
      "Epoch 217/1000\n",
      "26/26 [==============================] - 0s 259us/step - loss: 2.3349\n",
      "Epoch 218/1000\n",
      "26/26 [==============================] - 0s 401us/step - loss: 2.3316\n",
      "Epoch 219/1000\n",
      "26/26 [==============================] - 0s 324us/step - loss: 2.3283\n",
      "Epoch 220/1000\n",
      "26/26 [==============================] - 0s 296us/step - loss: 2.3250\n",
      "Epoch 221/1000\n",
      "26/26 [==============================] - 0s 427us/step - loss: 2.3217\n",
      "Epoch 222/1000\n",
      "26/26 [==============================] - 0s 290us/step - loss: 2.3183\n",
      "Epoch 223/1000\n",
      "26/26 [==============================] - 0s 391us/step - loss: 2.3150\n",
      "Epoch 224/1000\n",
      "26/26 [==============================] - 0s 314us/step - loss: 2.3116\n",
      "Epoch 225/1000\n",
      "26/26 [==============================] - 0s 380us/step - loss: 2.3082\n",
      "Epoch 226/1000\n",
      "26/26 [==============================] - 0s 473us/step - loss: 2.3048\n",
      "Epoch 227/1000\n",
      "26/26 [==============================] - 0s 317us/step - loss: 2.3014\n",
      "Epoch 228/1000\n",
      "26/26 [==============================] - 0s 624us/step - loss: 2.2980\n",
      "Epoch 229/1000\n",
      "26/26 [==============================] - 0s 274us/step - loss: 2.2945\n",
      "Epoch 230/1000\n",
      "26/26 [==============================] - 0s 274us/step - loss: 2.2911\n",
      "Epoch 231/1000\n",
      "26/26 [==============================] - 0s 248us/step - loss: 2.2876\n",
      "Epoch 232/1000\n",
      "26/26 [==============================] - 0s 323us/step - loss: 2.2841\n",
      "Epoch 233/1000\n",
      "26/26 [==============================] - 0s 291us/step - loss: 2.2806\n",
      "Epoch 234/1000\n",
      "26/26 [==============================] - 0s 283us/step - loss: 2.2771\n",
      "Epoch 235/1000\n",
      "26/26 [==============================] - 0s 275us/step - loss: 2.2735\n",
      "Epoch 236/1000\n",
      "26/26 [==============================] - 0s 302us/step - loss: 2.2700\n",
      "Epoch 237/1000\n",
      "26/26 [==============================] - 0s 323us/step - loss: 2.2664\n",
      "Epoch 238/1000\n",
      "26/26 [==============================] - 0s 351us/step - loss: 2.2628\n",
      "Epoch 239/1000\n",
      "26/26 [==============================] - 0s 334us/step - loss: 2.2592\n",
      "Epoch 240/1000\n",
      "26/26 [==============================] - 0s 318us/step - loss: 2.2556\n",
      "Epoch 241/1000\n",
      "26/26 [==============================] - 0s 308us/step - loss: 2.2520\n",
      "Epoch 242/1000\n",
      "26/26 [==============================] - 0s 252us/step - loss: 2.2483\n",
      "Epoch 243/1000\n",
      "26/26 [==============================] - 0s 212us/step - loss: 2.2446\n",
      "Epoch 244/1000\n",
      "26/26 [==============================] - 0s 290us/step - loss: 2.2409\n",
      "Epoch 245/1000\n",
      "26/26 [==============================] - 0s 320us/step - loss: 2.2372\n",
      "Epoch 246/1000\n",
      "26/26 [==============================] - 0s 212us/step - loss: 2.2335\n",
      "Epoch 247/1000\n",
      "26/26 [==============================] - 0s 291us/step - loss: 2.2298\n",
      "Epoch 248/1000\n",
      "26/26 [==============================] - 0s 273us/step - loss: 2.2260\n",
      "Epoch 249/1000\n",
      "26/26 [==============================] - 0s 359us/step - loss: 2.2222\n",
      "Epoch 250/1000\n",
      "26/26 [==============================] - 0s 307us/step - loss: 2.2184\n",
      "Epoch 251/1000\n",
      "26/26 [==============================] - 0s 270us/step - loss: 2.2146\n",
      "Epoch 252/1000\n",
      "26/26 [==============================] - 0s 452us/step - loss: 2.2108\n",
      "Epoch 253/1000\n",
      "26/26 [==============================] - 0s 326us/step - loss: 2.2069\n",
      "Epoch 254/1000\n",
      "26/26 [==============================] - 0s 253us/step - loss: 2.2031\n",
      "Epoch 255/1000\n",
      "26/26 [==============================] - 0s 355us/step - loss: 2.1991\n",
      "Epoch 256/1000\n",
      "26/26 [==============================] - 0s 376us/step - loss: 2.1952\n",
      "Epoch 257/1000\n",
      "26/26 [==============================] - 0s 305us/step - loss: 2.1913\n",
      "Epoch 258/1000\n",
      "26/26 [==============================] - 0s 296us/step - loss: 2.1874\n",
      "Epoch 259/1000\n",
      "26/26 [==============================] - 0s 350us/step - loss: 2.1834\n",
      "Epoch 260/1000\n",
      "26/26 [==============================] - 0s 299us/step - loss: 2.1794\n",
      "Epoch 261/1000\n",
      "26/26 [==============================] - 0s 266us/step - loss: 2.1754\n",
      "Epoch 262/1000\n",
      "26/26 [==============================] - 0s 232us/step - loss: 2.1713\n",
      "Epoch 263/1000\n",
      "26/26 [==============================] - 0s 425us/step - loss: 2.1673\n",
      "Epoch 264/1000\n",
      "26/26 [==============================] - 0s 336us/step - loss: 2.1632\n",
      "Epoch 265/1000\n",
      "26/26 [==============================] - 0s 313us/step - loss: 2.1591\n",
      "Epoch 266/1000\n",
      "26/26 [==============================] - 0s 236us/step - loss: 2.1549\n",
      "Epoch 267/1000\n",
      "26/26 [==============================] - 0s 273us/step - loss: 2.1508\n",
      "Epoch 268/1000\n",
      "26/26 [==============================] - 0s 234us/step - loss: 2.1466\n",
      "Epoch 269/1000\n",
      "26/26 [==============================] - 0s 283us/step - loss: 2.1424\n",
      "Epoch 270/1000\n",
      "26/26 [==============================] - 0s 262us/step - loss: 2.1381\n",
      "Epoch 271/1000\n",
      "26/26 [==============================] - 0s 251us/step - loss: 2.1339\n",
      "Epoch 272/1000\n",
      "26/26 [==============================] - 0s 261us/step - loss: 2.1296\n",
      "Epoch 273/1000\n",
      "26/26 [==============================] - 0s 283us/step - loss: 2.1254\n",
      "Epoch 274/1000\n",
      "26/26 [==============================] - 0s 207us/step - loss: 2.1210\n",
      "Epoch 275/1000\n",
      "26/26 [==============================] - 0s 212us/step - loss: 2.1167\n",
      "Epoch 276/1000\n",
      "26/26 [==============================] - 0s 331us/step - loss: 2.1124\n",
      "Epoch 277/1000\n",
      "26/26 [==============================] - 0s 261us/step - loss: 2.1080\n",
      "Epoch 278/1000\n",
      "26/26 [==============================] - 0s 292us/step - loss: 2.1037\n",
      "Epoch 279/1000\n",
      "26/26 [==============================] - 0s 324us/step - loss: 2.0993\n",
      "Epoch 280/1000\n",
      "26/26 [==============================] - 0s 288us/step - loss: 2.0949\n",
      "Epoch 281/1000\n",
      "26/26 [==============================] - 0s 445us/step - loss: 2.0905\n",
      "Epoch 282/1000\n",
      "26/26 [==============================] - 0s 275us/step - loss: 2.0860\n",
      "Epoch 283/1000\n",
      "26/26 [==============================] - 0s 222us/step - loss: 2.0816\n",
      "Epoch 284/1000\n",
      "26/26 [==============================] - 0s 349us/step - loss: 2.0771\n",
      "Epoch 285/1000\n",
      "26/26 [==============================] - 0s 252us/step - loss: 2.0726\n",
      "Epoch 286/1000\n",
      "26/26 [==============================] - 0s 243us/step - loss: 2.0680\n",
      "Epoch 287/1000\n",
      "26/26 [==============================] - 0s 269us/step - loss: 2.0635\n",
      "Epoch 288/1000\n",
      "26/26 [==============================] - 0s 237us/step - loss: 2.0589\n",
      "Epoch 289/1000\n",
      "26/26 [==============================] - 0s 274us/step - loss: 2.0543\n",
      "Epoch 290/1000\n",
      "26/26 [==============================] - 0s 350us/step - loss: 2.0496\n",
      "Epoch 291/1000\n",
      "26/26 [==============================] - 0s 245us/step - loss: 2.0450\n",
      "Epoch 292/1000\n",
      "26/26 [==============================] - 0s 266us/step - loss: 2.0403\n",
      "Epoch 293/1000\n",
      "26/26 [==============================] - 0s 319us/step - loss: 2.0356\n",
      "Epoch 294/1000\n",
      "26/26 [==============================] - 0s 293us/step - loss: 2.0309\n",
      "Epoch 295/1000\n",
      "26/26 [==============================] - 0s 266us/step - loss: 2.0261\n",
      "Epoch 296/1000\n",
      "26/26 [==============================] - 0s 226us/step - loss: 2.0214\n",
      "Epoch 297/1000\n",
      "26/26 [==============================] - 0s 253us/step - loss: 2.0166\n",
      "Epoch 298/1000\n",
      "26/26 [==============================] - 0s 320us/step - loss: 2.0118\n",
      "Epoch 299/1000\n",
      "26/26 [==============================] - 0s 317us/step - loss: 2.0070\n",
      "Epoch 300/1000\n",
      "26/26 [==============================] - 0s 260us/step - loss: 2.0021\n",
      "Epoch 301/1000\n",
      "26/26 [==============================] - 0s 233us/step - loss: 1.9973\n",
      "Epoch 302/1000\n",
      "26/26 [==============================] - 0s 313us/step - loss: 1.9924\n",
      "Epoch 303/1000\n",
      "26/26 [==============================] - 0s 302us/step - loss: 1.9875\n",
      "Epoch 304/1000\n",
      "26/26 [==============================] - 0s 396us/step - loss: 1.9826\n",
      "Epoch 305/1000\n",
      "26/26 [==============================] - 0s 246us/step - loss: 1.9776\n",
      "Epoch 306/1000\n",
      "26/26 [==============================] - 0s 331us/step - loss: 1.9727\n",
      "Epoch 307/1000\n",
      "26/26 [==============================] - 0s 265us/step - loss: 1.9677\n",
      "Epoch 308/1000\n",
      "26/26 [==============================] - 0s 261us/step - loss: 1.9627\n",
      "Epoch 309/1000\n",
      "26/26 [==============================] - 0s 266us/step - loss: 1.9577\n",
      "Epoch 310/1000\n",
      "26/26 [==============================] - 0s 237us/step - loss: 1.9527\n",
      "Epoch 311/1000\n",
      "26/26 [==============================] - 0s 223us/step - loss: 1.9476\n",
      "Epoch 312/1000\n",
      "26/26 [==============================] - 0s 310us/step - loss: 1.9426\n",
      "Epoch 313/1000\n",
      "26/26 [==============================] - 0s 294us/step - loss: 1.9376\n",
      "Epoch 314/1000\n",
      "26/26 [==============================] - 0s 296us/step - loss: 1.9325\n",
      "Epoch 315/1000\n",
      "26/26 [==============================] - 0s 247us/step - loss: 1.9275\n",
      "Epoch 316/1000\n",
      "26/26 [==============================] - 0s 223us/step - loss: 1.9224\n",
      "Epoch 317/1000\n",
      "26/26 [==============================] - 0s 290us/step - loss: 1.9174\n",
      "Epoch 318/1000\n",
      "26/26 [==============================] - 0s 314us/step - loss: 1.9124\n",
      "Epoch 319/1000\n",
      "26/26 [==============================] - 0s 277us/step - loss: 1.9074\n",
      "Epoch 320/1000\n",
      "26/26 [==============================] - 0s 263us/step - loss: 1.9024\n",
      "Epoch 321/1000\n",
      "26/26 [==============================] - 0s 254us/step - loss: 1.8974\n",
      "Epoch 322/1000\n",
      "26/26 [==============================] - 0s 232us/step - loss: 1.8924\n",
      "Epoch 323/1000\n",
      "26/26 [==============================] - 0s 286us/step - loss: 1.8874\n",
      "Epoch 324/1000\n",
      "26/26 [==============================] - 0s 341us/step - loss: 1.8824\n",
      "Epoch 325/1000\n",
      "26/26 [==============================] - 0s 248us/step - loss: 1.8774\n",
      "Epoch 326/1000\n",
      "26/26 [==============================] - 0s 279us/step - loss: 1.8725\n",
      "Epoch 327/1000\n",
      "26/26 [==============================] - 0s 256us/step - loss: 1.8677\n",
      "Epoch 328/1000\n",
      "26/26 [==============================] - 0s 292us/step - loss: 1.8629\n",
      "Epoch 329/1000\n",
      "26/26 [==============================] - 0s 305us/step - loss: 1.8581\n",
      "Epoch 330/1000\n",
      "26/26 [==============================] - 0s 272us/step - loss: 1.8532\n",
      "Epoch 331/1000\n",
      "26/26 [==============================] - 0s 287us/step - loss: 1.8486\n",
      "Epoch 332/1000\n",
      "26/26 [==============================] - 0s 294us/step - loss: 1.8440\n",
      "Epoch 333/1000\n",
      "26/26 [==============================] - 0s 262us/step - loss: 1.8394\n",
      "Epoch 334/1000\n",
      "26/26 [==============================] - 0s 281us/step - loss: 1.8348\n",
      "Epoch 335/1000\n",
      "26/26 [==============================] - 0s 295us/step - loss: 1.8302\n",
      "Epoch 336/1000\n",
      "26/26 [==============================] - 0s 261us/step - loss: 1.8256\n",
      "Epoch 337/1000\n",
      "26/26 [==============================] - 0s 233us/step - loss: 1.8210\n",
      "Epoch 338/1000\n",
      "26/26 [==============================] - 0s 310us/step - loss: 1.8166\n",
      "Epoch 339/1000\n",
      "26/26 [==============================] - 0s 304us/step - loss: 1.8121\n",
      "Epoch 340/1000\n",
      "26/26 [==============================] - 0s 276us/step - loss: 1.8076\n",
      "Epoch 341/1000\n",
      "26/26 [==============================] - 0s 249us/step - loss: 1.8030\n",
      "Epoch 342/1000\n",
      "26/26 [==============================] - 0s 235us/step - loss: 1.7985\n",
      "Epoch 343/1000\n",
      "26/26 [==============================] - 0s 269us/step - loss: 1.7939\n",
      "Epoch 344/1000\n",
      "26/26 [==============================] - 0s 289us/step - loss: 1.7893\n",
      "Epoch 345/1000\n",
      "26/26 [==============================] - 0s 264us/step - loss: 1.7849\n",
      "Epoch 346/1000\n",
      "26/26 [==============================] - 0s 278us/step - loss: 1.7804\n",
      "Epoch 347/1000\n",
      "26/26 [==============================] - 0s 235us/step - loss: 1.7760\n",
      "Epoch 348/1000\n",
      "26/26 [==============================] - 0s 251us/step - loss: 1.7716\n",
      "Epoch 349/1000\n",
      "26/26 [==============================] - 0s 270us/step - loss: 1.7673\n",
      "Epoch 350/1000\n",
      "26/26 [==============================] - 0s 349us/step - loss: 1.7630\n",
      "Epoch 351/1000\n",
      "26/26 [==============================] - 0s 284us/step - loss: 1.7588\n",
      "Epoch 352/1000\n",
      "26/26 [==============================] - 0s 296us/step - loss: 1.7547\n",
      "Epoch 353/1000\n",
      "26/26 [==============================] - 0s 256us/step - loss: 1.7506\n",
      "Epoch 354/1000\n",
      "26/26 [==============================] - 0s 366us/step - loss: 1.7466\n",
      "Epoch 355/1000\n",
      "26/26 [==============================] - 0s 285us/step - loss: 1.7426\n",
      "Epoch 356/1000\n",
      "26/26 [==============================] - 0s 243us/step - loss: 1.7385\n",
      "Epoch 357/1000\n",
      "26/26 [==============================] - 0s 251us/step - loss: 1.7346\n",
      "Epoch 358/1000\n",
      "26/26 [==============================] - 0s 229us/step - loss: 1.7306\n",
      "Epoch 359/1000\n",
      "26/26 [==============================] - 0s 310us/step - loss: 1.7266\n",
      "Epoch 360/1000\n",
      "26/26 [==============================] - 0s 330us/step - loss: 1.7228\n",
      "Epoch 361/1000\n",
      "26/26 [==============================] - 0s 282us/step - loss: 1.7189\n",
      "Epoch 362/1000\n",
      "26/26 [==============================] - 0s 318us/step - loss: 1.7150\n",
      "Epoch 363/1000\n",
      "26/26 [==============================] - 0s 304us/step - loss: 1.7111\n",
      "Epoch 364/1000\n",
      "26/26 [==============================] - 0s 284us/step - loss: 1.7072\n",
      "Epoch 365/1000\n",
      "26/26 [==============================] - 0s 287us/step - loss: 1.7033\n",
      "Epoch 366/1000\n",
      "26/26 [==============================] - 0s 234us/step - loss: 1.6994\n",
      "Epoch 367/1000\n",
      "26/26 [==============================] - 0s 269us/step - loss: 1.6955\n",
      "Epoch 368/1000\n",
      "26/26 [==============================] - 0s 254us/step - loss: 1.6917\n",
      "Epoch 369/1000\n",
      "26/26 [==============================] - 0s 285us/step - loss: 1.6878\n",
      "Epoch 370/1000\n",
      "26/26 [==============================] - 0s 347us/step - loss: 1.6840\n",
      "Epoch 371/1000\n",
      "26/26 [==============================] - 0s 312us/step - loss: 1.6801\n",
      "Epoch 372/1000\n",
      "26/26 [==============================] - 0s 297us/step - loss: 1.6762\n",
      "Epoch 373/1000\n",
      "26/26 [==============================] - 0s 347us/step - loss: 1.6724\n",
      "Epoch 374/1000\n",
      "26/26 [==============================] - 0s 361us/step - loss: 1.6686\n",
      "Epoch 375/1000\n",
      "26/26 [==============================] - 0s 370us/step - loss: 1.6648\n",
      "Epoch 376/1000\n",
      "26/26 [==============================] - 0s 242us/step - loss: 1.6610\n",
      "Epoch 377/1000\n",
      "26/26 [==============================] - 0s 411us/step - loss: 1.6572\n",
      "Epoch 378/1000\n",
      "26/26 [==============================] - 0s 303us/step - loss: 1.6534\n",
      "Epoch 379/1000\n",
      "26/26 [==============================] - 0s 249us/step - loss: 1.6495\n",
      "Epoch 380/1000\n",
      "26/26 [==============================] - 0s 439us/step - loss: 1.6457\n",
      "Epoch 381/1000\n",
      "26/26 [==============================] - 0s 276us/step - loss: 1.6419\n",
      "Epoch 382/1000\n",
      "26/26 [==============================] - 0s 266us/step - loss: 1.6382\n",
      "Epoch 383/1000\n",
      "26/26 [==============================] - 0s 300us/step - loss: 1.6345\n",
      "Epoch 384/1000\n",
      "26/26 [==============================] - 0s 246us/step - loss: 1.6307\n",
      "Epoch 385/1000\n",
      "26/26 [==============================] - 0s 480us/step - loss: 1.6270\n",
      "Epoch 386/1000\n",
      "26/26 [==============================] - 0s 304us/step - loss: 1.6232\n",
      "Epoch 387/1000\n",
      "26/26 [==============================] - 0s 240us/step - loss: 1.6195\n",
      "Epoch 388/1000\n",
      "26/26 [==============================] - 0s 321us/step - loss: 1.6158\n",
      "Epoch 389/1000\n",
      "26/26 [==============================] - 0s 234us/step - loss: 1.6121\n",
      "Epoch 390/1000\n",
      "26/26 [==============================] - 0s 283us/step - loss: 1.6085\n",
      "Epoch 391/1000\n",
      "26/26 [==============================] - 0s 270us/step - loss: 1.6049\n",
      "Epoch 392/1000\n",
      "26/26 [==============================] - 0s 261us/step - loss: 1.6014\n",
      "Epoch 393/1000\n",
      "26/26 [==============================] - 0s 285us/step - loss: 1.5978\n",
      "Epoch 394/1000\n",
      "26/26 [==============================] - 0s 261us/step - loss: 1.5943\n",
      "Epoch 395/1000\n",
      "26/26 [==============================] - 0s 338us/step - loss: 1.5908\n",
      "Epoch 396/1000\n",
      "26/26 [==============================] - 0s 303us/step - loss: 1.5874\n",
      "Epoch 397/1000\n",
      "26/26 [==============================] - 0s 321us/step - loss: 1.5839\n",
      "Epoch 398/1000\n",
      "26/26 [==============================] - 0s 418us/step - loss: 1.5804\n",
      "Epoch 399/1000\n",
      "26/26 [==============================] - 0s 270us/step - loss: 1.5769\n",
      "Epoch 400/1000\n",
      "26/26 [==============================] - 0s 268us/step - loss: 1.5735\n",
      "Epoch 401/1000\n",
      "26/26 [==============================] - 0s 365us/step - loss: 1.5702\n",
      "Epoch 402/1000\n",
      "26/26 [==============================] - 0s 275us/step - loss: 1.5669\n",
      "Epoch 403/1000\n",
      "26/26 [==============================] - 0s 378us/step - loss: 1.5636\n",
      "Epoch 404/1000\n",
      "26/26 [==============================] - 0s 302us/step - loss: 1.5603\n",
      "Epoch 405/1000\n",
      "26/26 [==============================] - 0s 279us/step - loss: 1.5570\n",
      "Epoch 406/1000\n",
      "26/26 [==============================] - 0s 366us/step - loss: 1.5538\n",
      "Epoch 407/1000\n",
      "26/26 [==============================] - 0s 265us/step - loss: 1.5505\n",
      "Epoch 408/1000\n",
      "26/26 [==============================] - 0s 288us/step - loss: 1.5473\n",
      "Epoch 409/1000\n",
      "26/26 [==============================] - 0s 290us/step - loss: 1.5441\n",
      "Epoch 410/1000\n",
      "26/26 [==============================] - 0s 285us/step - loss: 1.5409\n",
      "Epoch 411/1000\n",
      "26/26 [==============================] - 0s 294us/step - loss: 1.5377\n",
      "Epoch 412/1000\n",
      "26/26 [==============================] - 0s 324us/step - loss: 1.5345\n",
      "Epoch 413/1000\n",
      "26/26 [==============================] - 0s 238us/step - loss: 1.5314\n",
      "Epoch 414/1000\n",
      "26/26 [==============================] - 0s 293us/step - loss: 1.5283\n",
      "Epoch 415/1000\n",
      "26/26 [==============================] - 0s 262us/step - loss: 1.5251\n",
      "Epoch 416/1000\n",
      "26/26 [==============================] - 0s 304us/step - loss: 1.5220\n",
      "Epoch 417/1000\n",
      "26/26 [==============================] - 0s 279us/step - loss: 1.5189\n",
      "Epoch 418/1000\n",
      "26/26 [==============================] - 0s 255us/step - loss: 1.5158\n",
      "Epoch 419/1000\n",
      "26/26 [==============================] - 0s 259us/step - loss: 1.5128\n",
      "Epoch 420/1000\n",
      "26/26 [==============================] - 0s 300us/step - loss: 1.5097\n",
      "Epoch 421/1000\n",
      "26/26 [==============================] - 0s 385us/step - loss: 1.5066\n",
      "Epoch 422/1000\n",
      "26/26 [==============================] - 0s 306us/step - loss: 1.5036\n",
      "Epoch 423/1000\n",
      "26/26 [==============================] - 0s 311us/step - loss: 1.5006\n",
      "Epoch 424/1000\n",
      "26/26 [==============================] - 0s 265us/step - loss: 1.4976\n",
      "Epoch 425/1000\n",
      "26/26 [==============================] - 0s 295us/step - loss: 1.4946\n",
      "Epoch 426/1000\n",
      "26/26 [==============================] - 0s 278us/step - loss: 1.4917\n",
      "Epoch 427/1000\n",
      "26/26 [==============================] - 0s 212us/step - loss: 1.4887\n",
      "Epoch 428/1000\n",
      "26/26 [==============================] - 0s 278us/step - loss: 1.4858\n",
      "Epoch 429/1000\n",
      "26/26 [==============================] - 0s 211us/step - loss: 1.4828\n",
      "Epoch 430/1000\n",
      "26/26 [==============================] - 0s 348us/step - loss: 1.4799\n",
      "Epoch 431/1000\n",
      "26/26 [==============================] - 0s 289us/step - loss: 1.4769\n",
      "Epoch 432/1000\n",
      "26/26 [==============================] - 0s 269us/step - loss: 1.4740\n",
      "Epoch 433/1000\n",
      "26/26 [==============================] - 0s 274us/step - loss: 1.4711\n",
      "Epoch 434/1000\n",
      "26/26 [==============================] - 0s 244us/step - loss: 1.4682\n",
      "Epoch 435/1000\n",
      "26/26 [==============================] - 0s 251us/step - loss: 1.4653\n",
      "Epoch 436/1000\n",
      "26/26 [==============================] - 0s 363us/step - loss: 1.4624\n",
      "Epoch 437/1000\n",
      "26/26 [==============================] - 0s 314us/step - loss: 1.4594\n",
      "Epoch 438/1000\n",
      "26/26 [==============================] - 0s 306us/step - loss: 1.4565\n",
      "Epoch 439/1000\n",
      "26/26 [==============================] - 0s 250us/step - loss: 1.4536\n",
      "Epoch 440/1000\n",
      "26/26 [==============================] - 0s 293us/step - loss: 1.4508\n",
      "Epoch 441/1000\n",
      "26/26 [==============================] - 0s 281us/step - loss: 1.4479\n",
      "Epoch 442/1000\n",
      "26/26 [==============================] - 0s 314us/step - loss: 1.4450\n",
      "Epoch 443/1000\n",
      "26/26 [==============================] - 0s 277us/step - loss: 1.4421\n",
      "Epoch 444/1000\n",
      "26/26 [==============================] - 0s 315us/step - loss: 1.4393\n",
      "Epoch 445/1000\n",
      "26/26 [==============================] - 0s 369us/step - loss: 1.4364\n",
      "Epoch 446/1000\n",
      "26/26 [==============================] - 0s 306us/step - loss: 1.4336\n",
      "Epoch 447/1000\n",
      "26/26 [==============================] - 0s 303us/step - loss: 1.4308\n",
      "Epoch 448/1000\n",
      "26/26 [==============================] - 0s 253us/step - loss: 1.4280\n",
      "Epoch 449/1000\n",
      "26/26 [==============================] - 0s 326us/step - loss: 1.4252\n",
      "Epoch 450/1000\n",
      "26/26 [==============================] - 0s 287us/step - loss: 1.4224\n",
      "Epoch 451/1000\n",
      "26/26 [==============================] - 0s 257us/step - loss: 1.4196\n",
      "Epoch 452/1000\n",
      "26/26 [==============================] - 0s 345us/step - loss: 1.4168\n",
      "Epoch 453/1000\n",
      "26/26 [==============================] - 0s 343us/step - loss: 1.4141\n",
      "Epoch 454/1000\n",
      "26/26 [==============================] - 0s 279us/step - loss: 1.4113\n",
      "Epoch 455/1000\n",
      "26/26 [==============================] - 0s 310us/step - loss: 1.4085\n",
      "Epoch 456/1000\n",
      "26/26 [==============================] - 0s 328us/step - loss: 1.4058\n",
      "Epoch 457/1000\n",
      "26/26 [==============================] - 0s 302us/step - loss: 1.4031\n",
      "Epoch 458/1000\n",
      "26/26 [==============================] - 0s 338us/step - loss: 1.4003\n",
      "Epoch 459/1000\n",
      "26/26 [==============================] - 0s 312us/step - loss: 1.3976\n",
      "Epoch 460/1000\n",
      "26/26 [==============================] - 0s 478us/step - loss: 1.3949\n",
      "Epoch 461/1000\n",
      "26/26 [==============================] - 0s 355us/step - loss: 1.3921\n",
      "Epoch 462/1000\n",
      "26/26 [==============================] - 0s 424us/step - loss: 1.3894\n",
      "Epoch 463/1000\n",
      "26/26 [==============================] - 0s 410us/step - loss: 1.3867\n",
      "Epoch 464/1000\n",
      "26/26 [==============================] - 0s 397us/step - loss: 1.3840\n",
      "Epoch 465/1000\n",
      "26/26 [==============================] - 0s 311us/step - loss: 1.3813\n",
      "Epoch 466/1000\n",
      "26/26 [==============================] - 0s 261us/step - loss: 1.3786\n",
      "Epoch 467/1000\n",
      "26/26 [==============================] - 0s 272us/step - loss: 1.3759\n",
      "Epoch 468/1000\n",
      "26/26 [==============================] - 0s 420us/step - loss: 1.3732\n",
      "Epoch 469/1000\n",
      "26/26 [==============================] - 0s 264us/step - loss: 1.3705\n",
      "Epoch 470/1000\n",
      "26/26 [==============================] - 0s 309us/step - loss: 1.3679\n",
      "Epoch 471/1000\n",
      "26/26 [==============================] - 0s 351us/step - loss: 1.3652\n",
      "Epoch 472/1000\n",
      "26/26 [==============================] - 0s 241us/step - loss: 1.3625\n",
      "Epoch 473/1000\n",
      "26/26 [==============================] - 0s 294us/step - loss: 1.3599\n",
      "Epoch 474/1000\n",
      "26/26 [==============================] - 0s 278us/step - loss: 1.3572\n",
      "Epoch 475/1000\n",
      "26/26 [==============================] - 0s 231us/step - loss: 1.3546\n",
      "Epoch 476/1000\n",
      "26/26 [==============================] - 0s 249us/step - loss: 1.3519\n",
      "Epoch 477/1000\n",
      "26/26 [==============================] - 0s 277us/step - loss: 1.3493\n",
      "Epoch 478/1000\n",
      "26/26 [==============================] - 0s 268us/step - loss: 1.3467\n",
      "Epoch 479/1000\n",
      "26/26 [==============================] - 0s 269us/step - loss: 1.3440\n",
      "Epoch 480/1000\n",
      "26/26 [==============================] - 0s 258us/step - loss: 1.3415\n",
      "Epoch 481/1000\n",
      "26/26 [==============================] - 0s 224us/step - loss: 1.3388\n",
      "Epoch 482/1000\n",
      "26/26 [==============================] - 0s 547us/step - loss: 1.3362\n",
      "Epoch 483/1000\n",
      "26/26 [==============================] - 0s 326us/step - loss: 1.3336\n",
      "Epoch 484/1000\n",
      "26/26 [==============================] - 0s 430us/step - loss: 1.3310\n",
      "Epoch 485/1000\n",
      "26/26 [==============================] - 0s 350us/step - loss: 1.3284\n",
      "Epoch 486/1000\n",
      "26/26 [==============================] - 0s 419us/step - loss: 1.3258\n",
      "Epoch 487/1000\n",
      "26/26 [==============================] - 0s 414us/step - loss: 1.3233\n",
      "Epoch 488/1000\n",
      "26/26 [==============================] - 0s 322us/step - loss: 1.3207\n",
      "Epoch 489/1000\n",
      "26/26 [==============================] - 0s 268us/step - loss: 1.3181\n",
      "Epoch 490/1000\n",
      "26/26 [==============================] - 0s 309us/step - loss: 1.3156\n",
      "Epoch 491/1000\n",
      "26/26 [==============================] - 0s 255us/step - loss: 1.3130\n",
      "Epoch 492/1000\n",
      "26/26 [==============================] - 0s 248us/step - loss: 1.3105\n",
      "Epoch 493/1000\n",
      "26/26 [==============================] - 0s 334us/step - loss: 1.3080\n",
      "Epoch 494/1000\n",
      "26/26 [==============================] - 0s 271us/step - loss: 1.3054\n",
      "Epoch 495/1000\n",
      "26/26 [==============================] - 0s 278us/step - loss: 1.3029\n",
      "Epoch 496/1000\n",
      "26/26 [==============================] - 0s 294us/step - loss: 1.3004\n",
      "Epoch 497/1000\n",
      "26/26 [==============================] - 0s 254us/step - loss: 1.2979\n",
      "Epoch 498/1000\n",
      "26/26 [==============================] - 0s 303us/step - loss: 1.2954\n",
      "Epoch 499/1000\n",
      "26/26 [==============================] - 0s 311us/step - loss: 1.2929\n",
      "Epoch 500/1000\n",
      "26/26 [==============================] - 0s 236us/step - loss: 1.2904\n",
      "Epoch 501/1000\n",
      "26/26 [==============================] - 0s 270us/step - loss: 1.2879\n",
      "Epoch 502/1000\n",
      "26/26 [==============================] - 0s 209us/step - loss: 1.2854\n",
      "Epoch 503/1000\n",
      "26/26 [==============================] - 0s 274us/step - loss: 1.2830\n",
      "Epoch 504/1000\n",
      "26/26 [==============================] - 0s 313us/step - loss: 1.2805\n",
      "Epoch 505/1000\n",
      "26/26 [==============================] - 0s 325us/step - loss: 1.2780\n",
      "Epoch 506/1000\n",
      "26/26 [==============================] - 0s 300us/step - loss: 1.2756\n",
      "Epoch 507/1000\n",
      "26/26 [==============================] - 0s 322us/step - loss: 1.2731\n",
      "Epoch 508/1000\n",
      "26/26 [==============================] - 0s 272us/step - loss: 1.2707\n",
      "Epoch 509/1000\n",
      "26/26 [==============================] - 0s 231us/step - loss: 1.2682\n",
      "Epoch 510/1000\n",
      "26/26 [==============================] - 0s 232us/step - loss: 1.2658\n",
      "Epoch 511/1000\n",
      "26/26 [==============================] - 0s 287us/step - loss: 1.2633\n",
      "Epoch 512/1000\n",
      "26/26 [==============================] - 0s 310us/step - loss: 1.2609\n",
      "Epoch 513/1000\n",
      "26/26 [==============================] - 0s 228us/step - loss: 1.2585\n",
      "Epoch 514/1000\n",
      "26/26 [==============================] - 0s 341us/step - loss: 1.2561\n",
      "Epoch 515/1000\n",
      "26/26 [==============================] - 0s 346us/step - loss: 1.2537\n",
      "Epoch 516/1000\n",
      "26/26 [==============================] - 0s 307us/step - loss: 1.2513\n",
      "Epoch 517/1000\n",
      "26/26 [==============================] - 0s 260us/step - loss: 1.2489\n",
      "Epoch 518/1000\n",
      "26/26 [==============================] - 0s 217us/step - loss: 1.2465\n",
      "Epoch 519/1000\n",
      "26/26 [==============================] - 0s 263us/step - loss: 1.2442\n",
      "Epoch 520/1000\n",
      "26/26 [==============================] - 0s 282us/step - loss: 1.2418\n",
      "Epoch 521/1000\n",
      "26/26 [==============================] - 0s 332us/step - loss: 1.2394\n",
      "Epoch 522/1000\n",
      "26/26 [==============================] - 0s 260us/step - loss: 1.2371\n",
      "Epoch 523/1000\n",
      "26/26 [==============================] - 0s 328us/step - loss: 1.2347\n",
      "Epoch 524/1000\n",
      "26/26 [==============================] - 0s 208us/step - loss: 1.2324\n",
      "Epoch 525/1000\n",
      "26/26 [==============================] - 0s 247us/step - loss: 1.2300\n",
      "Epoch 526/1000\n",
      "26/26 [==============================] - 0s 264us/step - loss: 1.2277\n",
      "Epoch 527/1000\n",
      "26/26 [==============================] - 0s 270us/step - loss: 1.2253\n",
      "Epoch 528/1000\n",
      "26/26 [==============================] - 0s 238us/step - loss: 1.2230\n",
      "Epoch 529/1000\n",
      "26/26 [==============================] - 0s 244us/step - loss: 1.2206\n",
      "Epoch 530/1000\n",
      "26/26 [==============================] - 0s 363us/step - loss: 1.2183\n",
      "Epoch 531/1000\n",
      "26/26 [==============================] - 0s 326us/step - loss: 1.2160\n",
      "Epoch 532/1000\n",
      "26/26 [==============================] - 0s 226us/step - loss: 1.2137\n",
      "Epoch 533/1000\n",
      "26/26 [==============================] - 0s 344us/step - loss: 1.2114\n",
      "Epoch 534/1000\n",
      "26/26 [==============================] - 0s 206us/step - loss: 1.2091\n",
      "Epoch 535/1000\n",
      "26/26 [==============================] - 0s 251us/step - loss: 1.2068\n",
      "Epoch 536/1000\n",
      "26/26 [==============================] - 0s 261us/step - loss: 1.2045\n",
      "Epoch 537/1000\n",
      "26/26 [==============================] - 0s 258us/step - loss: 1.2022\n",
      "Epoch 538/1000\n",
      "26/26 [==============================] - 0s 222us/step - loss: 1.1999\n",
      "Epoch 539/1000\n",
      "26/26 [==============================] - 0s 268us/step - loss: 1.1976\n",
      "Epoch 540/1000\n",
      "26/26 [==============================] - 0s 274us/step - loss: 1.1954\n",
      "Epoch 541/1000\n",
      "26/26 [==============================] - 0s 251us/step - loss: 1.1931\n",
      "Epoch 542/1000\n",
      "26/26 [==============================] - 0s 253us/step - loss: 1.1908\n",
      "Epoch 543/1000\n",
      "26/26 [==============================] - 0s 231us/step - loss: 1.1886\n",
      "Epoch 544/1000\n",
      "26/26 [==============================] - 0s 357us/step - loss: 1.1864\n",
      "Epoch 545/1000\n",
      "26/26 [==============================] - 0s 292us/step - loss: 1.1841\n",
      "Epoch 546/1000\n",
      "26/26 [==============================] - 0s 507us/step - loss: 1.1819\n",
      "Epoch 547/1000\n",
      "26/26 [==============================] - 0s 260us/step - loss: 1.1796\n",
      "Epoch 548/1000\n",
      "26/26 [==============================] - 0s 288us/step - loss: 1.1774\n",
      "Epoch 549/1000\n",
      "26/26 [==============================] - 0s 318us/step - loss: 1.1752\n",
      "Epoch 550/1000\n",
      "26/26 [==============================] - 0s 388us/step - loss: 1.1729\n",
      "Epoch 551/1000\n",
      "26/26 [==============================] - 0s 356us/step - loss: 1.1707\n",
      "Epoch 552/1000\n",
      "26/26 [==============================] - 0s 278us/step - loss: 1.1685\n",
      "Epoch 553/1000\n",
      "26/26 [==============================] - 0s 264us/step - loss: 1.1663\n",
      "Epoch 554/1000\n",
      "26/26 [==============================] - 0s 267us/step - loss: 1.1641\n",
      "Epoch 555/1000\n",
      "26/26 [==============================] - 0s 287us/step - loss: 1.1619\n",
      "Epoch 556/1000\n",
      "26/26 [==============================] - 0s 272us/step - loss: 1.1597\n",
      "Epoch 557/1000\n",
      "26/26 [==============================] - 0s 264us/step - loss: 1.1575\n",
      "Epoch 558/1000\n",
      "26/26 [==============================] - 0s 301us/step - loss: 1.1554\n",
      "Epoch 559/1000\n",
      "26/26 [==============================] - 0s 276us/step - loss: 1.1532\n",
      "Epoch 560/1000\n",
      "26/26 [==============================] - 0s 306us/step - loss: 1.1510\n",
      "Epoch 561/1000\n",
      "26/26 [==============================] - 0s 327us/step - loss: 1.1488\n",
      "Epoch 562/1000\n",
      "26/26 [==============================] - 0s 275us/step - loss: 1.1467\n",
      "Epoch 563/1000\n",
      "26/26 [==============================] - 0s 303us/step - loss: 1.1445\n",
      "Epoch 564/1000\n",
      "26/26 [==============================] - 0s 293us/step - loss: 1.1424\n",
      "Epoch 565/1000\n",
      "26/26 [==============================] - 0s 266us/step - loss: 1.1402\n",
      "Epoch 566/1000\n",
      "26/26 [==============================] - 0s 265us/step - loss: 1.1381\n",
      "Epoch 567/1000\n",
      "26/26 [==============================] - 0s 275us/step - loss: 1.1359\n",
      "Epoch 568/1000\n",
      "26/26 [==============================] - 0s 277us/step - loss: 1.1338\n",
      "Epoch 569/1000\n",
      "26/26 [==============================] - 0s 341us/step - loss: 1.1317\n",
      "Epoch 570/1000\n",
      "26/26 [==============================] - 0s 277us/step - loss: 1.1296\n",
      "Epoch 571/1000\n",
      "26/26 [==============================] - 0s 318us/step - loss: 1.1275\n",
      "Epoch 572/1000\n",
      "26/26 [==============================] - 0s 325us/step - loss: 1.1254\n",
      "Epoch 573/1000\n",
      "26/26 [==============================] - 0s 314us/step - loss: 1.1233\n",
      "Epoch 574/1000\n",
      "26/26 [==============================] - 0s 286us/step - loss: 1.1212\n",
      "Epoch 575/1000\n",
      "26/26 [==============================] - 0s 266us/step - loss: 1.1191\n",
      "Epoch 576/1000\n",
      "26/26 [==============================] - 0s 331us/step - loss: 1.1170\n",
      "Epoch 577/1000\n",
      "26/26 [==============================] - 0s 267us/step - loss: 1.1150\n",
      "Epoch 578/1000\n",
      "26/26 [==============================] - 0s 278us/step - loss: 1.1129\n",
      "Epoch 579/1000\n",
      "26/26 [==============================] - 0s 270us/step - loss: 1.1108\n",
      "Epoch 580/1000\n",
      "26/26 [==============================] - 0s 281us/step - loss: 1.1088\n",
      "Epoch 581/1000\n",
      "26/26 [==============================] - 0s 350us/step - loss: 1.1067\n",
      "Epoch 582/1000\n",
      "26/26 [==============================] - 0s 240us/step - loss: 1.1047\n",
      "Epoch 583/1000\n",
      "26/26 [==============================] - 0s 247us/step - loss: 1.1026\n",
      "Epoch 584/1000\n",
      "26/26 [==============================] - 0s 302us/step - loss: 1.1006\n",
      "Epoch 585/1000\n",
      "26/26 [==============================] - 0s 257us/step - loss: 1.0985\n",
      "Epoch 586/1000\n",
      "26/26 [==============================] - 0s 244us/step - loss: 1.0965\n",
      "Epoch 587/1000\n",
      "26/26 [==============================] - 0s 244us/step - loss: 1.0945\n",
      "Epoch 588/1000\n",
      "26/26 [==============================] - 0s 233us/step - loss: 1.0925\n",
      "Epoch 589/1000\n",
      "26/26 [==============================] - 0s 295us/step - loss: 1.0905\n",
      "Epoch 590/1000\n",
      "26/26 [==============================] - 0s 247us/step - loss: 1.0885\n",
      "Epoch 591/1000\n",
      "26/26 [==============================] - 0s 275us/step - loss: 1.0865\n",
      "Epoch 592/1000\n",
      "26/26 [==============================] - 0s 237us/step - loss: 1.0845\n",
      "Epoch 593/1000\n",
      "26/26 [==============================] - 0s 291us/step - loss: 1.0825\n",
      "Epoch 594/1000\n",
      "26/26 [==============================] - 0s 277us/step - loss: 1.0805\n",
      "Epoch 595/1000\n",
      "26/26 [==============================] - 0s 253us/step - loss: 1.0786\n",
      "Epoch 596/1000\n",
      "26/26 [==============================] - 0s 248us/step - loss: 1.0766\n",
      "Epoch 597/1000\n",
      "26/26 [==============================] - 0s 266us/step - loss: 1.0747\n",
      "Epoch 598/1000\n",
      "26/26 [==============================] - 0s 295us/step - loss: 1.0727\n",
      "Epoch 599/1000\n",
      "26/26 [==============================] - 0s 244us/step - loss: 1.0707\n",
      "Epoch 600/1000\n",
      "26/26 [==============================] - 0s 297us/step - loss: 1.0688\n",
      "Epoch 601/1000\n",
      "26/26 [==============================] - 0s 302us/step - loss: 1.0668\n",
      "Epoch 602/1000\n",
      "26/26 [==============================] - 0s 248us/step - loss: 1.0649\n",
      "Epoch 603/1000\n",
      "26/26 [==============================] - 0s 262us/step - loss: 1.0629\n",
      "Epoch 604/1000\n",
      "26/26 [==============================] - 0s 280us/step - loss: 1.0610\n",
      "Epoch 605/1000\n",
      "26/26 [==============================] - 0s 252us/step - loss: 1.0591\n",
      "Epoch 606/1000\n",
      "26/26 [==============================] - 0s 305us/step - loss: 1.0571\n",
      "Epoch 607/1000\n",
      "26/26 [==============================] - 0s 281us/step - loss: 1.0552\n",
      "Epoch 608/1000\n",
      "26/26 [==============================] - 0s 297us/step - loss: 1.0533\n",
      "Epoch 609/1000\n",
      "26/26 [==============================] - 0s 301us/step - loss: 1.0513\n",
      "Epoch 610/1000\n",
      "26/26 [==============================] - 0s 235us/step - loss: 1.0494\n",
      "Epoch 611/1000\n",
      "26/26 [==============================] - 0s 244us/step - loss: 1.0475\n",
      "Epoch 612/1000\n",
      "26/26 [==============================] - 0s 310us/step - loss: 1.0456\n",
      "Epoch 613/1000\n",
      "26/26 [==============================] - 0s 244us/step - loss: 1.0437\n",
      "Epoch 614/1000\n",
      "26/26 [==============================] - 0s 279us/step - loss: 1.0419\n",
      "Epoch 615/1000\n",
      "26/26 [==============================] - 0s 227us/step - loss: 1.0400\n",
      "Epoch 616/1000\n",
      "26/26 [==============================] - 0s 229us/step - loss: 1.0381\n",
      "Epoch 617/1000\n",
      "26/26 [==============================] - 0s 327us/step - loss: 1.0362\n",
      "Epoch 618/1000\n",
      "26/26 [==============================] - 0s 274us/step - loss: 1.0344\n",
      "Epoch 619/1000\n",
      "26/26 [==============================] - 0s 309us/step - loss: 1.0325\n",
      "Epoch 620/1000\n",
      "26/26 [==============================] - 0s 281us/step - loss: 1.0307\n",
      "Epoch 621/1000\n",
      "26/26 [==============================] - 0s 297us/step - loss: 1.0288\n",
      "Epoch 622/1000\n",
      "26/26 [==============================] - 0s 255us/step - loss: 1.0270\n",
      "Epoch 623/1000\n",
      "26/26 [==============================] - 0s 300us/step - loss: 1.0251\n",
      "Epoch 624/1000\n",
      "26/26 [==============================] - 0s 269us/step - loss: 1.0233\n",
      "Epoch 625/1000\n",
      "26/26 [==============================] - 0s 331us/step - loss: 1.0215\n",
      "Epoch 626/1000\n",
      "26/26 [==============================] - 0s 254us/step - loss: 1.0197\n",
      "Epoch 627/1000\n",
      "26/26 [==============================] - 0s 261us/step - loss: 1.0179\n",
      "Epoch 628/1000\n",
      "26/26 [==============================] - 0s 293us/step - loss: 1.0161\n",
      "Epoch 629/1000\n",
      "26/26 [==============================] - 0s 234us/step - loss: 1.0143\n",
      "Epoch 630/1000\n",
      "26/26 [==============================] - 0s 311us/step - loss: 1.0125\n",
      "Epoch 631/1000\n",
      "26/26 [==============================] - 0s 256us/step - loss: 1.0107\n",
      "Epoch 632/1000\n",
      "26/26 [==============================] - 0s 255us/step - loss: 1.0090\n",
      "Epoch 633/1000\n",
      "26/26 [==============================] - 0s 281us/step - loss: 1.0072\n",
      "Epoch 634/1000\n",
      "26/26 [==============================] - 0s 217us/step - loss: 1.0054\n",
      "Epoch 635/1000\n",
      "26/26 [==============================] - 0s 301us/step - loss: 1.0037\n",
      "Epoch 636/1000\n",
      "26/26 [==============================] - 0s 381us/step - loss: 1.0019\n",
      "Epoch 637/1000\n",
      "26/26 [==============================] - 0s 332us/step - loss: 1.0001\n",
      "Epoch 638/1000\n",
      "26/26 [==============================] - 0s 272us/step - loss: 0.9984\n",
      "Epoch 639/1000\n",
      "26/26 [==============================] - 0s 270us/step - loss: 0.9966\n",
      "Epoch 640/1000\n",
      "26/26 [==============================] - 0s 335us/step - loss: 0.9949\n",
      "Epoch 641/1000\n",
      "26/26 [==============================] - 0s 319us/step - loss: 0.9932\n",
      "Epoch 642/1000\n",
      "26/26 [==============================] - 0s 315us/step - loss: 0.9914\n",
      "Epoch 643/1000\n",
      "26/26 [==============================] - 0s 285us/step - loss: 0.9897\n",
      "Epoch 644/1000\n",
      "26/26 [==============================] - 0s 270us/step - loss: 0.9880\n",
      "Epoch 645/1000\n",
      "26/26 [==============================] - 0s 278us/step - loss: 0.9862\n",
      "Epoch 646/1000\n",
      "26/26 [==============================] - 0s 268us/step - loss: 0.9845\n",
      "Epoch 647/1000\n",
      "26/26 [==============================] - 0s 330us/step - loss: 0.9828\n",
      "Epoch 648/1000\n",
      "26/26 [==============================] - 0s 313us/step - loss: 0.9811\n",
      "Epoch 649/1000\n",
      "26/26 [==============================] - 0s 251us/step - loss: 0.9794\n",
      "Epoch 650/1000\n",
      "26/26 [==============================] - 0s 246us/step - loss: 0.9777\n",
      "Epoch 651/1000\n",
      "26/26 [==============================] - 0s 379us/step - loss: 0.9760\n",
      "Epoch 652/1000\n",
      "26/26 [==============================] - 0s 334us/step - loss: 0.9743\n",
      "Epoch 653/1000\n",
      "26/26 [==============================] - 0s 303us/step - loss: 0.9726\n",
      "Epoch 654/1000\n",
      "26/26 [==============================] - 0s 281us/step - loss: 0.9710\n",
      "Epoch 655/1000\n",
      "26/26 [==============================] - 0s 357us/step - loss: 0.9693\n",
      "Epoch 656/1000\n",
      "26/26 [==============================] - 0s 302us/step - loss: 0.9676\n",
      "Epoch 657/1000\n",
      "26/26 [==============================] - 0s 287us/step - loss: 0.9659\n",
      "Epoch 658/1000\n",
      "26/26 [==============================] - 0s 247us/step - loss: 0.9643\n",
      "Epoch 659/1000\n",
      "26/26 [==============================] - 0s 298us/step - loss: 0.9626\n",
      "Epoch 660/1000\n",
      "26/26 [==============================] - 0s 355us/step - loss: 0.9610\n",
      "Epoch 661/1000\n",
      "26/26 [==============================] - 0s 336us/step - loss: 0.9593\n",
      "Epoch 662/1000\n",
      "26/26 [==============================] - 0s 304us/step - loss: 0.9577\n",
      "Epoch 663/1000\n",
      "26/26 [==============================] - 0s 248us/step - loss: 0.9560\n",
      "Epoch 664/1000\n",
      "26/26 [==============================] - 0s 347us/step - loss: 0.9544\n",
      "Epoch 665/1000\n",
      "26/26 [==============================] - 0s 335us/step - loss: 0.9527\n",
      "Epoch 666/1000\n",
      "26/26 [==============================] - 0s 269us/step - loss: 0.9512\n",
      "Epoch 667/1000\n",
      "26/26 [==============================] - 0s 302us/step - loss: 0.9495\n",
      "Epoch 668/1000\n",
      "26/26 [==============================] - 0s 272us/step - loss: 0.9479\n",
      "Epoch 669/1000\n",
      "26/26 [==============================] - 0s 230us/step - loss: 0.9463\n",
      "Epoch 670/1000\n",
      "26/26 [==============================] - 0s 301us/step - loss: 0.9447\n",
      "Epoch 671/1000\n",
      "26/26 [==============================] - 0s 608us/step - loss: 0.9431\n",
      "Epoch 672/1000\n",
      "26/26 [==============================] - 0s 486us/step - loss: 0.9415\n",
      "Epoch 673/1000\n",
      "26/26 [==============================] - 0s 370us/step - loss: 0.9399\n",
      "Epoch 674/1000\n",
      "26/26 [==============================] - 0s 540us/step - loss: 0.9383\n",
      "Epoch 675/1000\n",
      "26/26 [==============================] - 0s 278us/step - loss: 0.9368\n",
      "Epoch 676/1000\n",
      "26/26 [==============================] - 0s 269us/step - loss: 0.9352\n",
      "Epoch 677/1000\n",
      "26/26 [==============================] - 0s 259us/step - loss: 0.9337\n",
      "Epoch 678/1000\n",
      "26/26 [==============================] - 0s 292us/step - loss: 0.9321\n",
      "Epoch 679/1000\n",
      "26/26 [==============================] - 0s 289us/step - loss: 0.9305\n",
      "Epoch 680/1000\n",
      "26/26 [==============================] - 0s 301us/step - loss: 0.9290\n",
      "Epoch 681/1000\n",
      "26/26 [==============================] - 0s 248us/step - loss: 0.9275\n",
      "Epoch 682/1000\n",
      "26/26 [==============================] - 0s 248us/step - loss: 0.9259\n",
      "Epoch 683/1000\n",
      "26/26 [==============================] - 0s 294us/step - loss: 0.9244\n",
      "Epoch 684/1000\n",
      "26/26 [==============================] - 0s 317us/step - loss: 0.9229\n",
      "Epoch 685/1000\n",
      "26/26 [==============================] - 0s 336us/step - loss: 0.9213\n",
      "Epoch 686/1000\n",
      "26/26 [==============================] - 0s 362us/step - loss: 0.9198\n",
      "Epoch 687/1000\n",
      "26/26 [==============================] - 0s 318us/step - loss: 0.9183\n",
      "Epoch 688/1000\n",
      "26/26 [==============================] - 0s 282us/step - loss: 0.9168\n",
      "Epoch 689/1000\n",
      "26/26 [==============================] - 0s 368us/step - loss: 0.9153\n",
      "Epoch 690/1000\n",
      "26/26 [==============================] - 0s 261us/step - loss: 0.9138\n",
      "Epoch 691/1000\n",
      "26/26 [==============================] - 0s 356us/step - loss: 0.9123\n",
      "Epoch 692/1000\n",
      "26/26 [==============================] - 0s 406us/step - loss: 0.9109\n",
      "Epoch 693/1000\n",
      "26/26 [==============================] - 0s 334us/step - loss: 0.9094\n",
      "Epoch 694/1000\n",
      "26/26 [==============================] - 0s 298us/step - loss: 0.9079\n",
      "Epoch 695/1000\n",
      "26/26 [==============================] - 0s 298us/step - loss: 0.9064\n",
      "Epoch 696/1000\n",
      "26/26 [==============================] - 0s 425us/step - loss: 0.9050\n",
      "Epoch 697/1000\n",
      "26/26 [==============================] - 0s 354us/step - loss: 0.9035\n",
      "Epoch 698/1000\n",
      "26/26 [==============================] - 0s 298us/step - loss: 0.9021\n",
      "Epoch 699/1000\n",
      "26/26 [==============================] - 0s 385us/step - loss: 0.9006\n",
      "Epoch 700/1000\n",
      "26/26 [==============================] - 0s 547us/step - loss: 0.8992\n",
      "Epoch 701/1000\n",
      "26/26 [==============================] - 0s 309us/step - loss: 0.8978\n",
      "Epoch 702/1000\n",
      "26/26 [==============================] - 0s 266us/step - loss: 0.8964\n",
      "Epoch 703/1000\n",
      "26/26 [==============================] - 0s 366us/step - loss: 0.8950\n",
      "Epoch 704/1000\n",
      "26/26 [==============================] - 0s 369us/step - loss: 0.8936\n",
      "Epoch 705/1000\n",
      "26/26 [==============================] - 0s 245us/step - loss: 0.8922\n",
      "Epoch 706/1000\n",
      "26/26 [==============================] - 0s 306us/step - loss: 0.8908\n",
      "Epoch 707/1000\n",
      "26/26 [==============================] - 0s 270us/step - loss: 0.8894\n",
      "Epoch 708/1000\n",
      "26/26 [==============================] - 0s 324us/step - loss: 0.8880\n",
      "Epoch 709/1000\n",
      "26/26 [==============================] - 0s 270us/step - loss: 0.8867\n",
      "Epoch 710/1000\n",
      "26/26 [==============================] - 0s 264us/step - loss: 0.8853\n",
      "Epoch 711/1000\n",
      "26/26 [==============================] - 0s 307us/step - loss: 0.8839\n",
      "Epoch 712/1000\n",
      "26/26 [==============================] - 0s 295us/step - loss: 0.8826\n",
      "Epoch 713/1000\n",
      "26/26 [==============================] - 0s 245us/step - loss: 0.8812\n",
      "Epoch 714/1000\n",
      "26/26 [==============================] - 0s 367us/step - loss: 0.8798\n",
      "Epoch 715/1000\n",
      "26/26 [==============================] - 0s 276us/step - loss: 0.8785\n",
      "Epoch 716/1000\n",
      "26/26 [==============================] - 0s 321us/step - loss: 0.8772\n",
      "Epoch 717/1000\n",
      "26/26 [==============================] - 0s 264us/step - loss: 0.8759\n",
      "Epoch 718/1000\n",
      "26/26 [==============================] - 0s 282us/step - loss: 0.8745\n",
      "Epoch 719/1000\n",
      "26/26 [==============================] - 0s 349us/step - loss: 0.8732\n",
      "Epoch 720/1000\n",
      "26/26 [==============================] - 0s 291us/step - loss: 0.8719\n",
      "Epoch 721/1000\n",
      "26/26 [==============================] - 0s 347us/step - loss: 0.8705\n",
      "Epoch 722/1000\n",
      "26/26 [==============================] - 0s 349us/step - loss: 0.8693\n",
      "Epoch 723/1000\n",
      "26/26 [==============================] - 0s 250us/step - loss: 0.8680\n",
      "Epoch 724/1000\n",
      "26/26 [==============================] - 0s 332us/step - loss: 0.8666\n",
      "Epoch 725/1000\n",
      "26/26 [==============================] - 0s 334us/step - loss: 0.8653\n",
      "Epoch 726/1000\n",
      "26/26 [==============================] - 0s 349us/step - loss: 0.8641\n",
      "Epoch 727/1000\n",
      "26/26 [==============================] - 0s 280us/step - loss: 0.8628\n",
      "Epoch 728/1000\n",
      "26/26 [==============================] - 0s 316us/step - loss: 0.8615\n",
      "Epoch 729/1000\n",
      "26/26 [==============================] - 0s 347us/step - loss: 0.8602\n",
      "Epoch 730/1000\n",
      "26/26 [==============================] - 0s 395us/step - loss: 0.8589\n",
      "Epoch 731/1000\n",
      "26/26 [==============================] - 0s 282us/step - loss: 0.8577\n",
      "Epoch 732/1000\n",
      "26/26 [==============================] - 0s 404us/step - loss: 0.8564\n",
      "Epoch 733/1000\n",
      "26/26 [==============================] - 0s 333us/step - loss: 0.8551\n",
      "Epoch 734/1000\n",
      "26/26 [==============================] - 0s 321us/step - loss: 0.8539\n",
      "Epoch 735/1000\n",
      "26/26 [==============================] - 0s 453us/step - loss: 0.8526\n",
      "Epoch 736/1000\n",
      "26/26 [==============================] - 0s 445us/step - loss: 0.8514\n",
      "Epoch 737/1000\n",
      "26/26 [==============================] - 0s 415us/step - loss: 0.8501\n",
      "Epoch 738/1000\n",
      "26/26 [==============================] - 0s 290us/step - loss: 0.8489\n",
      "Epoch 739/1000\n",
      "26/26 [==============================] - 0s 408us/step - loss: 0.8477\n",
      "Epoch 740/1000\n",
      "26/26 [==============================] - 0s 290us/step - loss: 0.8465\n",
      "Epoch 741/1000\n",
      "26/26 [==============================] - 0s 266us/step - loss: 0.8452\n",
      "Epoch 742/1000\n",
      "26/26 [==============================] - 0s 372us/step - loss: 0.8440\n",
      "Epoch 743/1000\n",
      "26/26 [==============================] - 0s 512us/step - loss: 0.8429\n",
      "Epoch 744/1000\n",
      "26/26 [==============================] - 0s 452us/step - loss: 0.8416\n",
      "Epoch 745/1000\n",
      "26/26 [==============================] - 0s 215us/step - loss: 0.8404\n",
      "Epoch 746/1000\n",
      "26/26 [==============================] - 0s 226us/step - loss: 0.8392\n",
      "Epoch 747/1000\n",
      "26/26 [==============================] - 0s 293us/step - loss: 0.8380\n",
      "Epoch 748/1000\n",
      "26/26 [==============================] - 0s 327us/step - loss: 0.8368\n",
      "Epoch 749/1000\n",
      "26/26 [==============================] - 0s 290us/step - loss: 0.8357\n",
      "Epoch 750/1000\n",
      "26/26 [==============================] - 0s 406us/step - loss: 0.8345\n",
      "Epoch 751/1000\n",
      "26/26 [==============================] - 0s 353us/step - loss: 0.8333\n",
      "Epoch 752/1000\n",
      "26/26 [==============================] - 0s 484us/step - loss: 0.8321\n",
      "Epoch 753/1000\n",
      "26/26 [==============================] - 0s 325us/step - loss: 0.8310\n",
      "Epoch 754/1000\n",
      "26/26 [==============================] - 0s 390us/step - loss: 0.8298\n",
      "Epoch 755/1000\n",
      "26/26 [==============================] - 0s 356us/step - loss: 0.8286\n",
      "Epoch 756/1000\n",
      "26/26 [==============================] - 0s 313us/step - loss: 0.8275\n",
      "Epoch 757/1000\n",
      "26/26 [==============================] - 0s 369us/step - loss: 0.8263\n",
      "Epoch 758/1000\n",
      "26/26 [==============================] - 0s 389us/step - loss: 0.8252\n",
      "Epoch 759/1000\n",
      "26/26 [==============================] - 0s 303us/step - loss: 0.8241\n",
      "Epoch 760/1000\n",
      "26/26 [==============================] - 0s 347us/step - loss: 0.8229\n",
      "Epoch 761/1000\n",
      "26/26 [==============================] - 0s 368us/step - loss: 0.8218\n",
      "Epoch 762/1000\n",
      "26/26 [==============================] - 0s 241us/step - loss: 0.8206\n",
      "Epoch 763/1000\n",
      "26/26 [==============================] - 0s 285us/step - loss: 0.8196\n",
      "Epoch 764/1000\n",
      "26/26 [==============================] - 0s 285us/step - loss: 0.8184\n",
      "Epoch 765/1000\n",
      "26/26 [==============================] - 0s 293us/step - loss: 0.8173\n",
      "Epoch 766/1000\n",
      "26/26 [==============================] - 0s 290us/step - loss: 0.8162\n",
      "Epoch 767/1000\n",
      "26/26 [==============================] - 0s 272us/step - loss: 0.8151\n",
      "Epoch 768/1000\n",
      "26/26 [==============================] - 0s 323us/step - loss: 0.8140\n",
      "Epoch 769/1000\n",
      "26/26 [==============================] - 0s 301us/step - loss: 0.8129\n",
      "Epoch 770/1000\n",
      "26/26 [==============================] - 0s 304us/step - loss: 0.8118\n",
      "Epoch 771/1000\n",
      "26/26 [==============================] - 0s 466us/step - loss: 0.8107\n",
      "Epoch 772/1000\n",
      "26/26 [==============================] - 0s 389us/step - loss: 0.8096\n",
      "Epoch 773/1000\n",
      "26/26 [==============================] - 0s 291us/step - loss: 0.8086\n",
      "Epoch 774/1000\n",
      "26/26 [==============================] - 0s 313us/step - loss: 0.8074\n",
      "Epoch 775/1000\n",
      "26/26 [==============================] - 0s 263us/step - loss: 0.8064\n",
      "Epoch 776/1000\n",
      "26/26 [==============================] - 0s 329us/step - loss: 0.8053\n",
      "Epoch 777/1000\n",
      "26/26 [==============================] - 0s 336us/step - loss: 0.8043\n",
      "Epoch 778/1000\n",
      "26/26 [==============================] - 0s 262us/step - loss: 0.8032\n",
      "Epoch 779/1000\n",
      "26/26 [==============================] - 0s 335us/step - loss: 0.8022\n",
      "Epoch 780/1000\n",
      "26/26 [==============================] - 0s 588us/step - loss: 0.8011\n",
      "Epoch 781/1000\n",
      "26/26 [==============================] - 0s 287us/step - loss: 0.8001\n",
      "Epoch 782/1000\n",
      "26/26 [==============================] - 0s 224us/step - loss: 0.7991\n",
      "Epoch 783/1000\n",
      "26/26 [==============================] - 0s 617us/step - loss: 0.7981\n",
      "Epoch 784/1000\n",
      "26/26 [==============================] - 0s 387us/step - loss: 0.7970\n",
      "Epoch 785/1000\n",
      "26/26 [==============================] - 0s 343us/step - loss: 0.7960\n",
      "Epoch 786/1000\n",
      "26/26 [==============================] - 0s 410us/step - loss: 0.7950\n",
      "Epoch 787/1000\n",
      "26/26 [==============================] - 0s 597us/step - loss: 0.7940\n",
      "Epoch 788/1000\n",
      "26/26 [==============================] - 0s 360us/step - loss: 0.7930\n",
      "Epoch 789/1000\n",
      "26/26 [==============================] - 0s 458us/step - loss: 0.7920\n",
      "Epoch 790/1000\n",
      "26/26 [==============================] - 0s 355us/step - loss: 0.7911\n",
      "Epoch 791/1000\n",
      "26/26 [==============================] - 0s 264us/step - loss: 0.7901\n",
      "Epoch 792/1000\n",
      "26/26 [==============================] - 0s 454us/step - loss: 0.7891\n",
      "Epoch 793/1000\n",
      "26/26 [==============================] - 0s 313us/step - loss: 0.7882\n",
      "Epoch 794/1000\n",
      "26/26 [==============================] - 0s 333us/step - loss: 0.7873\n",
      "Epoch 795/1000\n",
      "26/26 [==============================] - 0s 251us/step - loss: 0.7864\n",
      "Epoch 796/1000\n",
      "26/26 [==============================] - 0s 365us/step - loss: 0.7854\n",
      "Epoch 797/1000\n",
      "26/26 [==============================] - 0s 285us/step - loss: 0.7845\n",
      "Epoch 798/1000\n",
      "26/26 [==============================] - 0s 378us/step - loss: 0.7836\n",
      "Epoch 799/1000\n",
      "26/26 [==============================] - 0s 341us/step - loss: 0.7827\n",
      "Epoch 800/1000\n",
      "26/26 [==============================] - 0s 298us/step - loss: 0.7817\n",
      "Epoch 801/1000\n",
      "26/26 [==============================] - 0s 408us/step - loss: 0.7809\n",
      "Epoch 802/1000\n",
      "26/26 [==============================] - 0s 319us/step - loss: 0.7800\n",
      "Epoch 803/1000\n",
      "26/26 [==============================] - 0s 310us/step - loss: 0.7791\n",
      "Epoch 804/1000\n",
      "26/26 [==============================] - 0s 222us/step - loss: 0.7782\n",
      "Epoch 805/1000\n",
      "26/26 [==============================] - 0s 324us/step - loss: 0.7773\n",
      "Epoch 806/1000\n",
      "26/26 [==============================] - 0s 288us/step - loss: 0.7764\n",
      "Epoch 807/1000\n",
      "26/26 [==============================] - 0s 342us/step - loss: 0.7756\n",
      "Epoch 808/1000\n",
      "26/26 [==============================] - 0s 225us/step - loss: 0.7747\n",
      "Epoch 809/1000\n",
      "26/26 [==============================] - 0s 312us/step - loss: 0.7739\n",
      "Epoch 810/1000\n",
      "26/26 [==============================] - 0s 306us/step - loss: 0.7730\n",
      "Epoch 811/1000\n",
      "26/26 [==============================] - 0s 281us/step - loss: 0.7722\n",
      "Epoch 812/1000\n",
      "26/26 [==============================] - 0s 283us/step - loss: 0.7714\n",
      "Epoch 813/1000\n",
      "26/26 [==============================] - 0s 211us/step - loss: 0.7705\n",
      "Epoch 814/1000\n",
      "26/26 [==============================] - 0s 280us/step - loss: 0.7697\n",
      "Epoch 815/1000\n",
      "26/26 [==============================] - 0s 340us/step - loss: 0.7689\n",
      "Epoch 816/1000\n",
      "26/26 [==============================] - 0s 335us/step - loss: 0.7681\n",
      "Epoch 817/1000\n",
      "26/26 [==============================] - 0s 362us/step - loss: 0.7672\n",
      "Epoch 818/1000\n",
      "26/26 [==============================] - 0s 351us/step - loss: 0.7664\n",
      "Epoch 819/1000\n",
      "26/26 [==============================] - 0s 235us/step - loss: 0.7656\n",
      "Epoch 820/1000\n",
      "26/26 [==============================] - 0s 291us/step - loss: 0.7648\n",
      "Epoch 821/1000\n",
      "26/26 [==============================] - 0s 323us/step - loss: 0.7640\n",
      "Epoch 822/1000\n",
      "26/26 [==============================] - 0s 354us/step - loss: 0.7632\n",
      "Epoch 823/1000\n",
      "26/26 [==============================] - 0s 343us/step - loss: 0.7624\n",
      "Epoch 824/1000\n",
      "26/26 [==============================] - 0s 350us/step - loss: 0.7617\n",
      "Epoch 825/1000\n",
      "26/26 [==============================] - 0s 234us/step - loss: 0.7609\n",
      "Epoch 826/1000\n",
      "26/26 [==============================] - 0s 235us/step - loss: 0.7601\n",
      "Epoch 827/1000\n",
      "26/26 [==============================] - 0s 266us/step - loss: 0.7594\n",
      "Epoch 828/1000\n",
      "26/26 [==============================] - 0s 247us/step - loss: 0.7586\n",
      "Epoch 829/1000\n",
      "26/26 [==============================] - 0s 315us/step - loss: 0.7578\n",
      "Epoch 830/1000\n",
      "26/26 [==============================] - 0s 312us/step - loss: 0.7571\n",
      "Epoch 831/1000\n",
      "26/26 [==============================] - 0s 257us/step - loss: 0.7563\n",
      "Epoch 832/1000\n",
      "26/26 [==============================] - 0s 348us/step - loss: 0.7555\n",
      "Epoch 833/1000\n",
      "26/26 [==============================] - 0s 289us/step - loss: 0.7548\n",
      "Epoch 834/1000\n",
      "26/26 [==============================] - 0s 276us/step - loss: 0.7541\n",
      "Epoch 835/1000\n",
      "26/26 [==============================] - 0s 418us/step - loss: 0.7533\n",
      "Epoch 836/1000\n",
      "26/26 [==============================] - 0s 283us/step - loss: 0.7526\n",
      "Epoch 837/1000\n",
      "26/26 [==============================] - 0s 337us/step - loss: 0.7519\n",
      "Epoch 838/1000\n",
      "26/26 [==============================] - 0s 285us/step - loss: 0.7511\n",
      "Epoch 839/1000\n",
      "26/26 [==============================] - 0s 290us/step - loss: 0.7504\n",
      "Epoch 840/1000\n",
      "26/26 [==============================] - 0s 312us/step - loss: 0.7497\n",
      "Epoch 841/1000\n",
      "26/26 [==============================] - 0s 320us/step - loss: 0.7490\n",
      "Epoch 842/1000\n",
      "26/26 [==============================] - 0s 240us/step - loss: 0.7483\n",
      "Epoch 843/1000\n",
      "26/26 [==============================] - 0s 282us/step - loss: 0.7476\n",
      "Epoch 844/1000\n",
      "26/26 [==============================] - 0s 309us/step - loss: 0.7468\n",
      "Epoch 845/1000\n",
      "26/26 [==============================] - 0s 272us/step - loss: 0.7461\n",
      "Epoch 846/1000\n",
      "26/26 [==============================] - 0s 287us/step - loss: 0.7455\n",
      "Epoch 847/1000\n",
      "26/26 [==============================] - 0s 276us/step - loss: 0.7448\n",
      "Epoch 848/1000\n",
      "26/26 [==============================] - 0s 269us/step - loss: 0.7441\n",
      "Epoch 849/1000\n",
      "26/26 [==============================] - 0s 375us/step - loss: 0.7434\n",
      "Epoch 850/1000\n",
      "26/26 [==============================] - 0s 297us/step - loss: 0.7428\n",
      "Epoch 851/1000\n",
      "26/26 [==============================] - 0s 314us/step - loss: 0.7421\n",
      "Epoch 852/1000\n",
      "26/26 [==============================] - 0s 350us/step - loss: 0.7414\n",
      "Epoch 853/1000\n",
      "26/26 [==============================] - 0s 313us/step - loss: 0.7408\n",
      "Epoch 854/1000\n",
      "26/26 [==============================] - 0s 290us/step - loss: 0.7401\n",
      "Epoch 855/1000\n",
      "26/26 [==============================] - 0s 333us/step - loss: 0.7395\n",
      "Epoch 856/1000\n",
      "26/26 [==============================] - 0s 292us/step - loss: 0.7388\n",
      "Epoch 857/1000\n",
      "26/26 [==============================] - 0s 315us/step - loss: 0.7382\n",
      "Epoch 858/1000\n",
      "26/26 [==============================] - 0s 312us/step - loss: 0.7375\n",
      "Epoch 859/1000\n",
      "26/26 [==============================] - 0s 293us/step - loss: 0.7369\n",
      "Epoch 860/1000\n",
      "26/26 [==============================] - 0s 338us/step - loss: 0.7363\n",
      "Epoch 861/1000\n",
      "26/26 [==============================] - 0s 312us/step - loss: 0.7357\n",
      "Epoch 862/1000\n",
      "26/26 [==============================] - 0s 270us/step - loss: 0.7351\n",
      "Epoch 863/1000\n",
      "26/26 [==============================] - 0s 296us/step - loss: 0.7344\n",
      "Epoch 864/1000\n",
      "26/26 [==============================] - 0s 402us/step - loss: 0.7338\n",
      "Epoch 865/1000\n",
      "26/26 [==============================] - 0s 275us/step - loss: 0.7332\n",
      "Epoch 866/1000\n",
      "26/26 [==============================] - 0s 290us/step - loss: 0.7326\n",
      "Epoch 867/1000\n",
      "26/26 [==============================] - 0s 340us/step - loss: 0.7320\n",
      "Epoch 868/1000\n",
      "26/26 [==============================] - 0s 315us/step - loss: 0.7314\n",
      "Epoch 869/1000\n",
      "26/26 [==============================] - 0s 262us/step - loss: 0.7308\n",
      "Epoch 870/1000\n",
      "26/26 [==============================] - 0s 345us/step - loss: 0.7302\n",
      "Epoch 871/1000\n",
      "26/26 [==============================] - 0s 297us/step - loss: 0.7296\n",
      "Epoch 872/1000\n",
      "26/26 [==============================] - 0s 306us/step - loss: 0.7291\n",
      "Epoch 873/1000\n",
      "26/26 [==============================] - 0s 397us/step - loss: 0.7285\n",
      "Epoch 874/1000\n",
      "26/26 [==============================] - 0s 315us/step - loss: 0.7279\n",
      "Epoch 875/1000\n",
      "26/26 [==============================] - 0s 317us/step - loss: 0.7273\n",
      "Epoch 876/1000\n",
      "26/26 [==============================] - 0s 362us/step - loss: 0.7268\n",
      "Epoch 877/1000\n",
      "26/26 [==============================] - 0s 300us/step - loss: 0.7262\n",
      "Epoch 878/1000\n",
      "26/26 [==============================] - 0s 276us/step - loss: 0.7256\n",
      "Epoch 879/1000\n",
      "26/26 [==============================] - 0s 252us/step - loss: 0.7251\n",
      "Epoch 880/1000\n",
      "26/26 [==============================] - 0s 228us/step - loss: 0.7245\n",
      "Epoch 881/1000\n",
      "26/26 [==============================] - 0s 320us/step - loss: 0.7240\n",
      "Epoch 882/1000\n",
      "26/26 [==============================] - 0s 307us/step - loss: 0.7234\n",
      "Epoch 883/1000\n",
      "26/26 [==============================] - 0s 249us/step - loss: 0.7229\n",
      "Epoch 884/1000\n",
      "26/26 [==============================] - 0s 363us/step - loss: 0.7223\n",
      "Epoch 885/1000\n",
      "26/26 [==============================] - 0s 355us/step - loss: 0.7218\n",
      "Epoch 886/1000\n",
      "26/26 [==============================] - 0s 292us/step - loss: 0.7212\n",
      "Epoch 887/1000\n",
      "26/26 [==============================] - 0s 228us/step - loss: 0.7207\n",
      "Epoch 888/1000\n",
      "26/26 [==============================] - 0s 316us/step - loss: 0.7202\n",
      "Epoch 889/1000\n",
      "26/26 [==============================] - 0s 285us/step - loss: 0.7196\n",
      "Epoch 890/1000\n",
      "26/26 [==============================] - 0s 334us/step - loss: 0.7191\n",
      "Epoch 891/1000\n",
      "26/26 [==============================] - 0s 343us/step - loss: 0.7186\n",
      "Epoch 892/1000\n",
      "26/26 [==============================] - 0s 287us/step - loss: 0.7180\n",
      "Epoch 893/1000\n",
      "26/26 [==============================] - 0s 355us/step - loss: 0.7175\n",
      "Epoch 894/1000\n",
      "26/26 [==============================] - 0s 321us/step - loss: 0.7170\n",
      "Epoch 895/1000\n",
      "26/26 [==============================] - 0s 325us/step - loss: 0.7165\n",
      "Epoch 896/1000\n",
      "26/26 [==============================] - 0s 390us/step - loss: 0.7160\n",
      "Epoch 897/1000\n",
      "26/26 [==============================] - 0s 355us/step - loss: 0.7155\n",
      "Epoch 898/1000\n",
      "26/26 [==============================] - 0s 244us/step - loss: 0.7150\n",
      "Epoch 899/1000\n",
      "26/26 [==============================] - 0s 258us/step - loss: 0.7145\n",
      "Epoch 900/1000\n",
      "26/26 [==============================] - 0s 245us/step - loss: 0.7140\n",
      "Epoch 901/1000\n",
      "26/26 [==============================] - 0s 306us/step - loss: 0.7135\n",
      "Epoch 902/1000\n",
      "26/26 [==============================] - 0s 289us/step - loss: 0.7130\n",
      "Epoch 903/1000\n",
      "26/26 [==============================] - 0s 298us/step - loss: 0.7125\n",
      "Epoch 904/1000\n",
      "26/26 [==============================] - 0s 256us/step - loss: 0.7120\n",
      "Epoch 905/1000\n",
      "26/26 [==============================] - 0s 252us/step - loss: 0.7116\n",
      "Epoch 906/1000\n",
      "26/26 [==============================] - 0s 323us/step - loss: 0.7111\n",
      "Epoch 907/1000\n",
      "26/26 [==============================] - 0s 293us/step - loss: 0.7106\n",
      "Epoch 908/1000\n",
      "26/26 [==============================] - 0s 348us/step - loss: 0.7102\n",
      "Epoch 909/1000\n",
      "26/26 [==============================] - 0s 247us/step - loss: 0.7097\n",
      "Epoch 910/1000\n",
      "26/26 [==============================] - 0s 248us/step - loss: 0.7092\n",
      "Epoch 911/1000\n",
      "26/26 [==============================] - 0s 263us/step - loss: 0.7088\n",
      "Epoch 912/1000\n",
      "26/26 [==============================] - 0s 271us/step - loss: 0.7083\n",
      "Epoch 913/1000\n",
      "26/26 [==============================] - 0s 270us/step - loss: 0.7078\n",
      "Epoch 914/1000\n",
      "26/26 [==============================] - 0s 279us/step - loss: 0.7074\n",
      "Epoch 915/1000\n",
      "26/26 [==============================] - 0s 278us/step - loss: 0.7069\n",
      "Epoch 916/1000\n",
      "26/26 [==============================] - 0s 253us/step - loss: 0.7065\n",
      "Epoch 917/1000\n",
      "26/26 [==============================] - 0s 265us/step - loss: 0.7060\n",
      "Epoch 918/1000\n",
      "26/26 [==============================] - 0s 256us/step - loss: 0.7056\n",
      "Epoch 919/1000\n",
      "26/26 [==============================] - 0s 281us/step - loss: 0.7052\n",
      "Epoch 920/1000\n",
      "26/26 [==============================] - 0s 260us/step - loss: 0.7047\n",
      "Epoch 921/1000\n",
      "26/26 [==============================] - 0s 341us/step - loss: 0.7043\n",
      "Epoch 922/1000\n",
      "26/26 [==============================] - 0s 312us/step - loss: 0.7039\n",
      "Epoch 923/1000\n",
      "26/26 [==============================] - 0s 312us/step - loss: 0.7034\n",
      "Epoch 924/1000\n",
      "26/26 [==============================] - 0s 360us/step - loss: 0.7030\n",
      "Epoch 925/1000\n",
      "26/26 [==============================] - 0s 364us/step - loss: 0.7026\n",
      "Epoch 926/1000\n",
      "26/26 [==============================] - 0s 372us/step - loss: 0.7022\n",
      "Epoch 927/1000\n",
      "26/26 [==============================] - 0s 241us/step - loss: 0.7017\n",
      "Epoch 928/1000\n",
      "26/26 [==============================] - 0s 289us/step - loss: 0.7013\n",
      "Epoch 929/1000\n",
      "26/26 [==============================] - 0s 281us/step - loss: 0.7009\n",
      "Epoch 930/1000\n",
      "26/26 [==============================] - 0s 317us/step - loss: 0.7005\n",
      "Epoch 931/1000\n",
      "26/26 [==============================] - 0s 315us/step - loss: 0.7001\n",
      "Epoch 932/1000\n",
      "26/26 [==============================] - 0s 339us/step - loss: 0.6997\n",
      "Epoch 933/1000\n",
      "26/26 [==============================] - 0s 248us/step - loss: 0.6993\n",
      "Epoch 934/1000\n",
      "26/26 [==============================] - 0s 351us/step - loss: 0.6989\n",
      "Epoch 935/1000\n",
      "26/26 [==============================] - 0s 286us/step - loss: 0.6985\n",
      "Epoch 936/1000\n",
      "26/26 [==============================] - 0s 274us/step - loss: 0.6981\n",
      "Epoch 937/1000\n",
      "26/26 [==============================] - 0s 445us/step - loss: 0.6977\n",
      "Epoch 938/1000\n",
      "26/26 [==============================] - 0s 286us/step - loss: 0.6973\n",
      "Epoch 939/1000\n",
      "26/26 [==============================] - 0s 201us/step - loss: 0.6969\n",
      "Epoch 940/1000\n",
      "26/26 [==============================] - 0s 292us/step - loss: 0.6965\n",
      "Epoch 941/1000\n",
      "26/26 [==============================] - 0s 277us/step - loss: 0.6962\n",
      "Epoch 942/1000\n",
      "26/26 [==============================] - 0s 291us/step - loss: 0.6958\n",
      "Epoch 943/1000\n",
      "26/26 [==============================] - 0s 257us/step - loss: 0.6954\n",
      "Epoch 944/1000\n",
      "26/26 [==============================] - 0s 345us/step - loss: 0.6950\n",
      "Epoch 945/1000\n",
      "26/26 [==============================] - 0s 373us/step - loss: 0.6946\n",
      "Epoch 946/1000\n",
      "26/26 [==============================] - 0s 370us/step - loss: 0.6943\n",
      "Epoch 947/1000\n",
      "26/26 [==============================] - 0s 271us/step - loss: 0.6939\n",
      "Epoch 948/1000\n",
      "26/26 [==============================] - 0s 252us/step - loss: 0.6935\n",
      "Epoch 949/1000\n",
      "26/26 [==============================] - 0s 326us/step - loss: 0.6932\n",
      "Epoch 950/1000\n",
      "26/26 [==============================] - 0s 382us/step - loss: 0.6928\n",
      "Epoch 951/1000\n",
      "26/26 [==============================] - 0s 313us/step - loss: 0.6924\n",
      "Epoch 952/1000\n",
      "26/26 [==============================] - 0s 273us/step - loss: 0.6921\n",
      "Epoch 953/1000\n",
      "26/26 [==============================] - 0s 398us/step - loss: 0.6917\n",
      "Epoch 954/1000\n",
      "26/26 [==============================] - 0s 247us/step - loss: 0.6914\n",
      "Epoch 955/1000\n",
      "26/26 [==============================] - 0s 326us/step - loss: 0.6910\n",
      "Epoch 956/1000\n",
      "26/26 [==============================] - 0s 273us/step - loss: 0.6907\n",
      "Epoch 957/1000\n",
      "26/26 [==============================] - 0s 233us/step - loss: 0.6903\n",
      "Epoch 958/1000\n",
      "26/26 [==============================] - 0s 306us/step - loss: 0.6900\n",
      "Epoch 959/1000\n",
      "26/26 [==============================] - 0s 270us/step - loss: 0.6896\n",
      "Epoch 960/1000\n",
      "26/26 [==============================] - 0s 272us/step - loss: 0.6893\n",
      "Epoch 961/1000\n",
      "26/26 [==============================] - 0s 452us/step - loss: 0.6889\n",
      "Epoch 962/1000\n",
      "26/26 [==============================] - 0s 302us/step - loss: 0.6886\n",
      "Epoch 963/1000\n",
      "26/26 [==============================] - 0s 268us/step - loss: 0.6883\n",
      "Epoch 964/1000\n",
      "26/26 [==============================] - 0s 376us/step - loss: 0.6879\n",
      "Epoch 965/1000\n",
      "26/26 [==============================] - 0s 320us/step - loss: 0.6876\n",
      "Epoch 966/1000\n",
      "26/26 [==============================] - 0s 293us/step - loss: 0.6873\n",
      "Epoch 967/1000\n",
      "26/26 [==============================] - 0s 283us/step - loss: 0.6870\n",
      "Epoch 968/1000\n",
      "26/26 [==============================] - 0s 263us/step - loss: 0.6866\n",
      "Epoch 969/1000\n",
      "26/26 [==============================] - 0s 278us/step - loss: 0.6863\n",
      "Epoch 970/1000\n",
      "26/26 [==============================] - 0s 291us/step - loss: 0.6860\n",
      "Epoch 971/1000\n",
      "26/26 [==============================] - 0s 240us/step - loss: 0.6857\n",
      "Epoch 972/1000\n",
      "26/26 [==============================] - 0s 272us/step - loss: 0.6854\n",
      "Epoch 973/1000\n",
      "26/26 [==============================] - 0s 318us/step - loss: 0.6851\n",
      "Epoch 974/1000\n",
      "26/26 [==============================] - 0s 291us/step - loss: 0.6848\n",
      "Epoch 975/1000\n",
      "26/26 [==============================] - 0s 223us/step - loss: 0.6844\n",
      "Epoch 976/1000\n",
      "26/26 [==============================] - 0s 456us/step - loss: 0.6841\n",
      "Epoch 977/1000\n",
      "26/26 [==============================] - 0s 343us/step - loss: 0.6838\n",
      "Epoch 978/1000\n",
      "26/26 [==============================] - 0s 250us/step - loss: 0.6835\n",
      "Epoch 979/1000\n",
      "26/26 [==============================] - 0s 245us/step - loss: 0.6832\n",
      "Epoch 980/1000\n",
      "26/26 [==============================] - 0s 357us/step - loss: 0.6829\n",
      "Epoch 981/1000\n",
      "26/26 [==============================] - 0s 235us/step - loss: 0.6826\n",
      "Epoch 982/1000\n",
      "26/26 [==============================] - 0s 336us/step - loss: 0.6824\n",
      "Epoch 983/1000\n",
      "26/26 [==============================] - 0s 235us/step - loss: 0.6821\n",
      "Epoch 984/1000\n",
      "26/26 [==============================] - 0s 348us/step - loss: 0.6818\n",
      "Epoch 985/1000\n",
      "26/26 [==============================] - 0s 289us/step - loss: 0.6815\n",
      "Epoch 986/1000\n",
      "26/26 [==============================] - 0s 325us/step - loss: 0.6812\n",
      "Epoch 987/1000\n",
      "26/26 [==============================] - 0s 390us/step - loss: 0.6809\n",
      "Epoch 988/1000\n",
      "26/26 [==============================] - 0s 328us/step - loss: 0.6806\n",
      "Epoch 989/1000\n",
      "26/26 [==============================] - 0s 348us/step - loss: 0.6804\n",
      "Epoch 990/1000\n",
      "26/26 [==============================] - 0s 320us/step - loss: 0.6801\n",
      "Epoch 991/1000\n",
      "26/26 [==============================] - 0s 284us/step - loss: 0.6798\n",
      "Epoch 992/1000\n",
      "26/26 [==============================] - 0s 382us/step - loss: 0.6795\n",
      "Epoch 993/1000\n",
      "26/26 [==============================] - 0s 309us/step - loss: 0.6793\n",
      "Epoch 994/1000\n",
      "26/26 [==============================] - 0s 348us/step - loss: 0.6790\n",
      "Epoch 995/1000\n",
      "26/26 [==============================] - 0s 355us/step - loss: 0.6787\n",
      "Epoch 996/1000\n",
      "26/26 [==============================] - 0s 277us/step - loss: 0.6785\n",
      "Epoch 997/1000\n",
      "26/26 [==============================] - 0s 326us/step - loss: 0.6782\n",
      "Epoch 998/1000\n",
      "26/26 [==============================] - 0s 439us/step - loss: 0.6780\n",
      "Epoch 999/1000\n",
      "26/26 [==============================] - 0s 226us/step - loss: 0.6777\n",
      "Epoch 1000/1000\n",
      "26/26 [==============================] - 0s 341us/step - loss: 0.6774\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x12696e780>"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define model\n",
    "model2 = Sequential()\n",
    "model2.add(Embedding(input_dim=len(vocab) + 1,  # vocabulary size. Adding an\n",
    "                                               # extra element for <PAD> word\n",
    "                    output_dim=5,  # size of embeddings\n",
    "                    input_length=maxlen - 1))  # length of the padded sequences\n",
    "model2.add(LSTM(10))\n",
    "model2.add(Dense(len(vocab), activation='softmax'))\n",
    "model2.compile('rmsprop', 'categorical_crossentropy')\n",
    "\n",
    "# Train network\n",
    "model2.fit(x, y, epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(w=come|h=)=0.2466132789850235\n",
      "P(w=back|h=come)=0.9139593839645386\n",
      "Prob. sentence: 0.22539452715216315\n"
     ]
    }
   ],
   "source": [
    "# Compute probability of occurence of a sentence\n",
    "sentence = \"come back,\"\n",
    "tok = tokenizer.texts_to_sequences([sentence])[0]\n",
    "x_test, y_test = prepare_sentence(tok, maxlen)\n",
    "x_test = np.array(x_test)\n",
    "y_test = np.array(y_test) - 1  # The word <PAD> does not constitute a class\n",
    "p_pred = model2.predict(x_test)  # array of conditional probabilities\n",
    "vocab_inv = {v: k for k, v in vocab.items()}\n",
    "\n",
    "# Compute product\n",
    "# Efficient version: np.exp(np.sum(np.log(np.diag(p_pred[:, y_test]))))\n",
    "log_p_sentence = 0\n",
    "for i, prob in enumerate(p_pred):\n",
    "    word = vocab_inv[y_test[i]+1]  # Index 0 from vocab is reserved to <PAD>\n",
    "    history = ' '.join([vocab_inv[w] for w in x_test[i, :] if w != 0])\n",
    "    prob_word = prob[y_test[i]]\n",
    "    log_p_sentence += np.log(prob_word)\n",
    "    print('P(w={}|h={})={}'.format(word, history, prob_word))\n",
    "print('Prob. sentence: {}'.format(np.exp(log_p_sentence)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plato "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can apply this weaponry on real data. In this case a piece of Plato"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "\t# open the file as read only\n",
    "\tfile = open(filename, 'r')\n",
    "\t# read all text\n",
    "\ttext = file.read()\n",
    "\t# close the file\n",
    "\tfile.close()\n",
    "\treturn text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "BOOK I\n",
      "\n",
      "Socrates - GLAUCON \n",
      "\n",
      "I went down yesterday to the Piraeus with Glaucon the son of Ariston,\n",
      "that I might offer up my prayers to the goddess; and also because\n",
      "I wanted to see in what manner th\n"
     ]
    }
   ],
   "source": [
    "# load document\n",
    "in_filename = 'republic.txt' # this should be a translation of choice of the Republic\n",
    "doc = load_doc(in_filename)\n",
    "print(doc[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenize cleanly\n",
    "import string\n",
    "\n",
    "# turn a doc into clean tokens\n",
    "def clean_doc(doc):\n",
    "\t# replace '--' with a space ' '\n",
    "\tdoc = doc.replace('--', ' ')\n",
    "\t# split into tokens by white space\n",
    "\ttokens = doc.split()\n",
    "\t# remove punctuation from each token\n",
    "\ttable = str.maketrans('', '', string.punctuation)\n",
    "\ttokens = [w.translate(table) for w in tokens]\n",
    "\t# remove remaining tokens that are not alphabetic\n",
    "\ttokens = [word for word in tokens if word.isalpha()]\n",
    "\t# make lower case\n",
    "\ttokens = [word.lower() for word in tokens]\n",
    "\treturn tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['book', 'i', 'socrates', 'glaucon', 'i', 'went', 'down', 'yesterday', 'to', 'the', 'piraeus', 'with', 'glaucon', 'the', 'son', 'of', 'ariston', 'that', 'i', 'might', 'offer', 'up', 'my', 'prayers', 'to', 'the', 'goddess', 'and', 'also', 'because', 'i', 'wanted', 'to', 'see', 'in', 'what', 'manner', 'they', 'would', 'celebrate', 'the', 'festival', 'which', 'was', 'a', 'new', 'thing', 'i', 'was', 'delighted', 'with', 'the', 'procession', 'of', 'the', 'inhabitants', 'but', 'that', 'of', 'the', 'thracians', 'was', 'equally', 'if', 'not', 'more', 'beautiful', 'when', 'we', 'had', 'finished', 'our', 'prayers', 'and', 'viewed', 'the', 'spectacle', 'we', 'turned', 'in', 'the', 'direction', 'of', 'the', 'city', 'and', 'at', 'that', 'instant', 'polemarchus', 'the', 'son', 'of', 'cephalus', 'chanced', 'to', 'catch', 'sight', 'of', 'us', 'from', 'a', 'distance', 'as', 'we', 'were', 'starting', 'on', 'our', 'way', 'home', 'and', 'told', 'his', 'servant', 'to', 'run', 'and', 'bid', 'us', 'wait', 'for', 'him', 'the', 'servant', 'took', 'hold', 'of', 'me', 'by', 'the', 'cloak', 'behind', 'and', 'said', 'polemarchus', 'desires', 'you', 'to', 'wait', 'i', 'turned', 'round', 'and', 'asked', 'him', 'where', 'his', 'master', 'was', 'there', 'he', 'is', 'said', 'the', 'youth', 'coming', 'after', 'you', 'if', 'you', 'will', 'only', 'wait', 'certainly', 'we', 'will', 'said', 'glaucon', 'and', 'in', 'a', 'few', 'minutes', 'polemarchus', 'appeared', 'and', 'with', 'him', 'adeimantus', 'glaucons', 'brother', 'niceratus', 'the', 'son', 'of', 'nicias', 'and', 'several', 'others', 'who', 'had', 'been', 'at', 'the', 'procession', 'socrates', 'polemarchus', 'glaucon', 'adeimantus']\n",
      "Total Tokens: 118369\n",
      "Unique Tokens: 7423\n"
     ]
    }
   ],
   "source": [
    "# clean document\n",
    "tokens = clean_doc(doc)\n",
    "print(tokens[:200])\n",
    "print('Total Tokens: %d' % len(tokens))\n",
    "print('Unique Tokens: %d' % len(set(tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save tokens to file, one dialog per line\n",
    "def save_doc(lines, filename):\n",
    "\tdata = '\\n'.join(lines)\n",
    "\tfile = open(filename, 'w')\n",
    "\tfile.write(data)\n",
    "\tfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Sequences: 118318\n"
     ]
    }
   ],
   "source": [
    "# organize into sequences of tokens\n",
    "length = 50 + 1\n",
    "sequences = list()\n",
    "for i in range(length, len(tokens)):\n",
    "\t# select sequence of tokens\n",
    "\tseq = tokens[i-length:i]\n",
    "\t# convert into a line\n",
    "\tline = ' '.join(seq)\n",
    "\t# store\n",
    "\tsequences.append(line)\n",
    "print('Total Sequences: %d' % len(sequences))\n",
    "\n",
    "# save sequences to file\n",
    "out_filename = 'republic_sequences.txt'\n",
    "save_doc(sequences, out_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118318\n"
     ]
    }
   ],
   "source": [
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "\t# open the file as read only\n",
    "\tfile = open(filename, 'r')\n",
    "\t# read all text\n",
    "\ttext = file.read()\n",
    "\t# close the file\n",
    "\tfile.close()\n",
    "\treturn text\n",
    "\n",
    "# load\n",
    "in_filename = 'republic_sequences.txt'\n",
    "doc = load_doc(in_filename)\n",
    "lines = doc.split('\\n')\n",
    "print(len(lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# integer encode sequences of words\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(lines)\n",
    "sequences = tokenizer.texts_to_sequences(lines)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocabulary size\n",
    "vocab_size = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(118318, 51)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# separate into input and output\n",
    "sequences = array(sequences)\n",
    "sequences.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create x and y\n",
    "X, y = sequences[:,:-1], sequences[:,-1]\n",
    "y = to_categorical(y, num_classes=vocab_size)\n",
    "seq_length = X.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 50, 50)            800       \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 50, 100)           60400     \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 16)                1616      \n",
      "=================================================================\n",
      "Total params: 153,316\n",
      "Trainable params: 153,316\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# define model\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 50, input_length=seq_length))\n",
    "model.add(LSTM(100, return_sequences=True))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(vocab_size, activation='softmax'))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "118318/118318 [==============================] - 117s 988us/step - loss: 3.8083 - acc: 0.2392\n",
      "Epoch 2/100\n",
      "118318/118318 [==============================] - 119s 1ms/step - loss: 3.6686 - acc: 0.2498\n",
      "Epoch 3/100\n",
      "118318/118318 [==============================] - 108s 909us/step - loss: 3.6043 - acc: 0.2553\n",
      "Epoch 4/100\n",
      "118318/118318 [==============================] - 108s 910us/step - loss: 3.5528 - acc: 0.2627\n",
      "Epoch 5/100\n",
      "118318/118318 [==============================] - 107s 907us/step - loss: 3.5052 - acc: 0.2669\n",
      "Epoch 6/100\n",
      "118318/118318 [==============================] - 112s 945us/step - loss: 3.4645 - acc: 0.2722\n",
      "Epoch 7/100\n",
      "118318/118318 [==============================] - 110s 930us/step - loss: 3.4253 - acc: 0.2783\n",
      "Epoch 8/100\n",
      "118318/118318 [==============================] - 109s 923us/step - loss: 3.3861 - acc: 0.2827\n",
      "Epoch 9/100\n",
      "118318/118318 [==============================] - 110s 927us/step - loss: 3.3548 - acc: 0.2877\n",
      "Epoch 10/100\n",
      "118318/118318 [==============================] - 112s 945us/step - loss: 3.3145 - acc: 0.2922\n",
      "Epoch 11/100\n",
      "118318/118318 [==============================] - 112s 948us/step - loss: 3.2786 - acc: 0.2969\n",
      "Epoch 12/100\n",
      "118318/118318 [==============================] - 114s 960us/step - loss: 3.2442 - acc: 0.3025\n",
      "Epoch 13/100\n",
      "118318/118318 [==============================] - 112s 944us/step - loss: 3.2108 - acc: 0.3071\n",
      "Epoch 14/100\n",
      "118318/118318 [==============================] - 110s 930us/step - loss: 3.1798 - acc: 0.3112\n",
      "Epoch 15/100\n",
      "118318/118318 [==============================] - 110s 933us/step - loss: 3.1473 - acc: 0.3167\n",
      "Epoch 16/100\n",
      "118318/118318 [==============================] - 111s 936us/step - loss: 3.1229 - acc: 0.3211\n",
      "Epoch 17/100\n",
      "118318/118318 [==============================] - 110s 929us/step - loss: 3.0881 - acc: 0.3238\n",
      "Epoch 18/100\n",
      "118318/118318 [==============================] - 110s 929us/step - loss: 3.0585 - acc: 0.3292\n",
      "Epoch 19/100\n",
      "118318/118318 [==============================] - 110s 928us/step - loss: 3.0480 - acc: 0.3322\n",
      "Epoch 20/100\n",
      "118318/118318 [==============================] - 110s 928us/step - loss: 3.0111 - acc: 0.3383\n",
      "Epoch 21/100\n",
      "118318/118318 [==============================] - 116s 979us/step - loss: 3.0061 - acc: 0.3404\n",
      "Epoch 22/100\n",
      "118318/118318 [==============================] - 113s 957us/step - loss: 2.9636 - acc: 0.3457\n",
      "Epoch 23/100\n",
      "118318/118318 [==============================] - 113s 959us/step - loss: 2.9483 - acc: 0.3488\n",
      "Epoch 24/100\n",
      "118318/118318 [==============================] - 114s 963us/step - loss: 2.9515 - acc: 0.3509\n",
      "Epoch 25/100\n",
      "118318/118318 [==============================] - 114s 963us/step - loss: 2.8824 - acc: 0.3588\n",
      "Epoch 26/100\n",
      "118318/118318 [==============================] - 113s 957us/step - loss: 2.8525 - acc: 0.3641\n",
      "Epoch 27/100\n",
      "118318/118318 [==============================] - 745s 6ms/step - loss: 2.8240 - acc: 0.3680\n",
      "Epoch 28/100\n",
      "118318/118318 [==============================] - 117s 991us/step - loss: 2.7958 - acc: 0.3733\n",
      "Epoch 29/100\n",
      "118318/118318 [==============================] - 117s 991us/step - loss: 2.7697 - acc: 0.3772\n",
      "Epoch 30/100\n",
      "118318/118318 [==============================] - 114s 960us/step - loss: 2.7398 - acc: 0.3815\n",
      "Epoch 31/100\n",
      "118318/118318 [==============================] - 116s 982us/step - loss: 2.7095 - acc: 0.3870\n",
      "Epoch 32/100\n",
      "118318/118318 [==============================] - 114s 962us/step - loss: 2.6864 - acc: 0.3918\n",
      "Epoch 33/100\n",
      "118318/118318 [==============================] - 114s 967us/step - loss: 2.6594 - acc: 0.3949\n",
      "Epoch 34/100\n",
      "118318/118318 [==============================] - 113s 958us/step - loss: 2.6355 - acc: 0.3990\n",
      "Epoch 35/100\n",
      "118318/118318 [==============================] - 114s 965us/step - loss: 2.6092 - acc: 0.4044\n",
      "Epoch 36/100\n",
      "118318/118318 [==============================] - 117s 992us/step - loss: 2.5847 - acc: 0.4094\n",
      "Epoch 37/100\n",
      "118318/118318 [==============================] - 115s 970us/step - loss: 2.5617 - acc: 0.4123\n",
      "Epoch 38/100\n",
      "118318/118318 [==============================] - 115s 971us/step - loss: 2.5364 - acc: 0.4182\n",
      "Epoch 39/100\n",
      " 11648/118318 [=>............................] - ETA: 1:41 - loss: 2.3975 - acc: 0.4469"
     ]
    }
   ],
   "source": [
    "# compile model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# fit model\n",
    "model.fit(X, y, batch_size=128, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model to file\n",
    "from pickle import dump\n",
    "model.save('model.h5')\n",
    "# save the tokenizer\n",
    "dump(tokenizer, open('tokenizer.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "\t# open the file as read only\n",
    "\tfile = open(filename, 'r')\n",
    "\t# read all text\n",
    "\ttext = file.read()\n",
    "\t# close the file\n",
    "\tfile.close()\n",
    "\treturn text\n",
    "\n",
    "# load cleaned text sequences\n",
    "in_filename = 'republic_sequences.txt'\n",
    "doc = load_doc(in_filename)\n",
    "lines = doc.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model\n",
    "from keras.models import load_model\n",
    "model = load_model('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "they remain in the upper world but this must not be allowed they must be made to descend again among the prisoners in the den and partake of their labours and honours whether they are worth having or not but is not this unjust he said ought we to give them\n",
      "\n",
      "to be the founders of the interdicted are the same of the soul and the other of the soul which is the most miserable of the soul and the other of the soul which is the most miserable of the soul and the other of the soul and the other\n"
     ]
    }
   ],
   "source": [
    "#generate a babbling Plato\n",
    "\n",
    "from random import randint\n",
    "from pickle import load\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "\t# open the file as read only\n",
    "\tfile = open(filename, 'r')\n",
    "\t# read all text\n",
    "\ttext = file.read()\n",
    "\t# close the file\n",
    "\tfile.close()\n",
    "\treturn text\n",
    "\n",
    "# generate a sequence from a language model\n",
    "def generate_seq(model, tokenizer, seq_length, seed_text, n_words):\n",
    "\tresult = list()\n",
    "\tin_text = seed_text\n",
    "\t# generate a fixed number of words\n",
    "\tfor _ in range(n_words):\n",
    "\t\t# encode the text as integer\n",
    "\t\tencoded = tokenizer.texts_to_sequences([in_text])[0]\n",
    "\t\t# truncate sequences to a fixed length\n",
    "\t\tencoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')\n",
    "\t\t# predict probabilities for each word\n",
    "\t\tyhat = model.predict_classes(encoded, verbose=0)\n",
    "\t\t# map predicted word index to word\n",
    "\t\tout_word = ''\n",
    "\t\tfor word, index in tokenizer.word_index.items():\n",
    "\t\t\tif index == yhat:\n",
    "\t\t\t\tout_word = word\n",
    "\t\t\t\tbreak\n",
    "\t\t# append to input\n",
    "\t\tin_text += ' ' + out_word\n",
    "\t\tresult.append(out_word)\n",
    "\treturn ' '.join(result)\n",
    "\n",
    "# load cleaned text sequences\n",
    "in_filename = 'republic_sequences.txt'\n",
    "doc = load_doc(in_filename)\n",
    "lines = doc.split('\\n')\n",
    "seq_length = len(lines[0].split()) - 1\n",
    "\n",
    "# load the model\n",
    "model = load_model('model.h5')\n",
    "\n",
    "# load the tokenizer\n",
    "tokenizer = load(open('tokenizer.pkl', 'rb'))\n",
    "\n",
    "# select a seed text\n",
    "seed_text = lines[randint(0,len(lines))]\n",
    "print(seed_text + '\\n')\n",
    "\n",
    "# generate new text\n",
    "generated = generate_seq(model, tokenizer, seq_length, seed_text, 50)\n",
    "print(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Translation corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can use it on our corpus!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'ADJ',\n",
       " 'ADP',\n",
       " 'ADV',\n",
       " 'AUX',\n",
       " 'CONJ',\n",
       " 'DET',\n",
       " 'NOUN',\n",
       " 'NUM',\n",
       " 'PART',\n",
       " 'PRON',\n",
       " 'PROPN',\n",
       " 'PUNCT',\n",
       " 'SCONJ',\n",
       " 'VERB',\n",
       " 'X'}"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#german 'native' text\n",
    "\n",
    "deu0 = open(\"train/epuds.de.pos\").read() # here we take the Parts of Speech sequences, as is custom\n",
    "pos = set(deu0.split())\n",
    "print(len(pos))\n",
    "pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking the trans corp#\n",
    "import timeit\n",
    "\n",
    "deu = open(\"train/epuds.de.pos\").read()\n",
    "tokens = deu.split()\n",
    "print(len(tokens))\n",
    "print(tokens[:10])\n",
    "\n",
    "# organize into sequences of tokens\n",
    "length = 50 + 1\n",
    "sequences = list()\n",
    "\n",
    "for i in range(length, len(tokens)):\n",
    "\t# select sequence of tokens\n",
    "\tseq = tokens[i-length:i]\n",
    "\t# convert into a line\n",
    "\tline = ' '.join(seq)\n",
    "\t# store\n",
    "\tsequences.append(line)\n",
    "print('Total Sequences: %d' % len(sequences))\n",
    "\n",
    "# save sequences to file\n",
    "out_filename = 'train/deupos_sequences.txt'\n",
    "save_doc(sequences, out_filename)\n",
    "\n",
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "\t# open the file as read only\n",
    "\tfile = open(filename, 'r')\n",
    "\t# read all text\n",
    "\ttext = file.read()\n",
    "\t# close the file\n",
    "\tfile.close()\n",
    "\treturn text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9062886\n",
      "2224223867\n",
      "9062886\n",
      "DET NOUN DET PROPN PROPN AUX PRON PRON PUNCT PROPN PROPN NOUN PUNCT ADV ADV VERB PUNCT PRON VERB PRON ADV ADV ADV ADV ADV PUNCT SCONJ PRON ADP NUM NOUN CONJ ADJ ADV DET PROPN PROPN VERB PUNCT PRON VERB PRON PUNCT PROPN NOUN PUNCT PRON NOUN PUNCT PRON PRON\n",
      "298.66911351100134\n",
      "4.977818558516689\n",
      "40000\n",
      "16\n",
      "(40000, 51)\n"
     ]
    }
   ],
   "source": [
    "# load\n",
    "in_filename = 'train/deupos_sequences.txt'#'republic_sequences.txt'#\n",
    "doc = load_doc(in_filename)\n",
    "lines = doc.split('\\n')\n",
    "print(len(lines))\n",
    "\n",
    "print(len(doc))\n",
    "lines = doc.split(\"\\n\")\n",
    "print(len(lines))\n",
    "print(lines[10])\n",
    "\n",
    "lines = doc.split(\"\\n\")\n",
    "\n",
    "start_time = timeit.default_timer()\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(lines)\n",
    "deu_sequences = tokenizer.texts_to_sequences(lines[:40000])\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "print(elapsed)\n",
    "print(elapsed/60)\n",
    "print(len(deu_sequences))\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "#vocab_size = len(poss) + 1 #(why+1?)\n",
    "print(vocab_size)\n",
    "\n",
    "deu_sequences = array(deu_sequences)\n",
    "print(deu_sequences.shape)\n",
    "\n",
    "#smaller?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating x and y\n",
    "sequences = deu_sequences[:30000]\n",
    "X, y = sequences[:,:-1], sequences[:,-1]\n",
    "y = to_categorical(y, num_classes=vocab_size)\n",
    "seq_length = X.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_8 (Embedding)      (None, 50, 50)            800       \n",
      "_________________________________________________________________\n",
      "lstm_13 (LSTM)               (None, 50, 100)           60400     \n",
      "_________________________________________________________________\n",
      "lstm_14 (LSTM)               (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 16)                1616      \n",
      "=================================================================\n",
      "Total params: 153,316\n",
      "Trainable params: 153,316\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# define model: this is a deeper one than usual. Feel free to make it shallo by \n",
    "# commenting out lines\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 50, input_length=seq_length))\n",
    "model.add(LSTM(100, return_sequences=True))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(vocab_size, activation='softmax'))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "30000/30000 [==============================] - 20s 676us/step - loss: 0.1347 - acc: 0.9613 - perplexity2: 1.2910\n",
      "Epoch 2/10\n",
      "30000/30000 [==============================] - 21s 686us/step - loss: 0.1032 - acc: 0.9747 - perplexity2: 1.2001\n",
      "Epoch 3/10\n",
      "30000/30000 [==============================] - 21s 708us/step - loss: 0.0790 - acc: 0.9835 - perplexity2: 1.0917\n",
      "Epoch 4/10\n",
      "30000/30000 [==============================] - 21s 713us/step - loss: 0.0656 - acc: 0.9876 - perplexity2: 1.0726\n",
      "Epoch 5/10\n",
      "30000/30000 [==============================] - 21s 712us/step - loss: 0.0634 - acc: 0.9878 - perplexity2: 1.0644\n",
      "Epoch 6/10\n",
      "30000/30000 [==============================] - 21s 714us/step - loss: 0.1065 - acc: 0.9702 - perplexity2: 1.3102\n",
      "Epoch 7/10\n",
      "30000/30000 [==============================] - 21s 715us/step - loss: 0.2579 - acc: 0.9115 - perplexity2: 3.2748\n",
      "Epoch 8/10\n",
      " 8576/30000 [=======>......................] - ETA: 15s - loss: 0.1866 - acc: 0.9367 - perplexity2: 1.7217"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-327-34bb2af95df8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy', perplexity2])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# fit model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1380\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1381\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1382\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1383\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1384\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# compile model\n",
    "#model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy', perplexity2])\n",
    "# fit model\n",
    "model.fit(X, y, batch_size=128, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8.335750579833984, 0.0, 323.0806579589844] 51 83\n",
      "PRON NOUN VERB ADV DET NOUN PUNCT PRON ADP DET NOUN DET ADV ADJ NOUN VERB AUX PUNCT\n",
      "[0.03127264976501465, 1.0, 1.0219131708145142] 51 51\n",
      "[0.0796106830239296, 1.0, 1.0567328929901123] 51 119\n",
      "[4.687814712524414, 0.0, 25.773466110229492] 51 60\n",
      "[5.0307586207054555e-05, 1.0, 1.0000349283218384] 51 87\n",
      "[0.15373742580413818, 1.0, 1.1124476194381714] 51 70\n",
      "[14.637717247009277, 0.0, 25491.298828125] 51 46\n",
      "DET NOUN ADP DET NOUN VERB ADJ CONJ NOUN PUNCT\n",
      "[16.11809539794922, 0.0, 71126.296875] 51 121\n",
      "PRON VERB PRON ADP NOUN ADP DET NUM NOUN DET NOUN PUNCT NOUN PUNCT NOUN CONJ NOUN PUNCT PRON PRON ADP DET NOUN VERB PUNCT\n",
      "[1.1081022024154663, 0.0, 2.155618906021118] 51 63\n",
      "[16.11809539794922, 0.0, 71126.296875] 51 37\n",
      "ADV VERB PRON PRON PART ADV ADV PUNCT\n",
      "[7.126777648925781, 0.0, 139.75709533691406] 51 49\n",
      "PRON VERB PUNCT SCONJ DET ADV ADJ NOUN VERB PUNCT\n",
      "[13.434011459350586, 0.0, 11067.267578125] 51 82\n",
      "ADP DET NOUN DET NOUN PUNCT CONJ ADV ADP PRON ADP DET NOUN CONJ ADP ADJ NOUN PUNCT\n",
      "[0.8629988431930542, 0.0, 1.818814992904663] 51 90\n",
      "[14.29748249053955, 0.0, 20135.90234375] 51 93\n",
      "PRON AUX ADV ADJ VERB PUNCT VERB ADV ADV DET NOUN PUNCT ADV VERB DET ADJ NOUN PRON NOUN PUNCT\n",
      "[5.644692897796631, 0.0, 50.02900695800781] 51 64\n",
      "[5.052097320556641, 0.0, 33.176673889160156] 51 27\n",
      "[9.852757453918457, 0.0, 924.6460571289062] 51 45\n",
      "PRON VERB ADV PART X NOUN DET NOUN VERB PUNCT\n",
      "[7.069738388061523, 0.0, 134.33937072753906] 51 65\n",
      "DET NOUN VERB ADV DET ADV ADJ NOUN PUNCT CONJ ADV VERB NOUN PUNCT\n",
      "[5.0720906257629395, 0.0, 33.639644622802734] 51 42\n",
      "[3.883791208267212, 0.0, 14.761743545532227] 51 59\n"
     ]
    }
   ],
   "source": [
    "#evaluate\n",
    "#I take some remaining sequence\n",
    "# split into input and output elements\n",
    "for i in range(30000,30020):\n",
    "        eva_sequences = array(deu_sequences[i:i+1])\n",
    "        X1, y1 = eva_sequences[:,:-1],eva_sequences[:,-1]\n",
    "        y1 = to_categorical(y1, num_classes=vocab_size)\n",
    "\n",
    "        eva = model.evaluate(X1,y1, verbose=0)\n",
    "        \n",
    "        print(eva, len(eva_sequences[0]), len(deu_lines[i]))\n",
    "        if eva[2]>100: print(deu_lines[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3677632\n",
      "900822859\n",
      "number of lines:  3677632\n",
      "VERB PUNCT PRON ADP PRON PROPN NOUN PART VERB PUNCT PRON AUX PRON NOUN ADP ADJ NOUN ADP PRON ADP NOUN PRON NOUN DET NOUN VERB PUNCT ADP ADJ NOUN VERB PRON PROPN ADP DET ADP DET NOUN PRON ADJ NOUN ADJ NOUN CONJ VERB PRON ADP PRON NOUN ADP ADJ\n",
      "85.28726735099917\n",
      "40000\n",
      "16\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# load\n",
    "in_filename = 'train/eng2deu_pos_sequences.txt'#'republic_sequences.txt'#\n",
    "doc = load_doc(in_filename)\n",
    "lines = doc.split('\\n')\n",
    "print(len(lines))\n",
    "\n",
    "print(len(doc))\n",
    "lines = doc.split(\"\\n\")\n",
    "print(\"number of lines: \",len(lines))\n",
    "print(lines[10])\n",
    "\n",
    "lines = doc.split(\"\\n\")\n",
    "\n",
    "start_time = timeit.default_timer()\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(lines)\n",
    "eng2deu_sequences = tokenizer.texts_to_sequences(lines[:40000])\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "print(elapsed)\n",
    "\n",
    "print(len(eng2deu_sequences))\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "#vocab_size = len(poss) + 1 #(why+1?)\n",
    "print(vocab_size)\n",
    "\n",
    "eng2deu_sequences = array(eng2deu_sequences)\n",
    "eng2deu_sequences.shape\n",
    "\n",
    "#smaller?\n",
    "eng2deu_sequences = eng2deu_sequences[:30000]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.007377952802926302, 1.0, 1.0051270723342896] 51 70\n",
      "[0.0988958328962326, 1.0, 1.0709534883499146] 51 75\n",
      "[0.0001467574038542807, 1.0, 1.0001016855239868] 51 28\n",
      "[0.007961110211908817, 1.0, 1.0055334568023682] 51 53\n",
      "[0.02812901884317398, 1.0, 1.0196888446807861] 51 58\n",
      "[0.04729606211185455, 1.0, 1.0333263874053955] 51 115\n",
      "[0.029552677646279335, 1.0, 1.0206955671310425] 51 77\n",
      "[0.004812656901776791, 1.0, 1.0033414363861084] 51 63\n",
      "[0.05605776607990265, 1.0, 1.039621114730835] 51 80\n",
      "[0.05483783781528473, 1.0, 1.0387423038482666] 51 33\n",
      "[1.392953634262085, 0.0, 2.626157760620117] 51 81\n",
      "[4.768382950715022e-06, 1.0, 1.0000033378601074] 51 156\n",
      "[0.011362489312887192, 1.0, 1.0079070329666138] 51 147\n",
      "[0.0011420808732509613, 1.0, 1.0007919073104858] 51 170\n",
      "[0.008442174643278122, 1.0, 1.0058687925338745] 51 206\n",
      "[0.03188585117459297, 1.0, 1.0223476886749268] 51 200\n",
      "[0.04800765588879585, 1.0, 1.0338362455368042] 51 148\n",
      "[0.015820838510990143, 1.0, 1.0110265016555786] 51 81\n",
      "[2.837221290974412e-05, 1.0, 1.0000196695327759] 51 87\n",
      "[0.3291422426700592, 1.0, 1.2562662363052368] 51 244\n",
      "[2.201917886734009, 0.0, 4.600905895233154] 51 131\n",
      "AUX PRON DET NOUN PROPN PUNCT PRON DET NOUN DET NOUN PUNCT CONJ NOUN PUNCT ADV DET NOUN ADP PROPN ADJ PUNCT ADJ NOUN VERB AUX PUNCT\n",
      "[0.24122527241706848, 1.0, 1.1819961071014404] 51 183\n",
      "[0.1183961033821106, 1.0, 1.0855274200439453] 51 129\n",
      "[0.0009359924006275833, 1.0, 1.0006489753723145] 51 138\n",
      "[0.0028326658066362143, 1.0, 1.0019654035568237] 51 106\n",
      "[0.007496250327676535, 1.0, 1.0052095651626587] 51 99\n",
      "[0.04910670593380928, 1.0, 1.0346240997314453] 51 72\n",
      "[0.010452167131006718, 1.0, 1.00727117061615] 51 98\n",
      "[0.0170865748077631, 1.0, 1.0119138956069946] 51 72\n",
      "[0.05310322344303131, 1.0, 1.03749418258667] 51 62\n",
      "[9.716029308037832e-05, 1.0, 1.0000673532485962] 51 109\n",
      "[0.003300205571576953, 1.0, 1.00229012966156] 51 166\n",
      "[0.006712503731250763, 1.0, 1.0046635866165161] 51 33\n",
      "[0.025344010442495346, 1.0, 1.0177223682403564] 51 224\n",
      "[0.00012332962069194764, 1.0, 1.000085473060608] 51 78\n",
      "[0.02007582224905491, 1.0, 1.0140128135681152] 51 57\n",
      "[0.007059759460389614, 1.0, 1.0049054622650146] 51 154\n",
      "[0.08955243974924088, 1.0, 1.0640400648117065] 51 182\n",
      "[0.19737160205841064, 1.0, 1.146607518196106] 51 241\n",
      "[0.04356996342539787, 1.0, 1.0306611061096191] 51 210\n",
      "[0.09870154410600662, 1.0, 1.070809245109558] 51 171\n",
      "[0.050721265375614166, 1.0, 1.0357825756072998] 51 138\n",
      "[0.005269558634608984, 1.0, 1.0036592483520508] 51 140\n",
      "[8.344653679159819e-07, 1.0, 1.0000005960464478] 51 274\n",
      "[0.0027284258976578712, 1.0, 1.0018930435180664] 51 151\n",
      "[0.0009320548269897699, 1.0, 1.0006462335586548] 51 198\n",
      "[0.465984046459198, 1.0, 1.3812592029571533] 51 193\n",
      "[0.005286276340484619, 1.0, 1.0036709308624268] 51 149\n",
      "[0.005945200566202402, 1.0, 1.004129409790039] 51 95\n",
      "[0.02517150342464447, 1.0, 1.017600655555725] 51 242\n",
      "[0.1617564558982849, 1.0, 1.1186482906341553] 51 206\n",
      "[9.787561430130154e-05, 1.0, 1.0000678300857544] 51 287\n",
      "[0.0004162462428212166, 1.0, 1.0002886056900024] 51 25\n",
      "[0.6380292773246765, 1.0, 1.5562019348144531] 51 76\n",
      "[1.530424952507019, 0.0, 2.88870906829834] 51 117\n",
      "[0.024451354518532753, 1.0, 1.0170928239822388] 51 159\n",
      "[0.06946083158254623, 1.0, 1.0493245124816895] 51 238\n",
      "[0.008723768405616283, 1.0, 1.0060651302337646] 51 62\n",
      "[0.0029075650963932276, 1.0, 1.0020173788070679] 51 225\n",
      "[0.05604269728064537, 1.0, 1.0396101474761963] 51 135\n",
      "[0.16756750643253326, 1.0, 1.123163104057312] 51 151\n",
      "[0.004384519066661596, 1.0, 1.0030437707901] 51 98\n",
      "[0.04441218450665474, 1.0, 1.0312628746032715] 51 130\n",
      "[0.022260615602135658, 1.0, 1.0155495405197144] 51 258\n",
      "[3.60614612873178e-05, 1.0, 1.0000250339508057] 51 125\n",
      "[0.367342472076416, 1.0, 1.2899744510650635] 51 154\n",
      "[0.13064618408679962, 1.0, 1.094783902168274] 51 241\n",
      "[0.013958867639303207, 1.0, 1.0097224712371826] 51 78\n",
      "[0.14268270134925842, 1.0, 1.1039559841156006] 51 119\n",
      "[0.05083194747567177, 1.0, 1.0358620882034302] 51 278\n",
      "[0.02149445004761219, 1.0, 1.0150103569030762] 51 320\n",
      "[0.08736038953065872, 1.0, 1.0624245405197144] 51 202\n",
      "[0.053636565804481506, 1.0, 1.0378777980804443] 51 136\n",
      "[0.0019063034560531378, 1.0, 1.0013222694396973] 51 104\n",
      "[0.25125595927238464, 1.0, 1.190242886543274] 51 59\n",
      "[0.0009601551573723555, 1.0, 1.0006657838821411] 51 169\n",
      "[0.10761813074350357, 1.0, 1.0774478912353516] 51 9\n",
      "[0.0029375143349170685, 1.0, 1.0020382404327393] 51 59\n",
      "[0.0007879857439547777, 1.0, 1.0005463361740112] 51 73\n",
      "[3.099489549640566e-05, 1.0, 1.0000214576721191] 51 67\n",
      "[0.0012149439426138997, 1.0, 1.0008424520492554] 51 58\n",
      "[0.00010622111585689709, 1.0, 1.0000736713409424] 51 154\n",
      "[0.00017304725770372897, 1.0, 1.000119924545288] 51 24\n",
      "[0.0003975227882619947, 1.0, 1.0002756118774414] 51 62\n",
      "[0.0009870630456134677, 1.0, 1.000684380531311] 51 23\n",
      "[0.0009009126806631684, 1.0, 1.000624656677246] 51 64\n",
      "[0.08070988953113556, 1.0, 1.0575382709503174] 51 103\n",
      "[0.05632831156253815, 1.0, 1.0398160219192505] 51 33\n",
      "[0.009785090573132038, 1.0, 1.0068055391311646] 51 29\n",
      "[0.0008685187203809619, 1.0, 1.0006022453308105] 51 84\n",
      "[0.042158737778663635, 1.0, 1.0296533107757568] 51 42\n",
      "[0.004994086921215057, 1.0, 1.0034676790237427] 51 84\n",
      "[0.12270025163888931, 1.0, 1.0887707471847534] 51 36\n",
      "[0.15064102411270142, 1.0, 1.110062599182129] 51 103\n",
      "[0.002278119558468461, 1.0, 1.0015803575515747] 51 158\n",
      "[0.14846469461917877, 1.0, 1.1083892583847046] 51 77\n",
      "[0.012998474761843681, 1.0, 1.0090506076812744] 51 171\n",
      "[9.966393554350361e-05, 1.0, 1.0000691413879395] 51 187\n",
      "[0.0006125656655058265, 1.0, 1.0004247426986694] 51 128\n",
      "[0.00046752888010814786, 1.0, 1.0003241300582886] 51 58\n",
      "[0.06654184311628342, 1.0, 1.047203540802002] 51 195\n",
      "[0.15716931223869324, 1.0, 1.1150970458984375] 51 109\n",
      "[1.2447341680526733, 0.0, 2.369748830795288] 51 214\n",
      "[0.00803182739764452, 1.0, 1.0055828094482422] 51 79\n",
      "[0.02747921273112297, 1.0, 1.0192296504974365] 51 80\n",
      "[0.0003310393658466637, 1.0, 1.0002294778823853] 51 97\n",
      "[0.07116596400737762, 1.0, 1.0505653619766235] 51 253\n",
      "[0.1432267129421234, 1.0, 1.1043723821640015] 51 40\n",
      "[0.21978694200515747, 1.0, 1.1645616292953491] 51 41\n",
      "[0.00563789252191782, 1.0, 1.003915548324585] 51 119\n",
      "[0.000976920360699296, 1.0, 1.0006773471832275] 51 365\n",
      "[0.023330552503466606, 1.0, 1.0163029432296753] 51 121\n",
      "[0.0009550242684781551, 1.0, 1.0006622076034546] 51 142\n",
      "[0.024400170892477036, 1.0, 1.0170567035675049] 51 179\n",
      "[0.7604767084121704, 0.0, 1.6940503120422363] 51 182\n",
      "[0.32624945044517517, 1.0, 1.2537497282028198] 51 152\n",
      "[0.0050766365602612495, 1.0, 1.0035250186920166] 51 55\n",
      "[0.06311163306236267, 1.0, 1.0447165966033936] 51 188\n",
      "[0.3841998875141144, 1.0, 1.305135726928711] 51 213\n",
      "[0.10221267491579056, 1.0, 1.0734184980392456] 51 231\n",
      "[0.05547795072197914, 1.0, 1.0392032861709595] 51 68\n",
      "[0.04910138249397278, 1.0, 1.0346202850341797] 51 85\n",
      "[0.056200187653303146, 1.0, 1.0397237539291382] 51 201\n",
      "[0.27317318320274353, 1.0, 1.2084629535675049] 51 34\n",
      "[4.255862222635187e-05, 1.0, 1.000029444694519] 51 55\n",
      "[0.055518653243780136, 1.0, 1.039232611656189] 51 251\n",
      "[4.178372910246253e-05, 1.0, 1.0000289678573608] 51 178\n",
      "[0.1377909779548645, 1.0, 1.1002192497253418] 51 90\n",
      "[0.2005964070558548, 1.0, 1.149173378944397] 51 191\n",
      "[0.005648142192512751, 1.0, 1.003922700881958] 51 107\n",
      "[0.006451751105487347, 1.0, 1.0044820308685303] 51 101\n",
      "[0.0008157838019542396, 1.0, 1.0005656480789185] 51 69\n",
      "[0.00043634159374050796, 1.0, 1.0003025531768799] 51 42\n",
      "[0.4393841028213501, 1.0, 1.3560253381729126] 51 140\n",
      "[0.0005092725623399019, 1.0, 1.0003530979156494] 51 93\n",
      "[0.12850727140903473, 1.0, 1.0931620597839355] 51 216\n",
      "[0.0030803990084677935, 1.0, 1.0021374225616455] 51 105\n",
      "[5.960466182841628e-07, 1.0, 1.0000003576278687] 51 32\n",
      "[0.010356945917010307, 1.0, 1.0072047710418701] 51 178\n",
      "[0.03764839097857475, 1.0, 1.0264393091201782] 51 107\n",
      "[0.02570366859436035, 1.0, 1.0179760456085205] 51 114\n",
      "[0.003796922042965889, 1.0, 1.0026352405548096] 51 152\n",
      "[0.0017317021265625954, 1.0, 1.0012010335922241] 51 136\n",
      "[0.0009032990201376379, 1.0, 1.0006263256072998] 51 159\n",
      "[1.7183854579925537, 0.0, 3.290679454803467] 51 81\n",
      "PRON AUX PRON PART VERB PUNCT ADV NOUN CONJ PROPN NOUN DET PROPN PROPN VERB PUNCT\n",
      "[0.07002899050712585, 1.0, 1.049737811088562] 51 209\n",
      "[0.08867831528186798, 1.0, 1.0633955001831055] 51 137\n",
      "[0.4027758240699768, 1.0, 1.3220491409301758] 51 153\n",
      "[0.028026767075061798, 1.0, 1.0196166038513184] 51 210\n",
      "[0.14202089607715607, 1.0, 1.1034497022628784] 51 107\n",
      "[0.001293362700380385, 1.0, 1.00089693069458] 51 96\n",
      "[0.00035244476748630404, 1.0, 1.000244379043579] 51 45\n",
      "[1.6826339960098267, 0.0, 3.210134983062744] 51 128\n",
      "PRON VERB ADP PRON NOUN ADV PART VERB AUX PUNCT CONJ AUX PRON DET ADV ADJ ADJ NOUN DET NOUN VERB CONJ NOUN PROPN PROPN ADV PUNCT\n",
      "[0.021995412185788155, 1.0, 1.0153628587722778] 51 99\n",
      "[0.0012921690940856934, 1.0, 1.0008960962295532] 51 85\n",
      "[0.000385418301448226, 1.0, 1.0002671480178833] 51 32\n",
      "[0.014491265639662743, 1.0, 1.010095238685608] 51 205\n",
      "[0.021600481122732162, 1.0, 1.015084981918335] 51 128\n",
      "[0.0019219497917219996, 1.0, 1.0013331174850464] 51 136\n",
      "[0.3769075572490692, 1.0, 1.2985553741455078] 51 81\n",
      "[0.1505509465932846, 1.0, 1.1099933385849] 51 166\n",
      "[0.0617208257317543, 1.0, 1.0437099933624268] 51 150\n",
      "[0.0357988066971302, 1.0, 1.0251243114471436] 51 121\n",
      "[7.867844033171423e-06, 1.0, 1.0000054836273193] 51 228\n",
      "[0.000899480888620019, 1.0, 1.0006237030029297] 51 209\n",
      "[0.15192116796970367, 1.0, 1.1110479831695557] 51 81\n",
      "[0.0016268007457256317, 1.0, 1.0011281967163086] 51 223\n",
      "[0.00699781347066164, 1.0, 1.0048623085021973] 51 109\n",
      "[0.020881924778223038, 1.0, 1.0145795345306396] 51 90\n",
      "[0.024396630004048347, 1.0, 1.0170542001724243] 51 54\n",
      "[0.023135701194405556, 1.0, 1.0161657333374023] 51 121\n",
      "[0.003769639180973172, 1.0, 1.002616286277771] 51 207\n",
      "[0.0032492559403181076, 1.0, 1.0022547245025635] 51 134\n",
      "[0.002280270215123892, 1.0, 1.0015817880630493] 51 111\n",
      "[0.03462209179997444, 1.0, 1.024288535118103] 51 71\n",
      "[1.0938611030578613, 0.0, 2.1344451904296875] 51 92\n",
      "[0.007274318486452103, 1.0, 1.0050549507141113] 51 23\n",
      "[0.007111383602023125, 1.0, 1.004941463470459] 51 64\n",
      "[5.400326699600555e-05, 1.0, 1.000037431716919] 51 118\n",
      "[0.008691481314599514, 1.0, 1.006042718887329] 51 62\n",
      "[0.03853699192404747, 1.0, 1.0270717144012451] 51 90\n",
      "[0.08716098964214325, 1.0, 1.0622776746749878] 51 117\n",
      "[2.3841860752327193e-07, 1.0, 1.0000001192092896] 51 148\n",
      "[0.22745051980018616, 1.0, 1.1707642078399658] 51 138\n",
      "[0.004721564706414938, 1.0, 1.003278136253357] 51 45\n",
      "[0.09972190856933594, 1.0, 1.0715669393539429] 51 38\n",
      "[0.0727008655667305, 1.0, 1.0516836643218994] 51 37\n",
      "[0.0070615001022815704, 1.0, 1.0049066543579102] 51 28\n",
      "[0.026991108432412148, 1.0, 1.0188848972320557] 51 45\n",
      "[0.0002532922080717981, 1.0, 1.0001755952835083] 51 66\n",
      "[0.013191784732043743, 1.0, 1.009185791015625] 51 99\n",
      "[0.00022443666239269078, 1.0, 1.0001555681228638] 51 79\n",
      "[0.013956570997834206, 1.0, 1.0097209215164185] 51 293\n",
      "[0.013228625990450382, 1.0, 1.009211540222168] 51 58\n",
      "[0.001406406401656568, 1.0, 1.0009753704071045] 51 211\n",
      "[0.09372283518314362, 1.0, 1.0671203136444092] 51 279\n",
      "[0.007259668782353401, 1.0, 1.00504469871521] 51 303\n",
      "[0.009622047655284405, 1.0, 1.006691813468933] 51 133\n",
      "[0.5930227637290955, 1.0, 1.5084038972854614] 51 126\n",
      "[0.00037939593312330544, 1.0, 1.000262975692749] 51 138\n",
      "[3.37964047503192e-05, 1.0, 1.0000234842300415] 51 46\n",
      "[0.01666799932718277, 1.0, 1.0116204023361206] 51 84\n",
      "[0.0008586157928220928, 1.0, 1.0005953311920166] 51 88\n",
      "[0.14736983180046082, 1.0, 1.107548475265503] 51 204\n",
      "[0.46372485160827637, 1.0, 1.379097819328308] 51 80\n",
      "[3.796887904172763e-05, 1.0, 1.0000263452529907] 51 105\n",
      "[0.0002361218648729846, 1.0, 1.0001636743545532] 51 42\n",
      "[0.003834137925878167, 1.0, 1.002661108970642] 51 83\n",
      "[0.15577425062656403, 1.0, 1.1140192747116089] 51 183\n",
      "[0.07592500001192093, 1.0, 1.0540366172790527] 51 91\n",
      "[0.002020670799538493, 1.0, 1.001401662826538] 51 126\n",
      "[0.15765537321567535, 1.0, 1.1154727935791016] 51 44\n",
      "[0.00039925199234858155, 1.0, 1.000276803970337] 51 123\n",
      "[0.01049890648573637, 1.0, 1.0073038339614868] 51 61\n",
      "[0.21659384667873383, 1.0, 1.1619869470596313] 51 170\n",
      "[0.033609047532081604, 1.0, 1.0235694646835327] 51 159\n",
      "[0.039899688214063644, 1.0, 1.0280423164367676] 51 231\n",
      "[0.00026193709345534444, 1.0, 1.0001815557479858] 51 126\n",
      "[0.01479217316955328, 1.0, 1.0103058815002441] 51 112\n",
      "[1.3113108252582606e-05, 1.0, 1.0000090599060059] 51 80\n",
      "[0.1810322254896164, 1.0, 1.1336947679519653] 51 31\n",
      "[0.0017254925332963467, 1.0, 1.0011967420578003] 51 41\n",
      "[1.1920928955078125e-07, 1.0, 1.0000001192092896] 51 72\n",
      "[0.11630361527204514, 1.0, 1.083954095840454] 51 137\n",
      "[0.00148555648047477, 1.0, 1.0010302066802979] 51 124\n",
      "[0.0019610668532550335, 1.0, 1.0013601779937744] 51 136\n",
      "[0.05609893053770065, 1.0, 1.0396506786346436] 51 93\n",
      "[0.014364882372319698, 1.0, 1.0100066661834717] 51 213\n",
      "[0.06011076271533966, 1.0, 1.0425457954406738] 51 152\n",
      "[0.00246697710826993, 1.0, 1.0017114877700806] 51 57\n",
      "[0.000846147770062089, 1.0, 1.0005866289138794] 51 93\n",
      "[4.500251816352829e-05, 1.0, 1.0000312328338623] 51 59\n",
      "[0.24662116169929504, 1.0, 1.1864252090454102] 51 190\n",
      "[0.006857909262180328, 1.0, 1.0047649145126343] 51 47\n",
      "[0.1088639348745346, 1.0, 1.078378677368164] 51 121\n",
      "[0.009853951632976532, 1.0, 1.0068535804748535] 51 37\n",
      "[0.009582269005477428, 1.0, 1.0066640377044678] 51 85\n",
      "[0.0006141163758002222, 1.0, 1.0004258155822754] 51 32\n",
      "[0.011656967923045158, 1.0, 1.0081126689910889] 51 109\n",
      "[0.7472262382507324, 1.0, 1.6785625219345093] 51 73\n",
      "[0.16445167362689972, 1.0, 1.1207400560379028] 51 116\n",
      "[0.09382234513759613, 1.0, 1.067193865776062] 51 124\n",
      "[0.0936780571937561, 1.0, 1.067087173461914] 51 83\n",
      "[0.006173614412546158, 1.0, 1.0042884349822998] 51 76\n",
      "[0.1869739592075348, 1.0, 1.1383734941482544] 51 87\n",
      "[0.009259643033146858, 1.0, 1.006438970565796] 51 24\n",
      "[0.01198060903698206, 1.0, 1.0083389282226562] 51 93\n",
      "[0.26320362091064453, 1.0, 1.2001407146453857] 51 46\n",
      "[1.0357145071029663, 0.0, 2.050128698348999] 51 36\n",
      "[0.02250211499631405, 1.0, 1.0157195329666138] 51 61\n",
      "[0.01685013808310032, 1.0, 1.0117480754852295] 51 32\n",
      "[0.023264175280928612, 1.0, 1.0162562131881714] 51 45\n",
      "[0.007384017575532198, 1.0, 1.0051313638687134] 51 275\n",
      "[0.006959339138120413, 1.0, 1.0048354864120483] 51 27\n",
      "[0.0001254756498383358, 1.0, 1.000087022781372] 51 51\n",
      "[8.380764484172687e-05, 1.0, 1.0000580549240112] 51 59\n",
      "[0.009023485705256462, 1.0, 1.0062742233276367] 51 117\n",
      "[0.27751994132995605, 1.0, 1.2121094465255737] 51 62\n",
      "[3.433286474319175e-05, 1.0, 1.0000238418579102] 51 73\n",
      "[4.4883305235998705e-05, 1.0, 1.0000311136245728] 51 200\n",
      "[0.0012594640720635653, 1.0, 1.000873327255249] 51 37\n",
      "[5.70432712265756e-05, 1.0, 1.0000395774841309] 51 102\n",
      "[1.3994144201278687, 0.0, 2.6379449367523193] 51 101\n",
      "[0.00022372124658431858, 1.0, 1.0001550912857056] 51 80\n",
      "[0.0007043576915748417, 1.0, 1.0004884004592896] 51 260\n",
      "[0.12357550859451294, 1.0, 1.0894315242767334] 51 72\n",
      "[0.06954274326562881, 1.0, 1.0493839979171753] 51 124\n",
      "[2.3842141672503203e-05, 1.0, 1.0000165700912476] 51 312\n",
      "[0.22386173903942108, 1.0, 1.1678555011749268] 51 227\n",
      "[3.826691317954101e-05, 1.0, 1.0000265836715698] 51 48\n",
      "[0.0007616797811351717, 1.0, 1.00052809715271] 51 140\n",
      "[0.08673225343227386, 1.0, 1.0619621276855469] 51 87\n",
      "[0.0059808772057294846, 1.0, 1.0041542053222656] 51 99\n",
      "[0.004740189760923386, 1.0, 1.0032910108566284] 51 90\n",
      "[0.015588091686367989, 1.0, 1.0108634233474731] 51 117\n",
      "[0.1452728807926178, 1.0, 1.1059398651123047] 51 166\n",
      "[0.1688844859600067, 1.0, 1.1241888999938965] 51 131\n",
      "[0.03430257365107536, 1.0, 1.024061679840088] 51 120\n",
      "[0.07537796348333359, 1.0, 1.0536370277404785] 51 83\n",
      "[0.004390505608171225, 1.0, 1.0030479431152344] 51 114\n",
      "[0.3976484537124634, 1.0, 1.3173588514328003] 51 37\n",
      "[4.1723259869286267e-07, 1.0, 1.000000238418579] 51 118\n",
      "[5.960482212685747e-06, 1.0, 1.0000041723251343] 51 66\n",
      "[0.008149241097271442, 1.0, 1.005664587020874] 51 60\n",
      "[0.12723439931869507, 1.0, 1.0921980142593384] 51 72\n",
      "[0.001847721403464675, 1.0, 1.0012816190719604] 51 90\n",
      "[0.0013701760908588767, 1.0, 1.0009502172470093] 51 54\n",
      "[0.016271773725748062, 1.0, 1.0113425254821777] 51 100\n",
      "[0.0111122140660882, 1.0, 1.0077321529388428] 51 172\n",
      "[0.0007864348590373993, 1.0, 1.0005452632904053] 51 138\n",
      "[1.1920928955078125e-07, 1.0, 1.0000001192092896] 51 82\n",
      "[0.00626525841653347, 1.0, 1.0043522119522095] 51 111\n",
      "[0.024684831500053406, 1.0, 1.0172574520111084] 51 44\n",
      "[0.10540484637022018, 1.0, 1.0757962465286255] 51 77\n",
      "[0.22051358222961426, 1.0, 1.1651482582092285] 51 239\n",
      "[0.09181535243988037, 1.0, 1.0657103061676025] 51 73\n",
      "[0.11203664541244507, 1.0, 1.0807528495788574] 51 218\n",
      "[0.0020020962692797184, 1.0, 1.001388669013977] 51 33\n",
      "[0.002997057046741247, 1.0, 1.0020796060562134] 51 91\n",
      "[0.0911446362733841, 1.0, 1.065214991569519] 51 91\n",
      "[0.035475023090839386, 1.0, 1.0248942375183105] 51 225\n",
      "[5.7102879509329796e-05, 1.0, 1.0000395774841309] 51 28\n",
      "[0.047459740191698074, 1.0, 1.0334436893463135] 51 82\n",
      "[0.0014404295943677425, 1.0, 1.0009989738464355] 51 70\n",
      "[0.020946137607097626, 1.0, 1.0146247148513794] 51 125\n",
      "[0.08096656948328018, 1.0, 1.057726502418518] 51 125\n",
      "[0.010208562016487122, 1.0, 1.0071011781692505] 51 32\n",
      "[7.46278019505553e-05, 1.0, 1.000051736831665] 51 133\n",
      "[0.16800139844417572, 1.0, 1.123500943183899] 51 102\n",
      "[0.0007709255442023277, 1.0, 1.0005345344543457] 51 83\n",
      "[3.325994475744665e-05, 1.0, 1.0000230073928833] 51 187\n",
      "[0.0025665287394076586, 1.0, 1.0017805099487305] 51 100\n",
      "[0.003774904413148761, 1.0, 1.002619981765747] 51 154\n",
      "[0.000637197750620544, 1.0, 1.0004417896270752] 51 95\n",
      "[0.02338388003408909, 1.0, 1.0163404941558838] 51 111\n",
      "[0.02960670366883278, 1.0, 1.0207338333129883] 51 76\n",
      "[0.12438814342021942, 1.0, 1.0900453329086304] 51 40\n",
      "[0.0009754287311807275, 1.0, 1.0006763935089111] 51 41\n",
      "[0.013351664878427982, 1.0, 1.0092976093292236] 51 117\n",
      "[0.9852694272994995, 0.0, 1.9796830415725708] 51 66\n",
      "[0.0019227857701480389, 1.0, 1.0013337135314941] 51 172\n",
      "[0.023541614413261414, 1.0, 1.0164517164230347] 51 71\n",
      "[0.21455787122249603, 1.0, 1.1603482961654663] 51 109\n",
      "[0.2584592401981354, 1.0, 1.1962004899978638] 51 29\n",
      "[0.11478453129529953, 1.0, 1.0828132629394531] 51 71\n",
      "[0.001265432103537023, 1.0, 1.0008774995803833] 51 59\n",
      "[7.152559646783629e-07, 1.0, 1.0000004768371582] 51 110\n",
      "[2.8489270210266113, 0.0, 7.204643249511719] 51 157\n",
      "ADP NOUN AUX PRON ADJ NOUN PUNCT SCONJ PRON ADP DET NOUN ADV ADV ADP NOUN VERB PUNCT CONJ PRON VERB PRON ADP ADV ADJ PUNCT PRON ADP PRON NOUN PART VERB PUNCT\n",
      "[0.001031334395520389, 1.0, 1.0007151365280151] 51 48\n",
      "[0.0341418981552124, 1.0, 1.0239475965499878] 51 41\n",
      "[0.0546828955411911, 1.0, 1.0386308431625366] 51 74\n",
      "[1.4305124977909145e-06, 1.0, 1.0000009536743164] 51 42\n",
      "[0.002152851317077875, 1.0, 1.0014933347702026] 51 63\n",
      "[8.118482219288126e-05, 1.0, 1.000056266784668] 51 31\n",
      "[0.09443274885416031, 1.0, 1.06764554977417] 51 59\n",
      "[0.0029724864289164543, 1.0, 1.002062439918518] 51 158\n",
      "[0.05413420498371124, 1.0, 1.0382359027862549] 51 40\n",
      "[0.02466326393187046, 1.0, 1.017242193222046] 51 104\n",
      "[0.009955746121704578, 1.0, 1.0069246292114258] 51 116\n",
      "[0.014463205821812153, 1.0, 1.010075569152832] 51 26\n",
      "[7.641607226105407e-05, 1.0, 1.0000529289245605] 51 89\n",
      "[0.04323083534836769, 1.0, 1.0304187536239624] 51 187\n",
      "[0.010203082114458084, 1.0, 1.0070973634719849] 51 37\n",
      "[0.004108392633497715, 1.0, 1.0028517246246338] 51 356\n",
      "[0.0025592383462935686, 1.0, 1.0017755031585693] 51 106\n",
      "[0.00594711909070611, 1.0, 1.0041307210922241] 51 116\n",
      "[0.0024005952291190624, 1.0, 1.0016653537750244] 51 87\n",
      "[0.0038467030972242355, 1.0, 1.0026699304580688] 51 96\n",
      "[0.005752327851951122, 1.0, 1.0039951801300049] 51 269\n",
      "[0.0006521085742861032, 1.0, 1.0004521608352661] 51 207\n",
      "[0.2350352704524994, 1.0, 1.1769355535507202] 51 41\n",
      "[0.026608828455209732, 1.0, 1.0186150074005127] 51 164\n",
      "[0.0019424337660893798, 1.0, 1.001347303390503] 51 156\n",
      "[1.919287933560554e-05, 1.0, 1.0000133514404297] 51 32\n",
      "[0.0030317322816699743, 1.0, 1.0021036863327026] 51 171\n",
      "[1.9073504518019035e-06, 1.0, 1.000001311302185] 51 23\n",
      "[0.0029729646630585194, 1.0, 1.0020627975463867] 51 94\n",
      "[0.009537375532090664, 1.0, 1.006632685661316] 51 70\n",
      "[0.055721305310726166, 1.0, 1.0393786430358887] 51 92\n",
      "[0.0022241154219955206, 1.0, 1.0015428066253662] 51 90\n",
      "[0.0313347652554512, 1.0, 1.0219571590423584] 51 63\n",
      "[1.37506103515625, 0.0, 2.5937888622283936] 51 146\n",
      "[0.032145753502845764, 1.0, 1.0225318670272827] 51 46\n",
      "[0.08454481512308121, 1.0, 1.06035315990448] 51 51\n",
      "[0.002070243936032057, 1.0, 1.0014359951019287] 51 145\n",
      "[0.195428267121315, 1.0, 1.1450639963150024] 51 113\n",
      "[0.00010109458526130766, 1.0, 1.0000700950622559] 51 52\n",
      "[1.7762342395144515e-05, 1.0, 1.0000122785568237] 51 44\n",
      "[0.00010061770444735885, 1.0, 1.0000697374343872] 51 37\n",
      "[0.5576610565185547, 1.0, 1.4718810319900513] 51 48\n",
      "[0.0034155696630477905, 1.0, 1.0023702383041382] 51 82\n",
      "[8.583437011111528e-05, 1.0, 1.0000594854354858] 51 46\n",
      "[0.3021048307418823, 1.0, 1.2329418659210205] 51 176\n",
      "[0.002165753860026598, 1.0, 1.001502275466919] 51 124\n",
      "[0.032086972147226334, 1.0, 1.02249014377594] 51 125\n",
      "[0.0029043969698250294, 1.0, 1.002015233039856] 51 61\n",
      "[0.0025425064377486706, 1.0, 1.001763939857483] 51 176\n",
      "[1.1920928955078125e-07, 1.0, 1.0000001192092896] 51 99\n",
      "[0.0062370686791837215, 1.0, 1.0043325424194336] 51 49\n",
      "[0.027213478460907936, 1.0, 1.0190420150756836] 51 130\n",
      "[0.0015694887842983007, 1.0, 1.0010885000228882] 51 105\n",
      "[0.012162499129772186, 1.0, 1.0084660053253174] 51 195\n",
      "[1.405051827430725, 0.0, 2.64827299118042] 51 247\n",
      "[0.0001567128929309547, 1.0, 1.0001085996627808] 51 76\n",
      "[0.025158485397696495, 1.0, 1.0175914764404297] 51 238\n",
      "[0.037204913794994354, 1.0, 1.0261238813400269] 51 5\n",
      "[0.0784154012799263, 1.0, 1.0558576583862305] 51 205\n",
      "[0.06077377125620842, 1.0, 1.043025016784668] 51 149\n",
      "[0.03965295851230621, 1.0, 1.0278666019439697] 51 208\n",
      "[0.07630087435245514, 1.0, 1.0543112754821777] 51 203\n",
      "[0.020527401939034462, 1.0, 1.014330267906189] 51 9\n",
      "[0.0020629570353776217, 1.0, 1.0014309883117676] 51 5\n",
      "[0.013828018680214882, 1.0, 1.0096309185028076] 51 30\n",
      "[0.010764030739665031, 1.0, 1.0074889659881592] 51 103\n",
      "[0.00024500509607605636, 1.0, 1.0001698732376099] 51 45\n",
      "[0.0021330201998353004, 1.0, 1.0014796257019043] 51 9\n",
      "[0.049561627209186554, 1.0, 1.0349503755569458] 51 10\n",
      "[0.10294642299413681, 1.0, 1.0739645957946777] 51 43\n",
      "[0.006534182000905275, 1.0, 1.0045393705368042] 51 95\n",
      "[0.004869615659117699, 1.0, 1.0033810138702393] 51 27\n",
      "[0.2031010240316391, 1.0, 1.1511701345443726] 51 36\n",
      "[0.0009325917926616967, 1.0, 1.0006465911865234] 51 62\n",
      "[0.04954810068011284, 1.0, 1.0349407196044922] 51 32\n",
      "[0.00024810529430396855, 1.0, 1.0001720190048218] 51 38\n",
      "[1.023534893989563, 0.0, 2.0328938961029053] 51 9\n",
      "[0.02737806737422943, 1.0, 1.0191582441329956] 51 86\n",
      "[0.00012923122267238796, 1.0, 1.0000895261764526] 51 19\n",
      "[0.02012696862220764, 1.0, 1.01404869556427] 51 23\n",
      "[0.0009012705995701253, 1.0, 1.0006248950958252] 51 27\n",
      "[0.10839936137199402, 1.0, 1.0780315399169922] 51 27\n",
      "[0.01370298769325018, 1.0, 1.0095434188842773] 51 9\n",
      "[0.01765303872525692, 1.0, 1.012311339378357] 51 12\n",
      "[0.010668840259313583, 1.0, 1.0074224472045898] 51 119\n",
      "[0.1440010964870453, 1.0, 1.104965329170227] 51 94\n",
      "[0.003495058510452509, 1.0, 1.0024255514144897] 51 73\n",
      "[0.01149880513548851, 1.0, 1.0080021619796753] 51 9\n",
      "[0.016464928165078163, 1.0, 1.0114779472351074] 51 22\n",
      "[0.0051155174151062965, 1.0, 1.0035520792007446] 51 32\n",
      "[0.000477129768114537, 1.0, 1.0003308057785034] 51 9\n",
      "[0.018132111057639122, 1.0, 1.0126475095748901] 51 5\n",
      "[0.004549704492092133, 1.0, 1.0031585693359375] 51 41\n",
      "[0.006753008812665939, 1.0, 1.0046918392181396] 51 9\n",
      "[8.118600845336914, 0.0, 277.9344482421875] 51 5\n",
      "PROPN\n",
      "[0.30827587842941284, 1.0, 1.2382270097732544] 51 14\n",
      "[0.00011122845171485096, 1.0, 1.0000771284103394] 51 9\n",
      "[0.010336228646337986, 1.0, 1.007190227508545] 51 24\n",
      "[0.07783885300159454, 1.0, 1.0554357767105103] 51 32\n",
      "[0.0019359840080142021, 1.0, 1.0013427734375] 51 52\n",
      "[0.02931131049990654, 1.0, 1.0205248594284058] 51 41\n",
      "[0.00041797550511546433, 1.0, 1.000289797782898] 51 37\n",
      "[0.22114890813827515, 1.0, 1.1656614542007446] 51 299\n",
      "[0.0071116238832473755, 1.0, 1.0049415826797485] 51 90\n",
      "[0.03820909932255745, 1.0, 1.0268383026123047] 51 63\n",
      "[0.016300611197948456, 1.0, 1.0113627910614014] 51 53\n",
      "[0.003137319115921855, 1.0, 1.0021770000457764] 51 65\n",
      "[0.2205684334039688, 1.0, 1.1651926040649414] 51 106\n",
      "[0.04004460200667381, 1.0, 1.028145670890808] 51 159\n",
      "[4.455351829528809, 0.0, 21.93787384033203] 51 70\n",
      "PROPN AUX DET PROPN PROPN ADV NUM NOUN PUNCT PROPN ADP NOUN VERB PUNCT\n",
      "[0.0029782853089272976, 1.0, 1.0020664930343628] 51 102\n",
      "[0.0008564682211726904, 1.0, 1.0005937814712524] 51 65\n",
      "[0.000977934687398374, 1.0, 1.0006780624389648] 51 27\n",
      "[3.576279254957626e-07, 1.0, 1.000000238418579] 51 56\n",
      "[0.15064221620559692, 1.0, 1.1100635528564453] 51 46\n",
      "[0.005408881697803736, 1.0, 1.0037561655044556] 51 45\n",
      "[0.013219204731285572, 1.0, 1.0092049837112427] 51 195\n",
      "[0.04654520004987717, 1.0, 1.0327887535095215] 51 40\n",
      "[0.15647269785404205, 1.0, 1.1145588159561157] 51 48\n",
      "[0.0006855691899545491, 1.0, 1.000475287437439] 51 287\n",
      "[0.003380163572728634, 1.0, 1.0023456811904907] 51 115\n",
      "[0.006241027265787125, 1.0, 1.0043352842330933] 51 282\n",
      "[0.03385256230831146, 1.0, 1.0237423181533813] 51 93\n",
      "[1.9694080352783203, 0.0, 3.91607403755188] 51 47\n",
      "ADJ NOUN PUNCT PRON AUX DET NOUN ADP NOUN PUNCT\n",
      "[0.0030740611255168915, 1.0, 1.0021330118179321] 51 356\n",
      "[6.050054435036145e-05, 1.0, 1.0000419616699219] 51 37\n",
      "[0.03350456804037094, 1.0, 1.0234953165054321] 51 186\n",
      "[9.382211283082142e-05, 1.0, 1.0000650882720947] 51 43\n",
      "[0.001439593848772347, 1.0, 1.0009983777999878] 51 180\n",
      "[0.18049712479114532, 1.0, 1.1332743167877197] 51 89\n",
      "[0.00029097258811816573, 1.0, 1.00020170211792] 51 229\n",
      "[5.954681182629429e-05, 1.0, 1.0000412464141846] 51 96\n",
      "[0.0472475066781044, 1.0, 1.0332916975021362] 51 32\n",
      "[0.23670929670333862, 1.0, 1.1783019304275513] 51 112\n",
      "[0.04934531822800636, 1.0, 1.0347952842712402] 51 75\n",
      "[0.00011170533980475739, 1.0, 1.000077486038208] 51 26\n",
      "[0.18276305496692657, 1.0, 1.135055661201477] 51 56\n",
      "[0.0833035260438919, 1.0, 1.0594412088394165] 51 73\n",
      "[0.004749412648379803, 1.0, 1.0032974481582642] 51 86\n",
      "[3.218656047465629e-06, 1.0, 1.0000022649765015] 51 252\n",
      "[0.01136990450322628, 1.0, 1.0079121589660645] 51 75\n",
      "[0.07976152747869492, 1.0, 1.0568432807922363] 51 122\n",
      "[0.0612550750374794, 1.0, 1.0433731079101562] 51 46\n",
      "[0.00994593370705843, 1.0, 1.0069178342819214] 51 72\n",
      "[4.64917320641689e-06, 1.0, 1.0000032186508179] 51 159\n",
      "[0.024197477847337723, 1.0, 1.016913890838623] 51 172\n",
      "[0.0013105511898174882, 1.0, 1.0009088516235352] 51 103\n",
      "[0.11401699483394623, 1.0, 1.0822373628616333] 51 102\n",
      "[1.2457448065106291e-05, 1.0, 1.0000085830688477] 51 210\n",
      "[0.005836438853293657, 1.0, 1.0040537118911743] 51 88\n",
      "[0.24527883529663086, 1.0, 1.1853218078613281] 51 105\n",
      "[0.11287571489810944, 1.0, 1.0813815593719482] 51 79\n",
      "[0.06559543311595917, 1.0, 1.0465167760849] 51 97\n",
      "[0.0005491691408678889, 1.0, 1.0003807544708252] 51 142\n",
      "[0.0039075566455721855, 1.0, 1.0027121305465698] 51 91\n",
      "[0.05072145164012909, 1.0, 1.035782814025879] 51 32\n",
      "[0.0058787669986486435, 1.0, 1.0040831565856934] 51 31\n",
      "[0.0005458890809677541, 1.0, 1.0003784894943237] 51 37\n",
      "[0.016895420849323273, 1.0, 1.0117799043655396] 51 149\n",
      "[0.05713098868727684, 1.0, 1.0403947830200195] 51 102\n",
      "[0.038167670369148254, 1.0, 1.0268088579177856] 51 127\n",
      "[0.005764917004853487, 1.0, 1.004003882408142] 51 67\n",
      "[0.0011275805300101638, 1.0, 1.0007818937301636] 51 77\n",
      "[0.9315602779388428, 0.0, 1.9073376655578613] 51 37\n",
      "[0.14928294718265533, 1.0, 1.109018087387085] 51 194\n",
      "[0.05968138575553894, 1.0, 1.0422356128692627] 51 90\n",
      "[0.008429971523582935, 1.0, 1.0058603286743164] 51 151\n",
      "[0.33610111474990845, 1.0, 1.2623405456542969] 51 110\n",
      "[0.14761076867580414, 1.0, 1.1077334880828857] 51 240\n",
      "[0.011639298871159554, 1.0, 1.0081003904342651] 51 46\n",
      "[0.009923839941620827, 1.0, 1.0069023370742798] 51 100\n",
      "[0.023975348100066185, 1.0, 1.0167572498321533] 51 81\n",
      "[0.04156157374382019, 1.0, 1.0292272567749023] 51 39\n",
      "[3.576279254957626e-07, 1.0, 1.000000238418579] 51 60\n",
      "[0.0003150601696688682, 1.0, 1.000218391418457] 51 151\n",
      "[0.004132931586354971, 1.0, 1.002868890762329] 51 94\n",
      "[0.07332352548837662, 1.0, 1.0521377325057983] 51 95\n",
      "[0.0014322518836706877, 1.0, 1.000993251800537] 51 99\n",
      "[0.07682356983423233, 1.0, 1.054693341255188] 51 103\n",
      "[0.6322752833366394, 1.0, 1.5500075817108154] 51 172\n",
      "[0.008131393231451511, 1.0, 1.0056521892547607] 51 133\n",
      "[0.023564988747239113, 1.0, 1.0164681673049927] 51 112\n",
      "[0.0019235024228692055, 1.0, 1.0013341903686523] 51 121\n",
      "[3.646578550338745, 0.0, 12.52361011505127] 51 67\n",
      "DET NOUN DET NOUN VERB DET ADJ ADJ NOUN ADP PRON ADV ADJ NOUN PUNCT\n",
      "[0.002524998039007187, 1.0, 1.0017517805099487] 51 404\n",
      "[0.26874539256095886, 1.0, 1.2047597169876099] 51 171\n",
      "[0.024052947759628296, 1.0, 1.0168119668960571] 51 72\n",
      "[2.3841860752327193e-07, 1.0, 1.0000001192092896] 51 45\n",
      "[0.6582398414611816, 1.0, 1.5781559944152832] 51 70\n",
      "[0.0032975743524730206, 1.0, 1.0022883415222168] 51 85\n",
      "[7.74863383412594e-06, 1.0, 1.0000053644180298] 51 45\n",
      "[0.00748910428956151, 1.0, 1.0052045583724976] 51 110\n",
      "[0.21247701346874237, 1.0, 1.1586757898330688] 51 105\n",
      "[0.08582356572151184, 1.0, 1.061293363571167] 51 32\n",
      "[0.016820255666971207, 1.0, 1.0117270946502686] 51 150\n",
      "[0.02031186781823635, 1.0, 1.0141786336898804] 51 43\n",
      "[0.006658379919826984, 1.0, 1.004625916481018] 51 38\n",
      "[0.02132382243871689, 1.0, 1.0148903131484985] 51 52\n",
      "[0.06502290815114975, 1.0, 1.0461015701293945] 51 92\n",
      "[0.10655646026134491, 1.0, 1.0766552686691284] 51 177\n",
      "[0.00041535182390362024, 1.0, 1.0002878904342651] 51 170\n",
      "[0.019728517159819603, 1.0, 1.0137686729431152] 51 267\n",
      "[0.00026533546042628586, 1.0, 1.0001839399337769] 51 19\n",
      "[0.07429385185241699, 1.0, 1.0528455972671509] 51 179\n",
      "[0.08832190930843353, 1.0, 1.063132882118225] 51 255\n",
      "[0.00035387580282986164, 1.0, 1.0002453327178955] 51 140\n",
      "[0.307807058095932, 1.0, 1.2378246784210205] 51 54\n",
      "[0.012623376213014126, 1.0, 1.0087882280349731] 51 100\n",
      "[0.020012274384498596, 1.0, 1.0139681100845337] 51 185\n",
      "[0.05106683447957039, 1.0, 1.0360307693481445] 51 134\n",
      "[0.006892358418554068, 1.0, 1.004788875579834] 51 98\n",
      "[0.001232131035067141, 1.0, 1.0008543729782104] 51 150\n",
      "[0.016130808740854263, 1.0, 1.0112438201904297] 51 187\n",
      "[0.007544293999671936, 1.0, 1.005242943763733] 51 157\n",
      "[3.421365181566216e-05, 1.0, 1.0000237226486206] 51 125\n",
      "[0.03701816499233246, 1.0, 1.0259910821914673] 51 114\n",
      "[2.9643893241882324, 0.0, 7.804949760437012] 51 144\n",
      "PRON VERB ADP DET NOUN PUNCT PRON ADJ ADP DET NOUN ADJ VERB PUNCT CONJ ADV AUX PRON ADP DET NOUN ADP NOUN PROPN ADP DET NOUN ADP NOUN VERB PUNCT\n",
      "[0.006559019908308983, 1.0, 1.0045567750930786] 51 170\n",
      "[0.04684441536664963, 1.0, 1.0330029726028442] 51 166\n",
      "[0.0031389931682497263, 1.0, 1.0021781921386719] 51 142\n",
      "[0.009383277967572212, 1.0, 1.0065251588821411] 51 84\n",
      "[0.010072060860693455, 1.0, 1.0070058107376099] 51 97\n",
      "[0.006387743633240461, 1.0, 1.0044374465942383] 51 68\n",
      "[0.0027870596386492252, 1.0, 1.0019336938858032] 51 175\n",
      "[0.024575477465987206, 1.0, 1.017180323600769] 51 165\n",
      "[0.00011563969746930525, 1.0, 1.0000801086425781] 51 106\n",
      "[7.307796477107331e-05, 1.0, 1.000050663948059] 51 199\n",
      "[0.23183979094028473, 1.0, 1.174331545829773] 51 141\n",
      "[0.0010371219832450151, 1.0, 1.0007191896438599] 51 147\n",
      "[0.036858536303043365, 1.0, 1.025877594947815] 51 81\n",
      "[0.006320260930806398, 1.0, 1.0043904781341553] 51 178\n",
      "[0.006177992559969425, 1.0, 1.0042914152145386] 51 135\n",
      "[1.150376283476362e-05, 1.0, 1.0000079870224] 51 114\n",
      "[0.33005061745643616, 1.0, 1.2570574283599854] 51 239\n",
      "[0.00824112631380558, 1.0, 1.0057286024093628] 51 224\n",
      "[0.015245790593326092, 1.0, 1.010623574256897] 51 117\n",
      "[0.007614261470735073, 1.0, 1.0052917003631592] 51 98\n",
      "[0.04904234781861305, 1.0, 1.0345779657363892] 51 54\n",
      "[0.8274957537651062, 0.0, 1.7746022939682007] 51 115\n",
      "[0.11438828706741333, 1.0, 1.0825159549713135] 51 103\n",
      "[0.08036044239997864, 1.0, 1.0572822093963623] 51 238\n",
      "[0.008984393440186977, 1.0, 1.0062469244003296] 51 118\n",
      "[0.00838723499327898, 1.0, 1.0058305263519287] 51 335\n",
      "[0.016895906999707222, 1.0, 1.0117801427841187] 51 105\n",
      "[0.0002693896531127393, 1.0, 1.000186800956726] 51 281\n",
      "[0.0466802753508091, 1.0, 1.0328854322433472] 51 81\n",
      "[0.08018206059932709, 1.0, 1.057151436805725] 51 103\n",
      "[0.11047995835542679, 1.0, 1.0795873403549194] 51 93\n",
      "[2.6882056772592478e-05, 1.0, 1.00001859664917] 51 122\n",
      "[0.015326286666095257, 1.0, 1.0106799602508545] 51 80\n",
      "[0.13185691833496094, 1.0, 1.095703125] 51 111\n",
      "[0.17116200923919678, 1.0, 1.1259649991989136] 51 58\n",
      "[0.03433656319975853, 1.0, 1.0240857601165771] 51 75\n",
      "[0.11660422384738922, 1.0, 1.0841799974441528] 51 125\n",
      "[0.6017892956733704, 1.0, 1.5175975561141968] 51 84\n",
      "[0.017391841858625412, 1.0, 1.0121281147003174] 51 95\n",
      "[1.1920928955078125e-07, 1.0, 1.0000001192092896] 51 95\n",
      "[0.01235866453498602, 1.0, 1.0086032152175903] 51 66\n",
      "[4.291543518775143e-06, 1.0, 1.0000029802322388] 51 68\n",
      "[0.015084396116435528, 1.0, 1.0105105638504028] 51 93\n",
      "[0.18654298782348633, 1.0, 1.1380335092544556] 51 174\n",
      "[0.04225529357790947, 1.0, 1.0297223329544067] 51 95\n",
      "[0.011341328732669353, 1.0, 1.00789213180542] 51 121\n",
      "[0.056130263954401016, 1.0, 1.0396733283996582] 51 47\n",
      "[0.004920109175145626, 1.0, 1.0034161806106567] 51 88\n",
      "[0.029675530269742012, 1.0, 1.020782470703125] 51 19\n",
      "[0.0007876874879002571, 1.0, 1.0005460977554321] 51 140\n",
      "[0.0048539238050580025, 1.0, 1.0033701658248901] 51 88\n",
      "[0.14533306658267975, 1.0, 1.1059859991073608] 51 61\n",
      "[0.01192963682115078, 1.0, 1.0083032846450806] 51 150\n",
      "[0.6327874064445496, 1.0, 1.5505578517913818] 51 70\n",
      "[4.410753263073275e-06, 1.0, 1.0000030994415283] 51 151\n",
      "[0.00017781645874492824, 1.0, 1.0001232624053955] 51 152\n",
      "[0.00042656221194192767, 1.0, 1.0002957582473755] 51 180\n",
      "[0.4351264536380768, 1.0, 1.3520293235778809] 51 121\n",
      "[0.08294819295406342, 1.0, 1.0591802597045898] 51 45\n",
      "[1.176745891571045, 0.0, 2.2606630325317383] 51 72\n",
      "[0.022732391953468323, 1.0, 1.0158816576004028] 51 33\n",
      "[0.34404462575912476, 1.0, 1.2693101167678833] 51 92\n",
      "[0.0002891839249059558, 1.0, 1.0002005100250244] 51 165\n",
      "[0.13079991936683655, 1.0, 1.0949006080627441] 51 71\n",
      "[0.3213634490966797, 1.0, 1.2495108842849731] 51 24\n",
      "[0.0012513477122411132, 1.0, 1.0008677244186401] 51 81\n",
      "[0.0011339654447510839, 1.0, 1.000786304473877] 51 119\n",
      "[0.2806658446788788, 1.0, 1.2147554159164429] 51 279\n",
      "[0.04429604113101959, 1.0, 1.0311799049377441] 51 164\n",
      "[3.445207767072134e-05, 1.0, 1.0000238418579102] 51 69\n",
      "[0.006949255708605051, 1.0, 1.0048284530639648] 51 224\n",
      "[0.039870597422122955, 1.0, 1.0280215740203857] 51 129\n",
      "[0.00821864977478981, 1.0, 1.0057129859924316] 51 158\n",
      "[0.0214373879134655, 1.0, 1.0149701833724976] 51 69\n",
      "[0.0003295487549621612, 1.0, 1.0002284049987793] 51 52\n",
      "[0.1406870037317276, 1.0, 1.102429986000061] 51 162\n",
      "[0.08276823908090591, 1.0, 1.0590481758117676] 51 46\n",
      "[6.073897748137824e-05, 1.0, 1.0000420808792114] 51 96\n",
      "[0.043647293001413345, 1.0, 1.0307163000106812] 51 154\n",
      "[0.0011060988763347268, 1.0, 1.0007669925689697] 51 106\n",
      "[0.0005445770802907646, 1.0, 1.0003775358200073] 51 103\n",
      "[0.013410140760242939, 1.0, 1.0093384981155396] 51 283\n",
      "[0.1210092231631279, 1.0, 1.0874953269958496] 51 85\n",
      "[0.009200990200042725, 1.0, 1.0063979625701904] 51 133\n",
      "[2.2113566956249997e-05, 1.0, 1.000015377998352] 51 167\n",
      "[0.05970466881990433, 1.0, 1.0422524213790894] 51 253\n",
      "[0.057300642132759094, 1.0, 1.0405170917510986] 51 228\n",
      "[0.006392482668161392, 1.0, 1.0044407844543457] 51 191\n",
      "[0.3486683666706085, 1.0, 1.273384690284729] 51 189\n",
      "[0.06326795369386673, 1.0, 1.0448298454284668] 51 72\n",
      "[0.004737015813589096, 1.0, 1.0032888650894165] 51 215\n",
      "[0.0020272405818104744, 1.0, 1.001406192779541] 51 168\n",
      "[0.152889683842659, 1.0, 1.111794114112854] 51 69\n",
      "[0.05223625898361206, 1.0, 1.0368708372116089] 51 85\n",
      "[0.030976951122283936, 1.0, 1.0217037200927734] 51 72\n",
      "[0.1588646024465561, 1.0, 1.1164082288742065] 51 46\n",
      "[0.00018324147094972432, 1.0, 1.0001270771026611] 51 112\n",
      "[0.0005628858343698084, 1.0, 1.0003902912139893] 51 119\n",
      "[0.0002456609217915684, 1.0, 1.000170350074768] 51 144\n",
      "[0.5767478346824646, 1.0, 1.4914833307266235] 51 66\n",
      "[0.009912642650306225, 1.0, 1.006894588470459] 51 100\n",
      "[0.017562346532940865, 1.0, 1.0122476816177368] 51 140\n",
      "[0.01866072602570057, 1.0, 1.0130186080932617] 51 140\n",
      "[0.0021114570554345846, 1.0, 1.001464605331421] 51 66\n",
      "[6.664021202595904e-05, 1.0, 1.0000461339950562] 51 76\n",
      "[0.00287289428524673, 1.0, 1.0019932985305786] 51 85\n",
      "[0.00036097128759138286, 1.0, 1.000250220298767] 51 89\n",
      "[0.0616278238594532, 1.0, 1.0436426401138306] 51 262\n",
      "[0.03941044583916664, 1.0, 1.027693748474121] 51 248\n",
      "[8.523827273165807e-05, 1.0, 1.0000591278076172] 51 56\n",
      "[0.1099962592124939, 1.0, 1.0792254209518433] 51 169\n",
      "[0.19908346235752106, 1.0, 1.1479687690734863] 51 178\n",
      "[2.467662670824211e-05, 1.0, 1.0000170469284058] 51 41\n",
      "[0.012333439663052559, 1.0, 1.0085855722427368] 51 106\n",
      "[0.012401389889419079, 1.0, 1.008633017539978] 51 223\n",
      "[0.05523150786757469, 1.0, 1.0390257835388184] 51 38\n",
      "[0.013751269318163395, 1.0, 1.0095772743225098] 51 73\n",
      "[1.1920928955078125e-07, 1.0, 1.0000001192092896] 51 190\n",
      "[0.014197764918208122, 1.0, 1.0098897218704224] 51 95\n",
      "[0.0033522346056997776, 1.0, 1.002326250076294] 51 181\n",
      "[0.1875869631767273, 1.0, 1.1388572454452515] 51 77\n",
      "[0.024576393887400627, 1.0, 1.0171810388565063] 51 37\n",
      "[0.1287974715232849, 1.0, 1.0933820009231567] 51 64\n",
      "[0.003643645904958248, 1.0, 1.0025287866592407] 51 153\n",
      "[0.0003036721609532833, 1.0, 1.0002105236053467] 51 122\n",
      "[1.0728841743912199e-06, 1.0, 1.0000007152557373] 51 73\n",
      "[0.00592199619859457, 1.0, 1.0041131973266602] 51 64\n",
      "[0.013150655664503574, 1.0, 1.0091570615768433] 51 36\n",
      "[0.0011101564159616828, 1.0, 1.000769853591919] 51 57\n",
      "[0.06601016968488693, 1.0, 1.046817660331726] 51 90\n",
      "[0.00014222679601516575, 1.0, 1.0000985860824585] 51 65\n",
      "[0.02765665203332901, 1.0, 1.019355058670044] 51 44\n",
      "[0.0011270433897152543, 1.0, 1.000781536102295] 51 98\n",
      "[0.0875883400440216, 1.0, 1.0625923871994019] 51 103\n",
      "[0.010365979745984077, 1.0, 1.0072109699249268] 51 69\n",
      "[0.05382317677140236, 1.0, 1.0380120277404785] 51 29\n",
      "[0.1692342311143875, 1.0, 1.1244615316390991] 51 257\n",
      "[0.0008469232707284391, 1.0, 1.0005872249603271] 51 45\n",
      "[1.1920935776288388e-06, 1.0, 1.0000008344650269] 51 73\n",
      "[0.0027192814741283655, 1.0, 1.0018866062164307] 51 124\n",
      "[0.0008933957433328032, 1.0, 1.0006194114685059] 51 151\n",
      "[0.0550193190574646, 1.0, 1.0388729572296143] 51 105\n",
      "[0.0006838990957476199, 1.0, 1.000474214553833] 51 230\n",
      "[0.04006706178188324, 1.0, 1.028161644935608] 51 84\n",
      "[0.07810936123132706, 1.0, 1.055633783340454] 51 64\n",
      "[0.00033032387727871537, 1.0, 1.000229001045227] 51 134\n",
      "[0.007781303022056818, 1.0, 1.0054081678390503] 51 40\n",
      "[0.0006966632790863514, 1.0, 1.0004830360412598] 51 37\n",
      "[0.002848087577149272, 1.0, 1.0019761323928833] 51 85\n",
      "[0.029850414022803307, 1.0, 1.0209062099456787] 51 92\n",
      "[1.5497220147153712e-06, 1.0, 1.000001072883606] 51 145\n",
      "[0.12736207246780396, 1.0, 1.092294692993164] 51 46\n",
      "[3.230623769923113e-05, 1.0, 1.0000224113464355] 51 102\n",
      "[0.0001317349378950894, 1.0, 1.000091314315796] 51 68\n",
      "[0.0024116486310958862, 1.0, 1.0016729831695557] 51 266\n",
      "[0.05663614720106125, 1.0, 1.040037989616394] 51 169\n",
      "[2.5033982637978625e-06, 1.0, 1.0000017881393433] 51 286\n",
      "[0.008722145110368729, 1.0, 1.0060640573501587] 51 70\n",
      "[0.03389189764857292, 1.0, 1.0237702131271362] 51 203\n",
      "[0.038154665380716324, 1.0, 1.0267995595932007] 51 112\n",
      "[0.0025802734307944775, 1.0, 1.001790165901184] 51 90\n",
      "[0.07456134259700775, 1.0, 1.053040862083435] 51 89\n",
      "[0.054476235061883926, 1.0, 1.0384820699691772] 51 115\n",
      "[0.0009744741255417466, 1.0, 1.0006756782531738] 51 138\n",
      "[0.3065989315509796, 1.0, 1.2367886304855347] 51 85\n",
      "[0.16384947299957275, 1.0, 1.1202722787857056] 51 38\n",
      "[0.00902986153960228, 1.0, 1.00627863407135] 51 107\n",
      "[0.0005105249001644552, 1.0, 1.0003539323806763] 51 82\n",
      "[0.012245039455592632, 1.0, 1.00852370262146] 51 68\n",
      "[0.1332603096961975, 1.0, 1.0967694520950317] 51 118\n",
      "[0.1040947437286377, 1.0, 1.074819803237915] 51 157\n",
      "[7.689029189350549e-06, 1.0, 1.0000053644180298] 51 127\n",
      "[0.03483239933848381, 1.0, 1.0244377851486206] 51 232\n",
      "[0.039859987795352936, 1.0, 1.028014063835144] 51 143\n",
      "[0.002921971958130598, 1.0, 1.0020273923873901] 51 45\n",
      "[0.016054609790444374, 1.0, 1.011190414428711] 51 44\n",
      "[0.1491641104221344, 1.0, 1.108926773071289] 51 62\n",
      "[0.02846459671854973, 1.0, 1.0199260711669922] 51 79\n",
      "[4.0233942854683846e-05, 1.0, 1.0000278949737549] 51 162\n",
      "[4.470448402571492e-05, 1.0, 1.0000309944152832] 51 19\n",
      "[0.031070712953805923, 1.0, 1.0217701196670532] 51 172\n",
      "[2.3841860752327193e-07, 1.0, 1.0000001192092896] 51 95\n",
      "[0.21577773988246918, 1.0, 1.161329746246338] 51 51\n",
      "[0.0064134784042835236, 1.0, 1.004455327987671] 51 127\n",
      "[0.0043518925085663795, 1.0, 1.003021001815796] 51 145\n",
      "[0.035785216838121414, 1.0, 1.02511465549469] 51 112\n",
      "[0.00010669800394680351, 1.0, 1.0000739097595215] 51 152\n",
      "[0.2859143316745758, 1.0, 1.2191827297210693] 51 146\n",
      "[0.00021728253341279924, 1.0, 1.0001505613327026] 51 132\n",
      "[0.2015535980463028, 1.0, 1.1499360799789429] 51 39\n",
      "[0.002255836268886924, 1.0, 1.001564860343933] 51 150\n",
      "[0.001079903799109161, 1.0, 1.0007487535476685] 51 54\n",
      "[0.11856653541326523, 1.0, 1.0856555700302124] 51 107\n",
      "[0.00971527211368084, 1.0, 1.0067567825317383] 51 171\n",
      "[0.0023925891146063805, 1.0, 1.0016597509384155] 51 99\n",
      "[0.0035283751785755157, 1.0, 1.0024486780166626] 51 83\n",
      "[0.002268620766699314, 1.0, 1.0015736818313599] 51 253\n",
      "[0.06109900772571564, 1.0, 1.0432602167129517] 51 188\n",
      "[0.0866565853357315, 1.0, 1.061906337738037] 51 85\n",
      "[0.0675741583108902, 1.0, 1.0479531288146973] 51 190\n",
      "[0.015610371716320515, 1.0, 1.0108790397644043] 51 269\n",
      "[3.695494797284482e-06, 1.0, 1.0000025033950806] 51 62\n",
      "[0.02998785860836506, 1.0, 1.0210034847259521] 51 143\n",
      "[0.012459930032491684, 1.0, 1.0086740255355835] 51 124\n",
      "[0.0024981084279716015, 1.0, 1.0017330646514893] 51 22\n",
      "[0.027565231546759605, 1.0, 1.0192904472351074] 51 149\n",
      "[0.024180684238672256, 1.0, 1.016901969909668] 51 85\n",
      "[0.08935969322919846, 1.0, 1.0638978481292725] 51 129\n",
      "[3.1352534278994426e-05, 1.0, 1.0000216960906982] 51 154\n",
      "[0.062360286712646484, 1.0, 1.0441726446151733] 51 116\n",
      "[0.014005227945744991, 1.0, 1.00975501537323] 51 122\n",
      "[0.002385000931099057, 1.0, 1.0016545057296753] 51 32\n",
      "[0.03802495077252388, 1.0, 1.0267072916030884] 51 64\n",
      "[0.014603634364902973, 1.0, 1.0101739168167114] 51 171\n",
      "[0.0001318541617365554, 1.0, 1.0000914335250854] 51 113\n",
      "[0.0018557232106104493, 1.0, 1.0012871026992798] 51 101\n",
      "[0.07822015136480331, 1.0, 1.0557148456573486] 51 165\n",
      "[0.006910483352839947, 1.0, 1.0048015117645264] 51 143\n",
      "[0.48405778408050537, 1.0, 1.398672103881836] 51 132\n",
      "[0.0006099414895288646, 1.0, 1.0004228353500366] 51 175\n",
      "[0.006324819754809141, 1.0, 1.0043936967849731] 51 177\n",
      "[0.1234939694404602, 1.0, 1.0893698930740356] 51 105\n",
      "[0.0014928989112377167, 1.0, 1.0010353326797485] 51 283\n",
      "[0.0009186909883283079, 1.0, 1.0006369352340698] 51 197\n",
      "[1.78815535036847e-05, 1.0, 1.0000123977661133] 51 138\n",
      "[0.00031476205913349986, 1.0, 1.000218152999878] 51 130\n",
      "[0.06539510190486908, 1.0, 1.0463714599609375] 51 67\n",
      "[0.025161050260066986, 1.0, 1.017593264579773] 51 103\n",
      "[0.002520815236493945, 1.0, 1.00174880027771] 51 91\n",
      "[0.025116370990872383, 1.0, 1.0175617933273315] 51 151\n",
      "[0.021327415481209755, 1.0, 1.014892816543579] 51 338\n",
      "[0.49905526638031006, 1.0, 1.4132877588272095] 51 38\n",
      "[0.02647380344569683, 1.0, 1.018519639968872] 51 70\n",
      "[0.30916720628738403, 1.0, 1.2389923334121704] 51 57\n",
      "[1.1444157280493528e-05, 1.0, 1.0000079870224] 51 128\n",
      "[0.05325289070606232, 1.0, 1.0376018285751343] 51 105\n",
      "[0.3026372790336609, 1.0, 1.2333970069885254] 51 78\n",
      "[0.051825813949108124, 1.0, 1.0365759134292603] 51 81\n",
      "[0.00016619155940134078, 1.0, 1.000115156173706] 51 98\n",
      "[0.008844451047480106, 1.0, 1.0061492919921875] 51 245\n",
      "[0.0007371638203039765, 1.0, 1.0005110502243042] 51 51\n",
      "[0.04271221160888672, 1.0, 1.0300484895706177] 51 59\n",
      "[0.011558075435459614, 1.0, 1.008043646812439] 51 120\n",
      "[0.0003572148270905018, 1.0, 1.000247597694397] 51 36\n",
      "[0.07730377465486526, 1.0, 1.055044412612915] 51 64\n",
      "[0.00097847159486264, 1.0, 1.0006784200668335] 51 161\n",
      "[0.046761155128479004, 1.0, 1.0329433679580688] 51 194\n",
      "[0.7859187722206116, 1.0, 1.7241899967193604] 51 56\n",
      "[0.0018704134272411466, 1.0, 1.0012973546981812] 51 143\n",
      "[4.545783519744873, 0.0, 23.35700798034668] 51 208\n",
      "PRON AUX ADV DET NOUN PUNCT SCONJ PRON NOUN PROPN PUNCT PROPN VERB DET ADV ADJ NOUN PUNCT ADP PRON PRON ADP DET NOUN ADP PRON NOUN VERB PUNCT PRON AUX DET NOUN PUNCT SCONJ DET NOUN DET ADJ NOUN VERB AUX PUNCT\n",
      "[0.03887515142560005, 1.0, 1.0273125171661377] 51 19\n",
      "[0.053957100957632065, 1.0, 1.0381083488464355] 51 94\n",
      "[0.013159352354705334, 1.0, 1.0091631412506104] 51 196\n",
      "[0.0002727284445427358, 1.0, 1.0001890659332275] 51 40\n",
      "[0.5841434001922607, 1.0, 1.4991486072540283] 51 73\n",
      "[0.006247744895517826, 1.0, 1.0043400526046753] 51 128\n",
      "[0.07046616077423096, 1.0, 1.0500558614730835] 51 96\n",
      "[0.00014037879009265453, 1.0, 1.0000972747802734] 51 292\n",
      "[0.004617367405444384, 1.0, 1.00320565700531] 51 210\n",
      "[0.24735389649868011, 1.0, 1.187027931213379] 51 103\n",
      "[0.860448956489563, 1.0, 1.815603256225586] 51 248\n",
      "[0.0015436398098245263, 1.0, 1.001070499420166] 51 211\n",
      "[0.0008559909765608609, 1.0, 1.0005935430526733] 51 67\n",
      "[0.15210887789726257, 1.0, 1.1111925840377808] 51 57\n",
      "[6.80708180880174e-05, 1.0, 1.000047206878662] 51 57\n",
      "[0.0020396036561578512, 1.0, 1.0014147758483887] 51 78\n",
      "[0.03644495829939842, 1.0, 1.0255835056304932] 51 37\n",
      "[0.13125191628932953, 1.0, 1.0952436923980713] 51 93\n",
      "[0.958266019821167, 0.0, 1.942973256111145] 51 112\n",
      "[0.015955401584506035, 1.0, 1.0111207962036133] 51 52\n",
      "[0.0003767723392229527, 1.0, 1.0002611875534058] 51 109\n",
      "[7.629423635080457e-06, 1.0, 1.0000052452087402] 51 192\n",
      "[0.010000416077673435, 1.0, 1.006955862045288] 51 125\n",
      "[0.005842254497110844, 1.0, 1.004057765007019] 51 73\n",
      "[3.814704541582614e-06, 1.0, 1.0000026226043701] 51 56\n",
      "[0.028252432122826576, 1.0, 1.0197761058807373] 51 72\n",
      "[0.11370760947465897, 1.0, 1.0820053815841675] 51 73\n",
      "[0.024380505084991455, 1.0, 1.017042875289917] 51 50\n",
      "[0.008477339521050453, 1.0, 1.005893349647522] 51 140\n",
      "[0.0045774877071380615, 1.0, 1.0031778812408447] 51 153\n",
      "[0.004969107918441296, 1.0, 1.0034502744674683] 51 278\n",
      "[0.036149267107248306, 1.0, 1.0253733396530151] 51 49\n",
      "[0.06484698504209518, 1.0, 1.0459740161895752] 51 119\n",
      "[0.0006881935405544937, 1.0, 1.0004770755767822] 51 81\n",
      "[0.0015673396410420537, 1.0, 1.001086950302124] 51 127\n",
      "[3.999551699962467e-05, 1.0, 1.0000277757644653] 51 217\n",
      "[0.008146597072482109, 1.0, 1.0056627988815308] 51 116\n",
      "[0.00022682138660456985, 1.0, 1.0001572370529175] 51 109\n",
      "[0.04309504106640816, 1.0, 1.0303218364715576] 51 117\n",
      "[0.018812311813235283, 1.0, 1.0131250619888306] 51 137\n",
      "[0.17511984705924988, 1.0, 1.1290582418441772] 51 125\n",
      "[0.00011552047362783924, 1.0, 1.0000801086425781] 51 77\n",
      "[0.006545341107994318, 1.0, 1.0045472383499146] 51 99\n",
      "[0.017118528485298157, 1.0, 1.0119363069534302] 51 34\n",
      "[0.008192268200218678, 1.0, 1.0056946277618408] 51 95\n",
      "[0.0008558716508559883, 1.0, 1.0005934238433838] 51 192\n",
      "[0.02135603316128254, 1.0, 1.0149129629135132] 51 158\n",
      "[0.14478543400764465, 1.0, 1.1055662631988525] 51 56\n",
      "[0.019362857565283775, 1.0, 1.0135117769241333] 51 64\n",
      "[0.019792838022112846, 1.0, 1.013813853263855] 51 43\n",
      "[0.005298740230500698, 1.0, 1.0036795139312744] 51 93\n",
      "[0.019784752279520035, 1.0, 1.013808250427246] 51 45\n",
      "[0.047667454928159714, 1.0, 1.0335924625396729] 51 84\n",
      "[1.1920928955078125e-07, 1.0, 1.0000001192092896] 51 143\n",
      "[0.061752717941999435, 1.0, 1.04373300075531] 51 65\n",
      "[0.000133761772303842, 1.0, 1.0000927448272705] 51 56\n",
      "[0.0011602811282500625, 1.0, 1.0008045434951782] 51 86\n",
      "[0.001366475597023964, 1.0, 1.0009475946426392] 51 114\n",
      "[0.012698107399046421, 1.0, 1.008840560913086] 51 49\n",
      "[7.748606662971724e-07, 1.0, 1.0000005960464478] 51 106\n",
      "[0.0009283559047617018, 1.0, 1.0006437301635742] 51 419\n",
      "[0.004349078983068466, 1.0, 1.003019094467163] 51 80\n",
      "[0.0836930125951767, 1.0, 1.0597273111343384] 51 137\n",
      "[0.014307016506791115, 1.0, 1.009966254234314] 51 49\n",
      "[0.0008857595385052264, 1.0, 1.0006141662597656] 51 98\n",
      "[0.000874066783580929, 1.0, 1.0006060600280762] 51 82\n",
      "[0.00014628049393650144, 1.0, 1.0001014471054077] 51 215\n",
      "[1.7881409348774469e-06, 1.0, 1.0000011920928955] 51 96\n",
      "[0.09817124158143997, 1.0, 1.070415735244751] 51 120\n",
      "[0.00024899959680624306, 1.0, 1.0001726150512695] 51 109\n",
      "[0.09937971830368042, 1.0, 1.0713127851486206] 51 135\n",
      "[0.01329530868679285, 1.0, 1.0092581510543823] 51 224\n",
      "[0.004271677229553461, 1.0, 1.0029653310775757] 51 59\n",
      "[0.009013080969452858, 1.0, 1.0062669515609741] 51 87\n",
      "[4.124726547161117e-05, 1.0, 1.0000286102294922] 51 142\n",
      "[0.005320431664586067, 1.0, 1.0036946535110474] 51 41\n",
      "[5.173817044124007e-05, 1.0, 1.0000358819961548] 51 88\n",
      "[0.004016407765448093, 1.0, 1.0027878284454346] 51 125\n",
      "[0.03636323660612106, 1.0, 1.025525450706482] 51 65\n",
      "[0.0020055007189512253, 1.0, 1.001391053199768] 51 86\n",
      "[0.006149985361844301, 1.0, 1.0042719841003418] 51 152\n",
      "[0.057961784303188324, 1.0, 1.0409940481185913] 51 70\n",
      "[0.005020025186240673, 1.0, 1.0034856796264648] 51 113\n",
      "[0.09386613965034485, 1.0, 1.0672262907028198] 51 193\n",
      "[0.00013894807489123195, 1.0, 1.000096321105957] 51 208\n",
      "[4.315469413995743e-05, 1.0, 1.0000299215316772] 51 164\n",
      "[0.017436543479561806, 1.0, 1.0121594667434692] 51 164\n",
      "[0.007371647749096155, 1.0, 1.0051226615905762] 51 94\n",
      "[0.09060748666524887, 1.0, 1.0648185014724731] 51 193\n",
      "[0.006587759125977755, 1.0, 1.0045766830444336] 51 104\n",
      "[0.04094440117478371, 1.0, 1.0287870168685913] 51 36\n",
      "[0.020900975912809372, 1.0, 1.0145928859710693] 51 46\n",
      "[0.00012851586507167667, 1.0, 1.0000890493392944] 51 127\n",
      "[0.1946430653333664, 1.0, 1.14444100856781] 51 62\n",
      "[0.1486850082874298, 1.0, 1.1085585355758667] 51 43\n",
      "[0.00037665307172574103, 1.0, 1.0002610683441162] 51 192\n",
      "[0.10776224732398987, 1.0, 1.077555537223816] 51 97\n",
      "[0.22090773284435272, 1.0, 1.1654666662216187] 51 69\n",
      "[0.002397070173174143, 1.0, 1.0016628503799438] 51 221\n",
      "[0.12207821011543274, 1.0, 1.088301420211792] 51 140\n",
      "[0.008658172562718391, 1.0, 1.0060194730758667] 51 139\n",
      "[0.03969389200210571, 1.0, 1.0278956890106201] 51 109\n",
      "[0.16351303458213806, 1.0, 1.1200110912322998] 51 219\n",
      "[0.25302112102508545, 1.0, 1.1916999816894531] 51 139\n",
      "[4.768372718899627e-07, 1.0, 1.0000003576278687] 51 86\n",
      "[7.152559646783629e-07, 1.0, 1.0000004768371582] 51 161\n",
      "[0.0005396272172220051, 1.0, 1.0003740787506104] 51 198\n",
      "[0.12678536772727966, 1.0, 1.091858148574829] 51 61\n",
      "[0.0004932308802381158, 1.0, 1.0003418922424316] 51 170\n",
      "[0.03620179742574692, 1.0, 1.0254106521606445] 51 133\n",
      "[0.12770192325115204, 1.0, 1.0925519466400146] 51 162\n",
      "[0.013175719417631626, 1.0, 1.0091745853424072] 51 76\n",
      "[0.0011532397475093603, 1.0, 1.0007996559143066] 51 203\n",
      "[0.0547051802277565, 1.0, 1.0386468172073364] 51 214\n",
      "[0.011843021027743816, 1.0, 1.0082427263259888] 51 189\n",
      "[0.014322313480079174, 1.0, 1.009976863861084] 51 23\n",
      "[0.00044707514462061226, 1.0, 1.000309944152832] 51 107\n",
      "[8.416530181420967e-05, 1.0, 1.0000582933425903] 51 80\n",
      "[0.14354954659938812, 1.0, 1.1046195030212402] 51 52\n",
      "[0.0010958355851471424, 1.0, 1.0007598400115967] 51 42\n",
      "[0.007500573992729187, 1.0, 1.0052125453948975] 51 78\n",
      "[0.0059815370477736, 1.0, 1.0041546821594238] 51 185\n",
      "[0.003966917749494314, 1.0, 1.002753496170044] 51 135\n",
      "[0.01996435783803463, 1.0, 1.0139343738555908] 51 83\n",
      "[0.0027737305499613285, 1.0, 1.0019243955612183] 51 87\n",
      "[1.7047073924914002e-05, 1.0, 1.0000118017196655] 51 165\n",
      "[0.04652971029281616, 1.0, 1.0327776670455933] 51 128\n",
      "[0.05495477095246315, 1.0, 1.038826584815979] 51 236\n",
      "[0.057249896228313446, 1.0, 1.0404804944992065] 51 399\n",
      "[0.00035029821447096765, 1.0, 1.000242829322815] 51 133\n",
      "[0.008097322657704353, 1.0, 1.0056284666061401] 51 206\n",
      "[0.02259453758597374, 1.0, 1.0157846212387085] 51 173\n",
      "[0.0003619253111537546, 1.0, 1.0002509355545044] 51 198\n",
      "[0.0001576070935698226, 1.0, 1.0001091957092285] 51 103\n",
      "[0.009600742720067501, 1.0, 1.0066769123077393] 51 249\n",
      "[8.213857654482126e-05, 1.0, 1.0000569820404053] 51 208\n",
      "[0.07707808911800385, 1.0, 1.0548794269561768] 51 145\n",
      "[0.0001227335014846176, 1.0, 1.0000851154327393] 51 318\n",
      "[0.056728433817625046, 1.0, 1.0401045083999634] 51 204\n",
      "[0.02371200919151306, 1.0, 1.0165717601776123] 51 251\n",
      "[0.035581253468990326, 1.0, 1.0249696969985962] 51 168\n",
      "[0.02053811214864254, 1.0, 1.0143377780914307] 51 222\n",
      "[0.08835075795650482, 1.0, 1.0631541013717651] 51 107\n",
      "[0.00025019198074005544, 1.0, 1.0001734495162964] 51 146\n",
      "[0.1444704532623291, 1.0, 1.1053248643875122] 51 36\n",
      "[2.2649790025752736e-06, 1.0, 1.0000015497207642] 51 49\n",
      "[0.01577548310160637, 1.0, 1.010994791984558] 51 197\n",
      "[0.046557873487472534, 1.0, 1.0327978134155273] 51 74\n",
      "[0.005127619486302137, 1.0, 1.0035605430603027] 51 222\n",
      "[0.15499085187911987, 1.0, 1.1134145259857178] 51 219\n",
      "[0.0008796148467808962, 1.0, 1.0006098747253418] 51 150\n",
      "[7.39124880055897e-05, 1.0, 1.0000512599945068] 51 91\n",
      "[0.023442333564162254, 1.0, 1.0163817405700684] 51 22\n",
      "[0.38051319122314453, 1.0, 1.3018049001693726] 51 175\n",
      "[0.04724138602614403, 1.0, 1.0332872867584229] 51 35\n",
      "[2.3120763301849365, 0.0, 4.965972900390625] 51 208\n",
      "PRON VERB PUNCT PRON AUX ADP PRON NOUN PRON NOUN VERB PUNCT ADV AUX PRON PRON ADV ADP DET NOUN DET NOUN PROPN CONJ DET PROPN PROPN VERB PUNCT PRON ADP DET NOUN ADJ NOUN PUNCT ADJ NOUN CONJ ADJ NOUN VERB PUNCT\n",
      "[0.0021357079967856407, 1.0, 1.0014814138412476] 51 85\n",
      "[8.344653679159819e-07, 1.0, 1.0000005960464478] 51 144\n",
      "[0.3189608156681061, 1.0, 1.2474316358566284] 51 143\n",
      "[0.03659932315349579, 1.0, 1.0256932973861694] 51 60\n",
      "[0.0021662914659827948, 1.0, 1.0015026330947876] 51 148\n",
      "[0.21385887265205383, 1.0, 1.1597862243652344] 51 66\n",
      "[0.00041058147326111794, 1.0, 1.0002846717834473] 51 348\n",
      "[5.163388729095459, 0.0, 35.8372688293457] 51 51\n",
      "ADV VERB DET PRON ADJ PROPN ADP PROPN ADV ADJ PUNCT\n",
      "[2.425938509986736e-05, 1.0, 1.0000168085098267] 51 145\n",
      "[0.3761703073978424, 1.0, 1.2978919744491577] 51 106\n",
      "[0.04506043344736099, 1.0, 1.031726360321045] 51 169\n",
      "[0.08340005576610565, 1.0, 1.0595121383666992] 51 295\n",
      "[2.98023678624304e-06, 1.0, 1.0000020265579224] 51 75\n",
      "[0.000822047411929816, 1.0, 1.0005699396133423] 51 81\n",
      "[0.0032486580312252045, 1.0, 1.0022543668746948] 51 178\n",
      "[0.14793621003627777, 1.0, 1.1079833507537842] 51 176\n",
      "[0.012977341189980507, 1.0, 1.0090358257293701] 51 77\n",
      "[0.0019097074400633574, 1.0, 1.0013245344161987] 51 100\n",
      "[0.036343395709991455, 1.0, 1.0255112648010254] 51 63\n",
      "[0.014683171175420284, 1.0, 1.0102295875549316] 51 66\n",
      "[0.00012863508891314268, 1.0, 1.000089168548584] 51 93\n",
      "[0.07562723755836487, 1.0, 1.0538190603256226] 51 94\n",
      "[0.014455587603151798, 1.0, 1.0100702047348022] 51 32\n",
      "[0.000891427043825388, 1.0, 1.0006181001663208] 51 50\n",
      "[0.22600193321704865, 1.0, 1.1695892810821533] 51 29\n",
      "[0.003110054414719343, 1.0, 1.0021580457687378] 51 96\n",
      "[3.580120325088501, 0.0, 11.95979118347168] 51 108\n",
      "SCONJ PRON AUX ADV PART PUNCT SCONJ PRON ADP PRON NOUN VERB PUNCT DET NOUN ADP NOUN CONJ DET NOUN VERB PUNCT\n",
      "[0.007000574376434088, 1.0, 1.00486421585083] 51 28\n",
      "[0.27585569024086, 1.0, 1.21071195602417] 51 133\n",
      "[0.000693144160322845, 1.0, 1.0004805326461792] 51 23\n",
      "[0.10655639320611954, 1.0, 1.0766552686691284] 51 82\n",
      "[1.1920928955078125e-07, 1.0, 1.0000001192092896] 51 95\n",
      "[0.0015860253479331732, 1.0, 1.001099944114685] 51 104\n",
      "[0.2392890900373459, 1.0, 1.1804108619689941] 51 107\n",
      "[0.002639615209773183, 1.0, 1.001831293106079] 51 121\n",
      "[0.04993424192070961, 1.0, 1.0352177619934082] 51 67\n",
      "[0.01352268923074007, 1.0, 1.0094172954559326] 51 138\n",
      "[0.0050725629553198814, 1.0, 1.003522276878357] 51 65\n",
      "[0.08012580871582031, 1.0, 1.0571101903915405] 51 97\n",
      "[0.023172179237008095, 1.0, 1.0161913633346558] 51 124\n",
      "[0.0012551074614748359, 1.0, 1.0008703470230103] 51 180\n",
      "[0.022502664476633072, 1.0, 1.0157198905944824] 51 140\n",
      "[0.057930838316679, 1.0, 1.0409716367721558] 51 153\n",
      "[0.014221827499568462, 1.0, 1.009906530380249] 51 187\n",
      "[0.058405909687280655, 1.0, 1.0413144826889038] 51 185\n",
      "[0.1106971874833107, 1.0, 1.0797499418258667] 51 259\n",
      "[0.016731277108192444, 1.0, 1.0116647481918335] 51 194\n",
      "[0.014414465986192226, 1.0, 1.0100414752960205] 51 47\n",
      "[0.013175900094211102, 1.0, 1.0091747045516968] 51 394\n",
      "[0.4675862193107605, 1.0, 1.3827940225601196] 51 95\n",
      "[0.08529705554246902, 1.0, 1.060906171798706] 51 223\n",
      "[0.052384667098522186, 1.0, 1.0369775295257568] 51 69\n",
      "[0.018132658675312996, 1.0, 1.0126478672027588] 51 77\n",
      "[0.060395896434783936, 1.0, 1.0427519083023071] 51 72\n",
      "[1.9073504518019035e-06, 1.0, 1.000001311302185] 51 83\n",
      "[0.009582208469510078, 1.0, 1.0066640377044678] 51 175\n",
      "[0.0001525995321571827, 1.0, 1.0001057386398315] 51 162\n",
      "[0.01331463735550642, 1.0, 1.0092717409133911] 51 122\n",
      "[0.00889815203845501, 1.0, 1.006186842918396] 51 29\n",
      "[0.0030942698940634727, 1.0, 1.0021470785140991] 51 107\n",
      "[0.0001384115603286773, 1.0, 1.0000959634780884] 51 96\n",
      "[0.0004703912709373981, 1.0, 1.000326156616211] 51 101\n",
      "[0.018245315179228783, 1.0, 1.0127270221710205] 51 37\n",
      "[0.03164872154593468, 1.0, 1.0221796035766602] 51 91\n",
      "[0.209844708442688, 1.0, 1.156563639640808] 51 224\n",
      "[0.3503776788711548, 1.0, 1.2748943567276] 51 133\n",
      "[1.406679530191468e-05, 1.0, 1.0000097751617432] 51 106\n",
      "[5.245222382654902e-06, 1.0, 1.0000035762786865] 51 120\n",
      "[0.05695139616727829, 1.0, 1.0402652025222778] 51 79\n"
     ]
    }
   ],
   "source": [
    "#individual\n",
    "for i in range(1200,2220):\n",
    "        eva_sequences = array(eng2deu_sequences[i:i+1])\n",
    "        X1, y1 = eva_sequences[:,:-1],eva_sequences[:,-1]\n",
    "        y1 = to_categorical(y1, num_classes=vocab_size)\n",
    "\n",
    "        eva = model.evaluate(X1,y1, verbose=0)\n",
    "        \n",
    "        print(eva, len(eva_sequences[0]), len(deu_lines[i]))\n",
    "        if eva[2]>3: print(deu_lines[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Breaking the lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#1. train and test a model on german\n",
    "#2. train and test a model on english\n",
    "#3. cross-test them on the translations: eng on deu tra, deu on eng tra\n",
    "#4. cross-validate them: eng on eng ori, deu on deu ori ..."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#1. train and test a model on german"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 599,
   "metadata": {},
   "outputs": [],
   "source": [
    "#german pos sequence\n",
    "data = open(\"train/epuds.de.pos\").read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 600,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we need a common max lenght for both English and German corpora. \n",
    "#Emipirically, I will fix it to 252, the longest sequence of both corpora.\n",
    "\n",
    "max_length = 252 #max([len(seq) for seq in sequences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 601,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Sequences: 8635158\n",
      "Max Sequence Length: 252\n"
     ]
    }
   ],
   "source": [
    "# create line-based sequences\n",
    "\n",
    "sequences = list()\n",
    "for line in data.split('\\n'):\n",
    "#for line in data_sents[:100]:\n",
    "\tencoded = tokenizer.texts_to_sequences([line])[0]\n",
    "\tfor i in range(1, len(encoded)):\n",
    "\t\tsequence = encoded[:i+1]\n",
    "\t\tsequences.append(sequence)\n",
    "print('Total Sequences: %d' % len(sequences))\n",
    "\n",
    "# pad input sequences\n",
    "sequences = pad_sequences(sequences, maxlen=max_length, padding='pre')\n",
    "print('Max Sequence Length: %d' % max_length)\n",
    "\n",
    "# split into input and output elements\n",
    "sequences = array(sequences)\n",
    "X, y = sequences[:,:-1],sequences[:,-1]\n",
    "y = to_categorical(y, num_classes=vocab_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 804,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8635158, 251) (8635158, 16) (6000, 251) (6000, 16)\n"
     ]
    }
   ],
   "source": [
    "#we set a training part. Very small here (I am toying)\n",
    "trainX = X[:6000]\n",
    "trainY = y[:6000]\n",
    "\n",
    "print(X.shape, y.shape, trainX.shape, trainY.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 849,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_34 (Embedding)     (None, 251, 10)           160       \n",
      "_________________________________________________________________\n",
      "lstm_52 (LSTM)               (None, 20)                2480      \n",
      "_________________________________________________________________\n",
      "dense_60 (Dense)             (None, 10)                210       \n",
      "_________________________________________________________________\n",
      "dense_61 (Dense)             (None, 16)                176       \n",
      "=================================================================\n",
      "Total params: 3,026\n",
      "Trainable params: 3,026\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# define model\n",
    "from keras.layers import Dropout\n",
    "\n",
    "model = Sequential()\n",
    "#model.add(Embedding(vocab_size, 10, input_length=max_length-1))\n",
    "model.add(Embedding(vocab_size, 30, input_length=max_length-1))\n",
    "#model.add(LSTM(100, return_sequences=True))\n",
    "#model.add(Dropout(.2))\n",
    "model.add(LSTM(20)) #100\n",
    "model.add(Dense(10, activation='relu')) #100\n",
    "model.add(Dense(vocab_size, activation='softmax'))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 850,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile network\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy',perplexity2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 852,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1199 samples, validate on 4801 samples\n",
      "Epoch 1/20\n",
      " - 11s - loss: 2.6981 - acc: 0.1334 - perplexity2: 6.5978 - val_loss: 2.6014 - val_acc: 0.1250 - val_perplexity2: 6.5653\n",
      "Epoch 2/20\n",
      " - 10s - loss: 2.5589 - acc: 0.1334 - perplexity2: 6.3211 - val_loss: 2.5214 - val_acc: 0.1250 - val_perplexity2: 6.1824\n",
      "Epoch 3/20\n",
      " - 10s - loss: 2.5034 - acc: 0.1735 - perplexity2: 6.2316 - val_loss: 2.4831 - val_acc: 0.1795 - val_perplexity2: 6.0623\n",
      "Epoch 4/20\n",
      " - 10s - loss: 2.4719 - acc: 0.1610 - perplexity2: 6.1001 - val_loss: 2.4567 - val_acc: 0.1795 - val_perplexity2: 6.0278\n",
      "Epoch 5/20\n",
      " - 10s - loss: 2.4493 - acc: 0.1610 - perplexity2: 5.9478 - val_loss: 2.4345 - val_acc: 0.1829 - val_perplexity2: 5.9847\n",
      "Epoch 6/20\n",
      " - 10s - loss: 2.4244 - acc: 0.2110 - perplexity2: 5.9565 - val_loss: 2.4057 - val_acc: 0.2258 - val_perplexity2: 5.8901\n",
      "Epoch 7/20\n",
      " - 10s - loss: 2.3969 - acc: 0.2319 - perplexity2: 5.9580 - val_loss: 2.3676 - val_acc: 0.2456 - val_perplexity2: 5.7939\n",
      "Epoch 8/20\n",
      " - 10s - loss: 2.3578 - acc: 0.2285 - perplexity2: 5.8142 - val_loss: 2.3148 - val_acc: 0.2566 - val_perplexity2: 5.7098\n",
      "Epoch 9/20\n",
      " - 10s - loss: 2.3198 - acc: 0.2394 - perplexity2: 5.7749 - val_loss: 2.2755 - val_acc: 0.2545 - val_perplexity2: 5.6034\n",
      "Epoch 10/20\n",
      " - 10s - loss: 2.2816 - acc: 0.2335 - perplexity2: 5.6943 - val_loss: 2.2339 - val_acc: 0.2595 - val_perplexity2: 5.6006\n",
      "Epoch 11/20\n",
      " - 11s - loss: 2.2515 - acc: 0.2410 - perplexity2: 5.7092 - val_loss: 2.2109 - val_acc: 0.2604 - val_perplexity2: 5.5222\n",
      "Epoch 12/20\n",
      " - 10s - loss: 2.2302 - acc: 0.2594 - perplexity2: 5.6285 - val_loss: 2.1872 - val_acc: 0.2652 - val_perplexity2: 5.5393\n",
      "Epoch 13/20\n",
      " - 10s - loss: 2.2057 - acc: 0.2661 - perplexity2: 5.6162 - val_loss: 2.1697 - val_acc: 0.2747 - val_perplexity2: 5.5262\n",
      "Epoch 14/20\n",
      " - 10s - loss: 2.1880 - acc: 0.2619 - perplexity2: 5.5781 - val_loss: 2.1556 - val_acc: 0.2814 - val_perplexity2: 5.5258\n",
      "Epoch 15/20\n",
      " - 10s - loss: 2.1696 - acc: 0.2877 - perplexity2: 5.5527 - val_loss: 2.1450 - val_acc: 0.2762 - val_perplexity2: 5.5204\n",
      "Epoch 16/20\n",
      " - 10s - loss: 2.1548 - acc: 0.2777 - perplexity2: 5.4415 - val_loss: 2.1355 - val_acc: 0.2876 - val_perplexity2: 5.5766\n",
      "Epoch 17/20\n",
      " - 10s - loss: 2.1437 - acc: 0.2894 - perplexity2: 5.5389 - val_loss: 2.1192 - val_acc: 0.2841 - val_perplexity2: 5.4905\n",
      "Epoch 18/20\n",
      " - 10s - loss: 2.1317 - acc: 0.2911 - perplexity2: 5.4717 - val_loss: 2.1152 - val_acc: 0.2951 - val_perplexity2: 5.5486\n",
      "Epoch 19/20\n",
      " - 10s - loss: 2.1138 - acc: 0.2944 - perplexity2: 5.4716 - val_loss: 2.0976 - val_acc: 0.2979 - val_perplexity2: 5.4829\n",
      "Epoch 20/20\n",
      " - 10s - loss: 2.0995 - acc: 0.3003 - perplexity2: 5.4611 - val_loss: 2.0890 - val_acc: 0.2989 - val_perplexity2: 5.5085\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1b054f6f98>"
      ]
     },
     "execution_count": 852,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit network\n",
    "model.fit(trainX, trainY, epochs=20, verbose=2,validation_split=.8) #overfitting happens fast!\n",
    "# for the record: \n",
    "#1000X, first iteration: 11-16s\n",
    "#2000X, first iteration: 25-28ss\n",
    "#4000X, first iteration: 52s\n",
    "#6000X, 70s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 853,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a new piece of data\n",
    "data = \"\"\"PROPN VERB ADJ DET NOUN VERB PUNCT PROPN VERB PRON PRON PUNCT\"\"\".lower() # a specific, German sequence from the 'test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 854,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Sequences: 11\n",
      "Max Sequence Length: 252\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.3281173706054688, 0.1818181872367859, 6.2405290603637695]"
      ]
     },
     "execution_count": 854,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluating this specific sequence\n",
    "sequences = list()\n",
    "for line in data.split('\\n'):\n",
    "\tencoded = tokenizer.texts_to_sequences([line])[0]\n",
    "\tfor i in range(1, len(encoded)):\n",
    "\t\tsequence = encoded[:i+1]\n",
    "\t\tsequences.append(sequence)\n",
    "print('Total Sequences: %d' % len(sequences))\n",
    "\n",
    "# pad input sequences\n",
    "#max_length = max([len(seq) for seq in sequences])\n",
    "sequences = pad_sequences(sequences, maxlen=max_length, padding='pre')\n",
    "print('Max Sequence Length: %d' % max_length)\n",
    "\n",
    "# split into input and output elements\n",
    "sequences = array(sequences)\n",
    "X1, y1 = sequences[:,:-1],sequences[:,-1]\n",
    "y1 = to_categorical(y1, num_classes=vocab_size)\n",
    "\n",
    "model.evaluate(X1,y1, verbose=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 855,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2.1413686943054198, 0.30083333333333334, 5.670599614461263]"
      ]
     },
     "execution_count": 855,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#evaluating the model on an equally small slice of the corpus\n",
    "model.evaluate(X[6000:12000],y[6000:12000], verbose=0) # should be similar to the scale of the original perplexity"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#2. train and test a model on english"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 812,
   "metadata": {},
   "outputs": [],
   "source": [
    "#english model\n",
    "dataE = open(\"train/epuds.en.diff.pos\").read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 877,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Sequences: 5727538\n",
      "Max Sequence Length: 252\n"
     ]
    }
   ],
   "source": [
    "#create the sequences\n",
    "import random\n",
    "\n",
    "sequences = list()\n",
    "for line in dataE.split('\\n'):\n",
    "#for line in data_sents[:100]:\n",
    "\tencoded = tokenizer.texts_to_sequences([line])[0]\n",
    "\tfor i in range(1, len(encoded)):\n",
    "\t\tsequence = encoded[:i+1]\n",
    "\t\tsequences.append(sequence)\n",
    "print('Total Sequences: %d' % len(sequences))\n",
    "\n",
    "# pad input sequences\n",
    "#max_length = max([len(seq) for seq in sequences]) > we keep the original length\n",
    "\n",
    "sequences = pad_sequences(sequences, maxlen=max_length, padding='pre')\n",
    "print('Max Sequence Length: %d' % max_length)\n",
    "\n",
    "# split into input and output elements\n",
    "#random.shuffle(sequences)\n",
    "sequences = array(sequences)\n",
    "XE, yE = sequences[:,:-1],sequences[:,-1]\n",
    "yE = to_categorical(yE, num_classes=vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 878,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5727538, 251) (5727538, 16) (6000, 251) (6000, 16)\n"
     ]
    }
   ],
   "source": [
    "#train slice\n",
    "\n",
    "trainXE = XE[:6000]\n",
    "trainYE = yE[:6000]\n",
    "\n",
    "print(XE.shape, yE.shape, trainXE.shape, trainYE.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 902,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_40 (Embedding)     (None, 251, 50)           800       \n",
      "_________________________________________________________________\n",
      "lstm_60 (LSTM)               (None, 251, 100)          60400     \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 251, 100)          0         \n",
      "_________________________________________________________________\n",
      "lstm_61 (LSTM)               (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dense_72 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_73 (Dense)             (None, 16)                1616      \n",
      "=================================================================\n",
      "Total params: 153,316\n",
      "Trainable params: 153,316\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# define model\n",
    "modelE = Sequential()\n",
    "modelE.add(Embedding(vocab_size, 50, input_length=max_length-1))\n",
    "#model.add(Embedding(vocab_size, 50, input_length=max_length-1))\n",
    "modelE.add(LSTM(100, return_sequences=True))\n",
    "modelE.add(Dropout(.2))\n",
    "modelE.add(LSTM(100))\n",
    "modelE.add(Dense(100, activation='relu'))\n",
    "modelE.add(Dense(vocab_size, activation='softmax'))\n",
    "print(modelE.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 903,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile network\n",
    "modelE.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy',perplexity2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 904,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1199 samples, validate on 4801 samples\n",
      "Epoch 1/2\n",
      " - 31s - loss: 2.4666 - acc: 0.1985 - perplexity2: 6.2917 - val_loss: 2.4056 - val_acc: 0.1189 - val_perplexity2: 6.0951\n",
      "Epoch 2/2\n",
      " - 23s - loss: 2.3023 - acc: 0.1935 - perplexity2: 5.8183 - val_loss: 2.3880 - val_acc: 0.2304 - val_perplexity2: 6.2839\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1afd5af518>"
      ]
     },
     "execution_count": 904,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit network\n",
    "modelE.fit(trainXE, trainYE, epochs=2, verbose=2, validation_split=.8)\n",
    "#for the record: 2000X takes 26ss first iteration"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "After 25 epochs, simpler model:\n",
    "7s - loss: 1.9055 - acc: 0.3542 - perplexity2: 4.8884 - val_loss: 1.9871 - val_acc: 0.3800 - val_perplexity2: 7.1503\n",
    "\n",
    "After 5 epochs, complex model:\n",
    "- 14s - loss: 2.3339 - acc: 0.2192 - perplexity2: 5.8088 - val_loss: 2.4440 - val_acc: 0.1900 - val_perplexity2: 6.8791\n",
    "After 10 epochs, complex model:\n",
    " - 15s - loss: 2.1071 - acc: 0.2700 - perplexity2: 5.4199 - val_loss: 2.1624 - val_acc: 0.2500 - val_perplexity2: 6.2028"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 899,
   "metadata": {},
   "outputs": [],
   "source": [
    "#as above\n",
    "data = \"\"\"PROPN PUNCT PROPN PUNCT PROPN PROPN CONJ PROPN PROPN AUX VERB PRON NOUN VERB DET NOUN ADP ADJ NOUN PRON AUX VERB ADP ADP ADV PUNCT\"\"\".lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 900,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Sequences: 25\n",
      "Max Sequence Length: 252\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[3.1158721446990967, 0.1599999964237213, 14.805424690246582]"
      ]
     },
     "execution_count": 900,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#evaluate the data\n",
    "sequences = list()\n",
    "for line in data.split('\\n'):\n",
    "\tencoded = tokenizer.texts_to_sequences([line])[0]\n",
    "\tfor i in range(1, len(encoded)):\n",
    "\t\tsequence = encoded[:i+1]\n",
    "\t\tsequences.append(sequence)\n",
    "print('Total Sequences: %d' % len(sequences))\n",
    "\n",
    "# pad input sequences\n",
    "#max_length = max([len(seq) for seq in sequences])\n",
    "sequences = pad_sequences(sequences, maxlen=max_length, padding='pre')\n",
    "print('Max Sequence Length: %d' % max_length)\n",
    "\n",
    "# split into input and output elements\n",
    "sequences = array(sequences)\n",
    "X1, y1 = sequences[:,:-1],sequences[:,-1]\n",
    "y1 = to_categorical(y1, num_classes=vocab_size)\n",
    "\n",
    "modelE.evaluate(X1,y1, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 901,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2.4531072902679445, 0.21366666666666667, 8.294150867462157]"
      ]
     },
     "execution_count": 901,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#evaluate on other english sequences\n",
    "modelE.evaluate(XE[6000:12000],yE[6000:12000], verbose=0) #perplexity is a bit higher (overfitting already kicked in) but comparable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 885,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2.4642726554870604, 0.2245, 8.295025939941405]"
      ]
     },
     "execution_count": 885,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#how does the german model perform?\n",
    "model.evaluate(XE[2000:4000],yE[2000:4000], verbose=0) #the German model is very surprised by the english sentence (and this is good)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#3. cross-test them on the translations: eng on deu tra, deu on eng tra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 823,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data\n",
    "deutra = open(\"train/epuds.en-de.de.pos\").read() #this is German translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 824,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Sequences: 3533328\n",
      "Max Sequence Length: 252\n"
     ]
    }
   ],
   "source": [
    "#prepare the data \n",
    "sequences = list()\n",
    "for line in deutra.split('\\n'):\n",
    "\tencoded = tokenizer.texts_to_sequences([line])[0]\n",
    "\tfor i in range(1, len(encoded)):\n",
    "\t\tsequence = encoded[:i+1]\n",
    "\t\tsequences.append(sequence)\n",
    "print('Total Sequences: %d' % len(sequences))\n",
    "\n",
    "# pad input sequences\n",
    "sequences = pad_sequences(sequences, maxlen=max_length, padding='pre')\n",
    "print('Max Sequence Length: %d' % max_length)\n",
    "\n",
    "# split into input and output elements\n",
    "sequences = array(sequences)\n",
    "Xdeutra, ydeutra = sequences[:,:-1],sequences[:,-1]\n",
    "ydeutra = to_categorical(ydeutra, num_classes=vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 886,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2.1503540115356445, 0.3135, 5.839153644561768]"
      ]
     },
     "execution_count": 886,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#How does German perform on the German translations? (moving small scale)\n",
    "model.evaluate(Xdeutra[:2000],ydeutra[:2000], verbose=0) #German models seems a bit surprised by the translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 887,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2.705368900299072, 0.1495, 8.670412788391113]"
      ]
     },
     "execution_count": 887,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#What about English?\n",
    "modelE.evaluate(Xdeutra[:2000],ydeutra[:2000], verbose=0) #English is very surprised! (good)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 888,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2.364716844558716, 0.22, 6.203449783325195]"
      ]
     },
     "execution_count": 888,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#For a comparison, this is the surprisal of English on English:\n",
    "modelE.evaluate(XE[4000:6000],yE[4000:6000], verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 889,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2.1052982006073, 0.303, 5.621415363311767]"
      ]
     },
     "execution_count": 889,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Same with German\n",
    "model.evaluate(X[4000:6000],y[4000:6000], verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and so forth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 890,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.4003945388793944, 0.2135, 6.539171417236328]\n",
      "[2.1339319400787353, 0.303, 5.613580635070801]\n",
      "[2.140509120941162, 0.31033333333333335, 5.8668851509094235]\n"
     ]
    }
   ],
   "source": [
    "#In \"conclusion\"\n",
    "#1. English on general English\n",
    "print(modelE.evaluate(XE[6000:8000],yE[6000:8000], verbose=0)[2])\n",
    "#2. English on to-be-translated English\n",
    "#3. German on general German\n",
    "print(model.evaluate(X[6000:8000],y[6000:8000], verbose=0)[2])\n",
    "#4. German on German translation\n",
    "print(model.evaluate(Xdeutra[:3000],ydeutra[:3000], verbose=0)[2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 891,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.513754270553589, 0.226, 8.65403955078125]\n"
     ]
    }
   ],
   "source": [
    "#German on English\n",
    "print(model.evaluate(XE[6000:8000],yE[6000:8000], verbose=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "it can be a good idea to save the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 831,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's save these guys\n",
    "from keras.models import load_model\n",
    "\n",
    "#model.save('German_deep_25.h5')  # creates a HDF5 file 'my_model.h5'\n",
    "#del model  # deletes the existing model\n",
    "modelE.save('English_deep_25.h5') \n",
    "# returns a compiled model\n",
    "# identical to the previous one\n",
    "#model = load_model('my_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can look at the single sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 637,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.629480838775635\n",
      "1.8778960704803467\n",
      "\n",
      "\n",
      "4.333113670349121\n",
      "2.245802164077759\n",
      "\n",
      "\n",
      "12.823036193847656\n",
      "13.32221794128418\n",
      "shining through?\n",
      "\n",
      "\n",
      "2.006741762161255\n",
      "2.4519245624542236\n",
      "shining through?\n",
      "\n",
      "\n",
      "4.848020076751709\n",
      "2.7908830642700195\n",
      "\n",
      "\n",
      "14.375545501708984\n",
      "3.5838027000427246\n",
      "\n",
      "\n",
      "5.641058921813965\n",
      "3.1994900703430176\n",
      "\n",
      "\n",
      "15.491340637207031\n",
      "4.042491912841797\n",
      "\n",
      "\n",
      "7.334090232849121\n",
      "4.221426486968994\n",
      "\n",
      "\n",
      "1.638213872909546\n",
      "3.4203832149505615\n",
      "shining through?\n",
      "\n",
      "\n",
      "7.282899379730225\n",
      "1.5249924659729004\n",
      "\n",
      "\n",
      "7.253878116607666\n",
      "2.966615915298462\n",
      "\n",
      "\n",
      "5.923389434814453\n",
      "7.25738525390625\n",
      "shining through?\n",
      "\n",
      "\n",
      "9.17809772491455\n",
      "6.407848834991455\n",
      "\n",
      "\n",
      "16.424650192260742\n",
      "6.8134989738464355\n",
      "\n",
      "\n",
      "3.1725151538848877\n",
      "3.2996888160705566\n",
      "shining through?\n",
      "\n",
      "\n",
      "10.633072853088379\n",
      "33.40980529785156\n",
      "shining through?\n",
      "\n",
      "\n",
      "1.2891801595687866\n",
      "5.816845417022705\n",
      "shining through?\n",
      "\n",
      "\n",
      "5.377060413360596\n",
      "1.453563928604126\n",
      "\n",
      "\n",
      "7.2516703605651855\n",
      "1.8786709308624268\n",
      "\n",
      "\n",
      "482.2633056640625\n",
      "2.143958568572998\n",
      "\n",
      "\n",
      "18.308876037597656\n",
      "4.985576152801514\n",
      "\n",
      "\n",
      "7.321451187133789\n",
      "5.779157638549805\n",
      "\n",
      "\n",
      "3.522669792175293\n",
      "6.087416172027588\n",
      "shining through?\n",
      "\n",
      "\n",
      "1.3660334348678589\n",
      "1.6126744747161865\n",
      "shining through?\n",
      "\n",
      "\n",
      "1.7741963863372803\n",
      "4.028468608856201\n",
      "shining through?\n",
      "\n",
      "\n",
      "9.52221393585205\n",
      "7.3112359046936035\n",
      "\n",
      "\n",
      "3.5849251747131348\n",
      "7.730266571044922\n",
      "shining through?\n",
      "\n",
      "\n",
      "3.201312303543091\n",
      "1.770248293876648\n",
      "\n",
      "\n",
      "38.71053695678711\n",
      "12.189131736755371\n",
      "\n",
      "\n",
      "6.974870681762695\n",
      "5.068640232086182\n",
      "\n",
      "\n",
      "12.273850440979004\n",
      "4.8920578956604\n",
      "\n",
      "\n",
      "1.9172377586364746\n",
      "1.586818814277649\n",
      "\n",
      "\n",
      "3.1886775493621826\n",
      "3.0175440311431885\n",
      "\n",
      "\n",
      "10.051145553588867\n",
      "1.5125300884246826\n",
      "\n",
      "\n",
      "4.752126693725586\n",
      "8.218689918518066\n",
      "shining through?\n",
      "\n",
      "\n",
      "1.3437963724136353\n",
      "1.6428921222686768\n",
      "shining through?\n",
      "\n",
      "\n",
      "3.379990339279175\n",
      "4.461911678314209\n",
      "shining through?\n",
      "\n",
      "\n",
      "6.418795108795166\n",
      "7.336945533752441\n",
      "shining through?\n",
      "\n",
      "\n",
      "29.953765869140625\n",
      "14.246355056762695\n",
      "\n",
      "\n",
      "5.401801586151123\n",
      "5.810525417327881\n",
      "shining through?\n",
      "\n",
      "\n",
      "1.9038883447647095\n",
      "3.343221426010132\n",
      "shining through?\n",
      "\n",
      "\n",
      "35.731849670410156\n",
      "110.50111389160156\n",
      "shining through?\n",
      "\n",
      "\n",
      "2.54945707321167\n",
      "20.785503387451172\n",
      "shining through?\n",
      "\n",
      "\n",
      "1.7394312620162964\n",
      "1.1537089347839355\n",
      "\n",
      "\n",
      "43.85743713378906\n",
      "14.513893127441406\n",
      "\n",
      "\n",
      "27.163042068481445\n",
      "5.182249069213867\n",
      "\n",
      "\n",
      "3.852680206298828\n",
      "1.9313143491744995\n",
      "\n",
      "\n",
      "19.66425323486328\n",
      "12.90278434753418\n",
      "\n",
      "\n",
      "3.370345115661621\n",
      "4.952810764312744\n",
      "shining through?\n",
      "\n",
      "\n",
      "10.609330177307129\n",
      "6.515657901763916\n",
      "\n",
      "\n",
      "8.016602516174316\n",
      "7.2759294509887695\n",
      "\n",
      "\n",
      "7.471157073974609\n",
      "13.86333179473877\n",
      "shining through?\n",
      "\n",
      "\n",
      "3.66760516166687\n",
      "5.602653980255127\n",
      "shining through?\n",
      "\n",
      "\n",
      "10.38975715637207\n",
      "7.908402442932129\n",
      "\n",
      "\n",
      "2.889955759048462\n",
      "2.5129623413085938\n",
      "\n",
      "\n",
      "3.2374038696289062\n",
      "3.681344985961914\n",
      "shining through?\n",
      "\n",
      "\n",
      "4.985504627227783\n",
      "6.3851237297058105\n",
      "shining through?\n",
      "\n",
      "\n",
      "1.4208569526672363\n",
      "1.8779603242874146\n",
      "shining through?\n",
      "\n",
      "\n",
      "2.8837435245513916\n",
      "2.6378021240234375\n",
      "\n",
      "\n",
      "11.58798885345459\n",
      "2.2741546630859375\n",
      "\n",
      "\n",
      "347.001708984375\n",
      "2.0547542572021484\n",
      "\n",
      "\n",
      "1.2075270414352417\n",
      "6.063653469085693\n",
      "shining through?\n",
      "\n",
      "\n",
      "6.707906246185303\n",
      "2.0836257934570312\n",
      "\n",
      "\n",
      "16.912057876586914\n",
      "2.8392558097839355\n",
      "\n",
      "\n",
      "7.1470232009887695\n",
      "2.8695778846740723\n",
      "\n",
      "\n",
      "4.066621780395508\n",
      "4.097252368927002\n",
      "shining through?\n",
      "\n",
      "\n",
      "1.9035590887069702\n",
      "3.912970542907715\n",
      "shining through?\n",
      "\n",
      "\n",
      "16.980379104614258\n",
      "4.527816295623779\n",
      "\n",
      "\n",
      "6.184070110321045\n",
      "6.686038970947266\n",
      "shining through?\n",
      "\n",
      "\n",
      "21.47637176513672\n",
      "1.9100301265716553\n",
      "\n",
      "\n",
      "2.5078234672546387\n",
      "1.8420368432998657\n",
      "\n",
      "\n",
      "2.9267632961273193\n",
      "4.5284318923950195\n",
      "shining through?\n",
      "\n",
      "\n",
      "7.128840923309326\n",
      "4.952328205108643\n",
      "\n",
      "\n",
      "8.877134323120117\n",
      "2.9892377853393555\n",
      "\n",
      "\n",
      "3.274660110473633\n",
      "6.181369781494141\n",
      "shining through?\n",
      "\n",
      "\n",
      "60.10594177246094\n",
      "11.299870491027832\n",
      "\n",
      "\n",
      "22.02524757385254\n",
      "8.705930709838867\n",
      "\n",
      "\n",
      "3.51672101020813\n",
      "3.5541555881500244\n",
      "shining through?\n",
      "\n",
      "\n",
      "1.7910382747650146\n",
      "2.661832332611084\n",
      "shining through?\n",
      "\n",
      "\n",
      "1.6561084985733032\n",
      "1.5520449876785278\n",
      "\n",
      "\n",
      "8.75070571899414\n",
      "5.307370185852051\n",
      "\n",
      "\n",
      "171.25173950195312\n",
      "32.531288146972656\n",
      "\n",
      "\n",
      "11.763965606689453\n",
      "5.242405414581299\n",
      "\n",
      "\n",
      "2.072728157043457\n",
      "2.621110200881958\n",
      "shining through?\n",
      "\n",
      "\n",
      "1.6082042455673218\n",
      "1.2964491844177246\n",
      "\n",
      "\n",
      "18.31443214416504\n",
      "3.402790069580078\n",
      "\n",
      "\n",
      "1.8629101514816284\n",
      "1.4605497121810913\n",
      "\n",
      "\n",
      "19.348237991333008\n",
      "3.94879150390625\n",
      "\n",
      "\n",
      "1.9581040143966675\n",
      "1.4932234287261963\n",
      "\n",
      "\n",
      "2.517909526824951\n",
      "2.935988664627075\n",
      "shining through?\n",
      "\n",
      "\n",
      "4.180631160736084\n",
      "2.3052384853363037\n",
      "\n",
      "\n",
      "8.920367240905762\n",
      "2.4226887226104736\n",
      "\n",
      "\n",
      "23.390859603881836\n",
      "4.0356974601745605\n",
      "\n",
      "\n",
      "1.5108450651168823\n",
      "1.6506733894348145\n",
      "shining through?\n",
      "\n",
      "\n",
      "2.0701167583465576\n",
      "3.5941972732543945\n",
      "shining through?\n",
      "\n",
      "\n",
      "1.5825096368789673\n",
      "2.8119242191314697\n",
      "shining through?\n",
      "\n",
      "\n",
      "2.919001817703247\n",
      "3.0643365383148193\n",
      "shining through?\n",
      "\n",
      "\n",
      "1.4035308361053467\n",
      "1.1676623821258545\n",
      "\n",
      "\n",
      "8.253711700439453\n",
      "4.583366870880127\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#qualitative: English and German perplexity, sentence by sentence!\n",
    "for i in range(100): \n",
    "    xprv,yprv = Xdeutra[i:i+1],ydeutra[i:i+1]\n",
    "    engper = modelE.evaluate(xprv,yprv,verbose=0)[2]\n",
    "    gerper = model.evaluate(xprv,yprv,verbose=0)[2]\n",
    "    print(engper)\n",
    "    print(gerper)\n",
    "    if gerper>engper: print(\"shining through?\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 838,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2.9305297107696533, 0.2565, 112.8181590499878]"
      ]
     },
     "execution_count": 838,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#So! what does english think of this english Machine translation of Kafka?\n",
    "modelE.evaluate(Xmet[:4000],ymet[:4000], verbose=0) #English test: English is quite surprised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 839,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2.847840805053711, 0.24175, 43.55324765014648]"
      ]
     },
     "execution_count": 839,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#what does German think?\n",
    "model.evaluate(Xmet[:4000],ymet[:4000], verbose=0) #German test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 673,
   "metadata": {},
   "outputs": [],
   "source": [
    "#German is less surprised than English!? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 674,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's try the human trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 840,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONJ PRON VERB ADP DET NOUN ADP PRON ADJ NOUN CONJ ADJ NOUN DET ADP DET NOUN ADP PRON NOUN PRON NOUN NOUN PART PRON NOUN ADJ CONJ VERB PRON ADJ NOUN VERB\n"
     ]
    }
   ],
   "source": [
    "#human translation\n",
    "mymeta = deutra = open(\"metamor.rtf\").read()\n",
    "#sentok\n",
    "mymetasen = nltk.sent_tokenize(mymeta)\n",
    "#pos\n",
    "mymetpos = [nltk.pos_tag(nltk.wordpunct_tokenize(sen), tagset='universal') for sen in mymetasen]\n",
    "#only pos\n",
    "mymetpos = [[tup[1] for tup in sen] for sen in mymetpos]\n",
    "#no punct (to keep the original format)\n",
    "newseq = []\n",
    "for seq in mymetpos:\n",
    "    prv=[]\n",
    "    for el in seq:\n",
    "        if el == '.': prv.append('PUNCT')\n",
    "        elif el == 'PRT': prv.append('PART')\n",
    "        else: prv.append(el)\n",
    "    newseq.append(prv)\n",
    "#then again\n",
    "mymetpos = [\" \".join(seq) for seq in newseq]\n",
    "\n",
    "print(mymetpos[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 841,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Sequences: 24550\n",
      "Max Sequence Length: 252\n"
     ]
    }
   ],
   "source": [
    "# check\n",
    "\n",
    "#max_length = 252 #has to be the same as the English model. Convoluted, I know!\n",
    "#mah?\n",
    "\n",
    "sequences = list()\n",
    "for line in mymetpos:#data.split(\"\\n\"):\n",
    "\tencoded = tokenizer.texts_to_sequences([line])[0]\n",
    "\tfor i in range(1, len(encoded)):\n",
    "\t\tsequence = encoded[:i+1]\n",
    "\t\tsequences.append(sequence)\n",
    "print('Total Sequences: %d' % len(sequences))\n",
    "\n",
    "# pad input sequences\n",
    "#max_length = max([len(seq) for seq in sequences])\n",
    "sequences = pad_sequences(sequences, maxlen=max_length, padding='pre')\n",
    "print('Max Sequence Length: %d' % max_length)\n",
    "\n",
    "# split into input and output elements\n",
    "sequences = array(sequences)\n",
    "Xmethum, ymethum = sequences[:,:-1],sequences[:,-1]\n",
    "ymethum = to_categorical(ymethum, num_classes=vocab_size) # i have NO IDEA what i am doing here!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 842,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test size\n",
    "n=6000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 843,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([3.1389377161661782, 0.2395, 126.3743952738444],\n",
       " [2.9279761555989583, 0.25883333333333336, 110.43650953928629])"
      ]
     },
     "execution_count": 843,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#double eval for english model\n",
    "modelE.evaluate(Xmethum[:n],ymethum[:n], verbose=0), modelE.evaluate(Xmet[:n],ymet[:n], verbose=0) #English test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 844,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([3.106140266418457, 0.21966666666666668, 40.23620025634766],\n",
       " [2.8224224751790365, 0.2425, 42.021267707824705])"
      ]
     },
     "execution_count": 844,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#double eval for german model\n",
    "model.evaluate(Xmethum[:n],ymethum[:n], verbose=0), model.evaluate(Xmet[:n],ymet[:n], verbose=0) #German test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## German is less surprised about the translations. Is this shining through a lot? Is it not shining through at all? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
